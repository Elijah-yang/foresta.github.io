<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Cross-domain vulnerability detection using graph embedding and domain adaptation</title>
      <link href="/2023/12/19/Papers/Vul/Cross-domain%20vulnerability%20detection%20using%20graph%20embedding%20and%20domain%20adaptation/"/>
      <url>/2023/12/19/Papers/Vul/Cross-domain%20vulnerability%20detection%20using%20graph%20embedding%20and%20domain%20adaptation/</url>
      
        <content type="html"><![CDATA[<p>a山东省计算机网络重点实验室，山东省计算机科学中心（济南国家超级计算机中心），齐鲁工业大学（山东科学院），济南250014，中国b北京邮电大学网络空间安全学院，北京100876，中国c公共大数据国家重点实验室，贵州大学计算机科学与技术学院，贵阳550025</p><h1 id="0-abstract"><a class="markdownIt-Anchor" href="#0-abstract"></a> 0 Abstract</h1><p>漏洞检测是维护网络空间安全的有效手段。机器学习方法由于其准确性和自动化的优势，在软件安全领域引起了人们的广泛关注。<strong>然而，目前的研究主要集中在训练数据和测试数据属于同一域的域内漏洞检测上。<strong>由于应用场景、编码习惯等因素，不同软件项目中的漏洞可能服从不同的概率分布。当机器学习方法应用于一个全新的项目时，这种差异会影响它们的性能。为了解决这个冷启动问题，我们</strong>提出了一个使用图嵌入和深度域自适应（VulGDA）的跨域漏洞检测框架。<strong>它以多种跨域方式工作，包括</strong>零样本方式</strong>，即目标域中没有标记数据可用于训练。将VulGDA分解为图嵌入和域自适应。在图嵌入阶段，我们将源代码中的样本转换为图表示，其中元素根据其语法和语义关系直接连接。然后，我们将来自图中定义的邻居和边的信息聚合为实值向量。通过图形嵌入，VulGDA提取了全面的漏洞特征，并解决了长期依赖性的挑战。针对训练数据和测试数据之间的差异，使用域自适应来训练特征生成器。该特征生成器将嵌入的图映射到一个“深层”特征，该特征对漏洞检测具有鉴别性，并且对域之间的移动保持不变。我们进行了一项系统实验来验证VulGDA的有效性。结果表明，将图嵌入和深域自适应相结合，提高了VulGDA在跨域漏洞检测中的性能。与最先进的方法相比，我们的方法在冷启动条件下具有更好的性能。</p><h1 id="1-contributions"><a class="markdownIt-Anchor" href="#1-contributions"></a> 1 Contributions</h1><p>主要问题就是目前的研究针对某一数据集或是某些特定的漏洞，没有在跨域问题上提出适当的解决方法；</p><ul><li><p>提出了一个结合了图嵌入和域自适应的跨域漏洞检测框架VuLGDA，VulGDA以ZeroShot方式工作，这是跨域检测中最严格的方式。VulGDA通过将目标域中的监督信息添加到训练数据中，也适用于few-shot和域内方式。</p></li><li><p>提出了一种提取综合漏洞特征的图嵌入方法；减少噪声对漏洞特征的影响</p></li><li><p>提出了一种神经网络来学习特征生成器。该特征生成器通过减少分类损失和域差异来进行优化。最后，该特征生成器将学习到的源域中的漏洞模式传输到目标域。</p></li><li><p>实现了一个原型并进行了系统的实验。结果表明，图嵌入和域自适应的应用提高了VulGDA的性能。与现有方法相比，VulGDA在跨域漏洞检测方面取得了更好的性能。</p></li></ul><h1 id="2-method"><a class="markdownIt-Anchor" href="#2-method"></a> 2 Method</h1><p>VulGDA旨在检测现实世界软件中的漏洞。为了实用，本文考虑了最严格的条件。也就是说，应用在源域中训练的检测模型来检测目标域中的漏洞，并且该模型在训练期间无法从目标域获得任何监督信息（零样本）。这一假设使VulGDA具有良好的泛化能力，并易于应用于其他跨域检测情况，例如历史漏洞数据很少（很少发生）或丰富（Indomain）的项目。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191701536.png" alt="image-20231219170117488" /></p><p>问题定义：假设我们的检测架构可以访问源域中的标记数据集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>∼</mo><mo stretchy="false">(</mo><msub><mi>D</mi><mi>s</mi></msub><msup><mo stretchy="false">)</mo><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">S=\{(x_{i},y_{i})\}_{i=1}^{n}\sim(D_{s})^{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.008664em;vertical-align:-0.258664em;"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>  和目标域中的未标记数据集 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>∼</mo><mo stretchy="false">(</mo><msubsup><mi>D</mi><mi>T</mi><mi>X</mi></msubsup><msup><mo stretchy="false">)</mo><mrow><mi>N</mi><mo>−</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">T=\{({x}_{i})\}_{i=n+1}^{N}\sim(D_{T}^{X})^{N-n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.158326em;vertical-align:-0.316995em;"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.316995em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1166619999999998em;vertical-align:-0.275331em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>是训练样本的总数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>D</mi><mi>T</mi><mi>X</mi></msubsup></mrow><annotation encoding="application/x-tex">D_T^X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1166619999999998em;vertical-align:-0.275331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span></span></span></span>是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">D_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>上的边缘分布。</p><p>最终目标是建立一个预测<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>η</mi><mi mathvariant="normal">：</mi><mi>X</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">η：X→Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>：最小<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><msub><mi>D</mi><mi>T</mi></msub></msub><mo stretchy="false">(</mo><mi>η</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">*</mi><mo>⁡</mo><msub><mrow><mi>P</mi><mi>r</mi></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><mi>y</mi><mo stretchy="false">)</mo><mo>∼</mo><msub><mi>D</mi><mi>T</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>η</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">R_{D_T}(η)=\operatorname*{Pr}_{(x y)\sim D_{T}}(\eta(x)\neq y),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.000305em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mop"><span class="mord mathrm">*</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span>，而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">D_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中没有监督信息。</p><p><strong>VuLGDA有两个阶段组成：</strong></p><p><strong>图嵌入：</strong></p><p>图嵌入阶段的目的是将源代码中的漏洞样本转换为可由机器学习模型处理的实值向量。因此，通过句法和语义分析，图嵌入阶段首先将样本转换为CPG，即图结构中的中间表示。在CPG中，语法和语义相关的元素被直接连接在一起，这缓解了长期的依赖问题和域之间的分歧。然后，利用预训练的单词嵌入生成标记嵌入和节点嵌入。最后，我们使用GGNN通过传播和聚合来自CPG中定义的邻居的信息来获得嵌入向量。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191747401.png" alt="image-20231219174742348" /></p><p><strong>域自适应：</strong></p><p>域自适应阶段旨在学习在域之间转换分布的特征生成器。提出了一种由特征生成器、分类器和瓶颈组成的神经网络来学习该特征生成器。将源域中的特征作为输入的分类器产生分类损失。瓶颈度量源域和目标域中的特征之间的差异。通过最小化分类损失和领域差异来优化特征生成器。检测结果由生成的“深层”特征训练的分类器报告。经过上述阶段后，VulGDA可以在目标域中检测跨域漏洞，而无需标记数据。</p><p>本文的创新点主要在于领域自适应：</p><p>传统的机器学习方法假设训练数据和测试数据服从相同的特征分布。然而，由于编程风格和应用场景的不同，不同领域的漏洞特征可能服从不同的分布，从而影响模型的检测性能。在本节中，使用来自源域和目标域的数据，我们构建了一个特征生成器G f:XG→XF。G f将嵌入XG的图转换为“深度”特征向量XF，该向量对漏洞检测具有判别性，并且随着域之间的移动而不变。因此，目标函数定义为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><mi mathvariant="normal">*</mi><mo>⁡</mo><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi mathvariant="normal">ℓ</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><mi>R</mi><mo stretchy="false">(</mo><msub><mi>B</mi><mi>S</mi></msub><mo separator="true">,</mo><msub><mi>B</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f^{*}=\arg\operatorname*{min}\frac{1}{B}\sum_{i=1}^{B}\ell(f(x_{i}),y_{i})+\lambda R(B_{S},B_{T})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933136em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.738696em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mord mathrm">*</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">ℓ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">λ</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>其中B表示批次大小，（f（xi），yi）表示分类损失，BS和BT分别在源域和目标域中为2个批次，R（∗，∗）测量域差异。λ用于调整分类损失和域差异的权重。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312192225166.png" alt="image-20231219222546132" /></p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> Vulnerabilities </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安全 </tag>
            
            <tag> Vulnerabilities </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conv2d</title>
      <link href="/2023/12/19/AILearning/pytorch/conv2d/"/>
      <url>/2023/12/19/AILearning/pytorch/conv2d/</url>
      
        <content type="html"><![CDATA[<h4 id="1-用法"><a class="markdownIt-Anchor" href="#1-用法"></a> 1 用法</h4><ul><li><h5 id="conv2din_channels-out_channels-kernel_size-stride1padding0-dilation1-groups1biastrue-padding_modezeros"><a class="markdownIt-Anchor" href="#conv2din_channels-out_channels-kernel_size-stride1padding0-dilation1-groups1biastrue-padding_modezeros"></a> Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1,bias=True, padding_mode=‘zeros’)</h5></li></ul><h4 id="2-参数"><a class="markdownIt-Anchor" href="#2-参数"></a> 2 参数</h4><ul><li><h5 id="in_channels输入的通道数目-必选"><a class="markdownIt-Anchor" href="#in_channels输入的通道数目-必选"></a> in_channels：输入的通道数目 【必选】</h5></li><li><h5 id="out_channels-输出的通道数目-必选"><a class="markdownIt-Anchor" href="#out_channels-输出的通道数目-必选"></a> out_channels： 输出的通道数目 【必选】</h5></li><li><h5 id="kernel_size卷积核的大小类型为int-或者元组当卷积是方形的时候只需要一个整数边长即可卷积不是方形要输入一个元组表示-高和宽-必选"><a class="markdownIt-Anchor" href="#kernel_size卷积核的大小类型为int-或者元组当卷积是方形的时候只需要一个整数边长即可卷积不是方形要输入一个元组表示-高和宽-必选"></a> kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】</h5></li><li><h5 id="stride-卷积每次滑动的步长为多少默认是-1-可选"><a class="markdownIt-Anchor" href="#stride-卷积每次滑动的步长为多少默认是-1-可选"></a> stride： 卷积每次滑动的步长为多少，默认是 1 【可选】</h5></li><li><h5 id="padding-设置在所有边界增加-值为-0-的边距的大小也就是在feature-map-外围增加几圈-0-例如当-padding-1-的时候如果原来大小为-3-3-那么之后的大小为-5-5-即在外围加了一圈-0-可选"><a class="markdownIt-Anchor" href="#padding-设置在所有边界增加-值为-0-的边距的大小也就是在feature-map-外围增加几圈-0-例如当-padding-1-的时候如果原来大小为-3-3-那么之后的大小为-5-5-即在外围加了一圈-0-可选"></a> padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】</h5></li><li><h5 id="dilation控制卷积核之间的间距什么玩意请看例子可选"><a class="markdownIt-Anchor" href="#dilation控制卷积核之间的间距什么玩意请看例子可选"></a> dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】</h5></li><li><p>groups：控制输入和输出之间的连接。（不常用）【可选】</p><p>举例来说：<br />比如 groups 为1，那么所有的输入都会连接到所有输出<br />当 groups 为 2的时候，相当于将输入分为两组，并排放置两层，每层看到一半的输入通道并产生一半的输出通道，并且两者都是串联在一起的。这也是参数字面的意思：“组” 的含义。<br />需要注意的是，in_channels 和 out_channels 必须都可以整除 groups，否则会报错（因为要分成这么多组啊，除不开你让人家程序怎么办？）</p></li><li><h5 id="bias-是否将一个-学习到的-bias-增加输出中默认是-true-可选"><a class="markdownIt-Anchor" href="#bias-是否将一个-学习到的-bias-增加输出中默认是-true-可选"></a> bias： 是否将一个 学习到的 bias 增加输出中，默认是 True 。【可选】</h5></li><li><p>padding_mode ： 字符串类型，接收的字符串只有 “zeros” 和 “circular”。【可选】</p></li></ul><h4 id="3-相关形状"><a class="markdownIt-Anchor" href="#3-相关形状"></a> 3 相关形状</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191632440.png" alt="image-20231219163239412" /></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch入门</title>
      <link href="/2023/12/19/AILearning/pytorch/pytorch/"/>
      <url>/2023/12/19/AILearning/pytorch/pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="1torchnn简介"><a class="markdownIt-Anchor" href="#1torchnn简介"></a> 1.torch.nn简介</h1><p>1.1torch.nn相关库的导入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="12-torchnn概述"><a class="markdownIt-Anchor" href="#12-torchnn概述"></a> 1.2 torch.nn概述</h3><blockquote><p>Pytorch提供了几个设计得非常棒的模块和类，比如 torch.nn，torch.optim，Dataset 以及 DataLoader，来帮助程序员设计和训练神经网络。</p></blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：</p><ul><li><p>创建神经网络</p></li><li><p>训练神经网络</p></li><li><p>保存神经网络</p></li><li><p>恢复神经网络</p></li></ul><p>包括五大基本功能模块</p><ul><li>torch.nn是专门为神经网络设计的模块化接口</li><li>nn构建于autograd之上，可以用来定义和运行神经网络<ul><li>nn.Parameter</li><li>nn.Linear</li><li>nn.functional</li><li>nn.Module</li><li>nn.Sequential</li></ul></li></ul><h1 id="2nnlinear类全连接层"><a class="markdownIt-Anchor" href="#2nnlinear类全连接层"></a> 2.nn.Linear类（全连接层）</h1><h3 id="21函数功能"><a class="markdownIt-Anchor" href="#21函数功能"></a> 2.1函数功能</h3><p>用于创建一个多输入、多输出的全连接层。</p><p>nn.Linear本身并不包含激活函数（Functional）</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635002.png" alt="image-20231219163552911" /></p><h3 id="22函数说明"><a class="markdownIt-Anchor" href="#22函数说明"></a> 2.2函数说明</h3><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629041.png" alt="image-20211203155643310" /></p><ul><li>in_featrues：<ul><li>指输入的二维张量的大小，即输入的[batch_size, size]中的size</li><li>in_features的数量，决定的参数的个数Y=WX+b，X的维度就是in_features，X的维度决定W的维度，总参数的个数 = in_features + 1</li></ul></li><li>out_featrues<ul><li>指的是输出的二维张量的大小，<mark>即输出的二维张量的形状为[batch_size output_size].</mark></li><li>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。<strong>多少个输出，就需要多少个神经元</strong>。</li><li><mark>从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。</mark></li></ul></li></ul><h3 id="23多个全连接层构建全连接网络"><a class="markdownIt-Anchor" href="#23多个全连接层构建全连接网络"></a> 2.3多个全连接层构建全连接网络</h3><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629046.png" alt="image-20211203160715687" /></p><ul><li>使用nn.Linear类创建全连接层</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状 </span></span><br><span class="line">full_connect_layer = nn.Linear(in_features = <span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>, out_features = <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[28,28,1]</span></span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入</span></span><br><span class="line">x_input = x_input.view(<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629071.png" alt="image-20211203163313390" /></p><h2 id="3-nnfunctional常见函数"><a class="markdownIt-Anchor" href="#3-nnfunctional常见函数"></a> 3 nn.functional（常见函数）</h2><h3 id="31-nnfunctional概述"><a class="markdownIt-Anchor" href="#31-nnfunctional概述"></a> 3.1 nn.functional概述</h3><blockquote><p>nn.functional定义了创建神经网络所需要的一些常见的处理函数。如没有激活函数的神经元，各种激活函数等。</p></blockquote><ul><li>包含torch.nn库中所有函数，包含大量loss和activation function<ul><li>torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)</li><li>nn.functional.xxx是函数接口</li><li>nn.functional.xxx无法与nn.Sequential结合使用</li><li>没有学习参数的(eg. maxpool, loss_ func, activation func)<a href="http://xn--nn-gy2c4vz4a856fs0ap4ztl4ad2mv07c.functional.xn--xxxnn-zm6j.Xxx">等根据个人选择使用nn.functional.xxx或nn.Xxx</a></li><li>需要特别注意dropout层</li></ul></li></ul><h3 id="32-nnfunctional函数分类"><a class="markdownIt-Anchor" href="#32-nnfunctional函数分类"></a> 3.2 nn.functional函数分类</h3><p>nn.functional包括神经网络前向和后向处理所需要到的常见函数</p><ul><li>神经元处理函数</li><li>激活函数</li></ul><h3 id="33-激活函数案例"><a class="markdownIt-Anchor" href="#33-激活函数案例"></a> 3.3 激活函数案例</h3><ul><li>relu案例</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.relu( )</span></span><br><span class="line"><span class="built_in">print</span>(y_output)</span><br><span class="line">out = nn.functional.relu(y_output)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><h2 id="4-nnxxx和nnfunctionalxxx比较"><a class="markdownIt-Anchor" href="#4-nnxxx和nnfunctionalxxx比较"></a> 4 nn.xxx和nn.functional.xxx比较</h2><h3 id="41-相同点"><a class="markdownIt-Anchor" href="#41-相同点"></a> 4.1 相同点</h3><ul><li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</li><li>运行效率也是近乎相同。</li></ul><h3 id="42-不同点"><a class="markdownIt-Anchor" href="#42-不同点"></a> 4.2 不同点</h3><ul><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的类封装。</li><li>所有<code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code> 等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和<code>weight, bias</code>等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h2 id="5-nnparameter类"><a class="markdownIt-Anchor" href="#5-nnparameter类"></a> 5 nn.Parameter类</h2><h3 id="51-nnparameter概述"><a class="markdownIt-Anchor" href="#51-nnparameter概述"></a> 5.1 nn.Parameter概述</h3><blockquote><p>Parameter实际上也是tensor，也就是说是一个多维矩阵，是Variable类的一个特殊类。</p><p>当我们创建一个model时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter 成员列表中。</p></blockquote><h3 id="52-单个全连接层中参数的个数"><a class="markdownIt-Anchor" href="#52-单个全连接层中参数的个数"></a> 5.2 单个全连接层中参数的个数</h3><p>in_features的数量，决定的参数的个数   Y = WX + b,  X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 = in_features + 1</p><p>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</p><p>多少个输出，就需要多少个神经元。</p><p>总的W参数的个数=  in_features * out_features</p><p>总的b参数的个数=  1 * out_features</p><p>总的参数（W和B）的个数=  (in_features + 1) * out_features</p><h3 id="53-使用参数创建全连接层"><a class="markdownIt-Anchor" href="#53-使用参数创建全连接层"></a> 5.3 使用参数创建全连接层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629076.png" alt="image-20211203165921227" /></p><h2 id="6-nnmodule类"><a class="markdownIt-Anchor" href="#6-nnmodule类"></a> 6 nn.Module类</h2><ul><li>抽象概念，既可以表示神经网络中的某个层layer，也可以表示一个包含很多层的神经网络</li><li>modle.parameters()</li><li>modle.buffers()</li><li>modle.state_dict()</li><li>modle.modules()</li><li>forward(),to()</li></ul><h2 id="7-利用nnsequential类创建神经网络继承与nnmodule类"><a class="markdownIt-Anchor" href="#7-利用nnsequential类创建神经网络继承与nnmodule类"></a> 7 利用nn.Sequential类创建神经网络（继承与nn.Module类）</h2><blockquote><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中，同时以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p></blockquote><h2 id="8自定义神经网络模型类继承于module类"><a class="markdownIt-Anchor" href="#8自定义神经网络模型类继承于module类"></a> 8.自定义神经网络模型类（继承于Module类）</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># -1表示自动匹配</span></span><br><span class="line">        h1 = self.h1(x)</span><br><span class="line">        a1 =  self.relu1(h1)</span><br><span class="line">        out = self.out(a1)</span><br><span class="line">        a_out = self.softmax(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span>*<span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch tensor操作</title>
      <link href="/2023/12/19/AILearning/pytorch/tensor%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/12/19/AILearning/pytorch/tensor%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="一-张量的基本操作"><a class="markdownIt-Anchor" href="#一-张量的基本操作"></a> 一、张量的基本操作</h1><p>Pytorch 中，张量的操作分为<strong>结构操作和数学运算</strong>，其理解就如字面意思。结构操作就是改变张量本身的结构，数学运算就是对张量的元素值完成数学运算。</p><ul><li>常使用的张量结构操作：维度变换（tranpose、view 等）、合并分割（split、chunk等）、索引切片（index_select、gather 等）。</li><li>常使用的张量数学运算：标量运算、向量运算、矩阵运算。</li></ul><h1 id="二-维度变换"><a class="markdownIt-Anchor" href="#二-维度变换"></a> 二、维度变换</h1><h2 id="21-squeeze-vs-unsqueeze-维度增减"><a class="markdownIt-Anchor" href="#21-squeeze-vs-unsqueeze-维度增减"></a> <strong>2.1 squeeze vs unsqueeze 维度增减</strong></h2><ul><li><strong>squeeze()</strong>：对 tensor 进行维度的压缩，去掉维数为 1 的维度。用法：torch.squeeze(a) 将 a 中所有为 1 的维度都删除，或者 a.squeeze(1) 是去掉 a中指定的维数为 1 的维度。</li><li><strong>unsqueeze()</strong>：对数据维度进行扩充，给指定位置加上维数为 1 的维度。用法：torch.unsqueeze(a, N)，或者 a.unsqueeze(N)，在 a 中指定位置 N 加上一个维数为 1 的维度。</li></ul><p>squeeze 用例程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.squeeze(a)</span><br><span class="line">c = a.squeeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br></pre></td></tr></table></figure><p>程序输出结果如下：</p><blockquote><p>torch.Size([3, 3]) torch.Size([1, 3, 3])</p></blockquote><p>unsqueeze 用例程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">y1 = torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line">y2 = x.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y1.shape)</span><br><span class="line"><span class="built_in">print</span>(y2.shape)</span><br></pre></td></tr></table></figure><p>程序输出结果如下：</p><blockquote><p>torch.Size([1, 3, 3]) torch.Size([1, 3, 3])</p></blockquote><h2 id="22-transpose-vs-permute-维度交换"><a class="markdownIt-Anchor" href="#22-transpose-vs-permute-维度交换"></a> <strong>2.2 transpose vs permute 维度交换</strong></h2><p>torch.transpose() 只能交换两个维度，而 .permute() 可以自由交换任意位置。函数定义如下：</p><ul><li>transpose(dim0, dim1) → Tensor # See torch.transpose()</li><li>permute(*dims) → Tensor # dim(int). Returns a view of the original tensor with its dimensions permuted.</li></ul><p>在 CNN 模型中，我们经常遇到交换维度的问题，举例：四个维度表示的 tensor：[batch, channel, h, w]（nchw），如果想把 channel 放到最后去，形成[batch, h, w, channel]（nhwc），如果使用 torch.transpose() 方法，至少要交换两次（先 1 3 交换再 1 2 交换），而使用 .permute() 方法只需一次操作，更加方便。例子程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">32</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 3, 28, 32]</span></span><br><span class="line"><span class="built_in">print</span>(b.transpose(<span class="number">1</span>, <span class="number">3</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 32, 28, 3])</span></span><br><span class="line"><span class="built_in">print</span>(b.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 32, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 32, 3]</span></span><br></pre></td></tr></table></figure><h2 id="23-reshape-vs-view"><a class="markdownIt-Anchor" href="#23-reshape-vs-view"></a> <strong>2.3 reshape vs view</strong></h2><blockquote><p>view只适合对满足连续性条件（contiguous）的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。view能干的reshape都能干，如果view不能干就可以用reshape来处理。更多可看[1]</p></blockquote><h2 id="24-einsum"><a class="markdownIt-Anchor" href="#24-einsum"></a> <strong>2.4 einsum</strong></h2><p>首先看下 einsum 实现矩阵乘法的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 语义解析：</span></span><br><span class="line"><span class="comment"># 输入a：2阶张量，下标为ik</span></span><br><span class="line"><span class="comment"># 输入b: 2阶张量，下标为kj</span></span><br><span class="line"><span class="comment"># 输出o: 2阶张量，下标为i和j</span></span><br><span class="line">c = torch.einsum(<span class="string">&quot;ik,kj-&gt;ij&quot;</span>, [a, b])</span><br><span class="line"><span class="comment"># 等价操作 torch.mm(a, b)</span></span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">60.</span>).reshape(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">b = np.arange(<span class="number">24.</span>).reshape(<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 语义解析：</span></span><br><span class="line"><span class="comment"># 输入a：3阶张量，下标为ijk</span></span><br><span class="line"><span class="comment"># 输入b: 3阶张量，下标为jil</span></span><br><span class="line"><span class="comment"># 输出o: 2阶张量，下标为k和l</span></span><br><span class="line">c = np.einsum(<span class="string">&#x27;ijk,jil-&gt;kl&#x27;</span>, a, b)</span><br></pre></td></tr></table></figure><p>这个方法可以实现矩阵乘法，但是也可以用来更换维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 语义解析:</span></span><br><span class="line"><span class="comment"># 当后面只有一个张量时，就是对自己维度进行变换</span></span><br><span class="line"><span class="comment"># 比较常用的就是将chanel换到最后</span></span><br><span class="line">x = torch.einsum(<span class="string">&#x27;nchw-&gt;nhwc&#x27;</span>, x)</span><br></pre></td></tr></table></figure><blockquote><p>更多可看[2]</p></blockquote><h1 id="三-索引切片"><a class="markdownIt-Anchor" href="#三-索引切片"></a> 三、索引切片</h1><h2 id="31-规则索引切片方式"><a class="markdownIt-Anchor" href="#31-规则索引切片方式"></a> <strong>3.1 规则索引切片方式</strong></h2><p>张量的索引切片方式和 numpy、python 多维列表几乎一致，都可以通过索引和切片对部分元素进行修改。切片时支持缺省参数和省略号。实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] <span class="comment"># 第 1 行数据</span></span><br><span class="line">tensor([<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">2</span>][<span class="number">2</span>]</span><br><span class="line">tensor(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>:<span class="number">3</span>,:]  <span class="comment"># 第1至第3行，全部列</span></span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>:<span class="number">2</span>,:]  <span class="comment"># 第1行至第2行</span></span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">1</span>:,-<span class="number">1</span>]  <span class="comment"># 第2行至最后行，最后一列</span></span><br><span class="line">tensor([<span class="number">9</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">1</span>:,::<span class="number">2</span>] <span class="comment"># 第1行至最后行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure><p>以上切片方式相对规则，对于不规则的切片提取,可以使用 torch.index_select, torch.take, torch.gather, torch.masked_select。</p><h2 id="32-gather-和-torchindex_select-算子"><a class="markdownIt-Anchor" href="#32-gather-和-torchindex_select-算子"></a> <strong>3.2 gather 和 torch.index_select 算子</strong></h2><blockquote><p>gather 算子的用法比较难以理解，在翻阅了官方文档和网上资料后，我有了一些自己的理解。</p></blockquote><p>1，gather 是不规则的切片提取算子（Gathers values along an axis specified by dim. 在指定维度上根据索引 index 来选取数据）。函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, *, sparse_grad=<span class="literal">False</span>, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p><strong>参数解释：</strong></p><ul><li>input (Tensor) – the source tensor.</li><li>dim (int) – the axis along which to index.</li><li>index (LongTensor) – the indices of elements to gather.</li></ul><p>对于 3D tensor，output 值的定义如下： gather 的官方定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]   <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure><p>下面结合 2D 和 3D tensor 的用例来直观理解算子用法。<br />（1）对于 2D tensor 的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">0</span>, <span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])  <span class="comment"># 选取对角线元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(a, <span class="number">0</span>, index)</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure><p>output 值定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照 index = tensor([[0, 1, 2, 3]])顺序作用在行上索引依次为0,1,2,3</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">a[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">5</span></span><br><span class="line">a[<span class="number">2</span>][<span class="number">2</span>] = <span class="number">10</span></span><br><span class="line">a[<span class="number">3</span>][<span class="number">3</span>] = <span class="number">15</span></span><br></pre></td></tr></table></figure><p>（2）索引更复杂的 2D tensor 例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>output 值的计算如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output[i][j] = <span class="built_in">input</span>[i][index[i][j]]  <span class="comment"># if dim = 1</span></span><br><span class="line">output[<span class="number">0</span>][<span class="number">0</span>] = <span class="built_in">input</span>[<span class="number">0</span>][index[<span class="number">0</span>][<span class="number">0</span>]] = <span class="built_in">input</span>[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">output[<span class="number">0</span>][<span class="number">1</span>] = <span class="built_in">input</span>[<span class="number">0</span>][index[<span class="number">0</span>][<span class="number">1</span>]] = <span class="built_in">input</span>[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">output[<span class="number">1</span>][<span class="number">0</span>] = <span class="built_in">input</span>[<span class="number">1</span>][index[<span class="number">1</span>][<span class="number">0</span>]] = <span class="built_in">input</span>[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">4</span></span><br><span class="line">output[<span class="number">1</span>][<span class="number">1</span>] = <span class="built_in">input</span>[<span class="number">1</span>][index[<span class="number">1</span>][<span class="number">1</span>]] = <span class="built_in">input</span>[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">3</span></span><br></pre></td></tr></table></figure><p>总结：<strong>可以看到 gather 是通过将索引在指定维度 dim 上的值替换为 index 的值，但是其他维度索引不变的情况下获取 tensor 数据</strong>。直观上可以理解为对矩阵进行重排，比如对每一行(dim=1)的元素进行变换，比如 torch.gather(a, 1, torch.tensor([[1,2,0], [1,2,0]])) 的作用就是对 矩阵 a 每一行的元素，进行 permtute(1,2,0) 操作。</p><p>2，理解了 gather 再看 index_select 就很简单，函数作用是返回沿着输入张量的指定维度的指定索引号进行索引的张量子集。函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>函数返回一个新的张量，它使用数据类型为 LongTensor 的 index 中的条目沿维度 dim 索引输入张量。返回的张量具有与原始张量（输入）相同的维数。 维度尺寸与索引长度相同； 其他尺寸与原始张量中的尺寸相同。实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>,  <span class="number">0.2647</span>, -<span class="number">0.1228</span>, -<span class="number">1.1068</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">0</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">1</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>, -<span class="number">0.5414</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>, -<span class="number">0.1228</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>,  <span class="number">0.7230</span>]])</span><br></pre></td></tr></table></figure><h1 id="四-合并分割"><a class="markdownIt-Anchor" href="#四-合并分割"></a> 四、合并分割</h1><h2 id="41-torchcat-和-torchstack"><a class="markdownIt-Anchor" href="#41-torchcat-和-torchstack"></a> <strong>4.1 torch.cat 和 torch.stack</strong></h2><p>可以用 torch.cat 方法和 torch.stack 方法将多个张量合并，也可以用 torch.split方法把一个张量分割成多个张量。torch.cat 和 torch.stack 有略微的区别，torch.cat 是连接，不会增加维度，而 torch.stack 是堆叠，会增加一个维度。两者函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</span></span><br><span class="line">torch.cat(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"><span class="comment"># Concatenates a sequence of tensors along **a new** dimension. All tensors need to be of the same size.</span></span><br><span class="line">torch.stack(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>torch.cat 和 torch.stack 用法实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">0</span>,<span class="number">9</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.arange(<span class="number">10</span>,<span class="number">19</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.arange(<span class="number">20</span>,<span class="number">29</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat_abc = torch.cat([a,b,c], dim=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(cat_abc.shape)</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(cat_abc)</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">        [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack_abc = torch.stack([a,b,c], axis=<span class="number">0</span>)  <span class="comment"># torch中dim和axis参数名可以混用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(stack_abc.shape)</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(stack_abc)</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">         [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">         [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chunk_abc = torch.chunk(cat_abc, <span class="number">3</span>, dim=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chunk_abc</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]]),</span><br><span class="line"> tensor([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]),</span><br><span class="line"> tensor([[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">         [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">         [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]]))</span><br></pre></td></tr></table></figure><h2 id="42-torchsplit-和-torchchunk"><a class="markdownIt-Anchor" href="#42-torchsplit-和-torchchunk"></a> <strong>4.2 torch.split 和 torch.chunk</strong></h2><p>torch.split() 和 torch.chunk() 可以看作是 torch.cat() 的逆运算。split() 作用是将张量拆分为多个块，每个块都是原始张量的视图。split() 函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Splits the tensor into chunks. Each chunk is a view of the original tensor.</span></span><br><span class="line"><span class="string">If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.</span></span><br><span class="line"><span class="string">If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>chunk() 作用是将 tensor 按 dim（行或列）分割成 chunks 个 tensor 块，返回的是一个元组。chunk() 函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>) → <span class="type">List</span> of Tensors</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.</span></span><br><span class="line"><span class="string">Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.</span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    input (Tensor) – the tensor to split</span></span><br><span class="line"><span class="string">    chunks (int) – number of chunks to return</span></span><br><span class="line"><span class="string">    dim (int) – dimension along which to split the tensor</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">10</span>).reshape(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(a, <span class="number">2</span>)</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">3</span>]]),</span><br><span class="line"> tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">          [<span class="number">6</span>, <span class="number">7</span>]]),</span><br><span class="line"> tensor([[<span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(a, [<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>]]),</span><br><span class="line"> tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">(tensor([[<span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">8</span>]]), </span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">5</span>],</span><br><span class="line">        [<span class="number">7</span>],</span><br><span class="line">        [<span class="number">9</span>]]))</span><br></pre></td></tr></table></figure><h1 id="五-卷积相关算子"><a class="markdownIt-Anchor" href="#五-卷积相关算子"></a> 五、卷积相关算子</h1><h2 id="51-上采样方法总结"><a class="markdownIt-Anchor" href="#51-上采样方法总结"></a> <strong>5.1 上采样方法总结</strong></h2><p>上采样大致被总结成了三个类别：</p><ol><li>基于线性插值的上采样：最近邻算法（nearest）、双线性插值算法（bilinear）、双三次插值算法（bicubic）等，这是传统图像处理方法。</li><li>基于深度学习的上采样（转置卷积，也叫反卷积 Conv2dTranspose2d等）</li><li>Unpooling 的方法（简单的补零或者扩充操作）<br />计算效果：最近邻插值算法 &lt; 双线性插值 &lt; 双三次插值。计算速度：最近邻插值算法 &gt; 双线性插值 &gt; 双三次插值。</li></ol><h2 id="52-finterpolate-采样函数"><a class="markdownIt-Anchor" href="#52-finterpolate-采样函数"></a> <strong>5.2 F.interpolate 采样函数</strong></h2><blockquote><p>Pytorch 老版本有 nn.Upsample 函数，新版本建议用 torch.nn.functional.interpolate，一个函数可实现定制化需求的上采样或者下采样功能，。</p></blockquote><p>F.interpolate() 函数全称是 torch.nn.functional.interpolate()，函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interpolate</span>(<span class="params"><span class="built_in">input</span>, size=<span class="literal">None</span>, scale_factor=<span class="literal">None</span>, mode=<span class="string">&#x27;nearest&#x27;</span>, align_corners=<span class="literal">None</span>, recompute_scale_factor=<span class="literal">None</span></span>):  <span class="comment"># noqa: F811</span></span><br><span class="line">    <span class="comment"># type: (Tensor, <span class="type">Optional</span>[<span class="built_in">int</span>], <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]], <span class="built_in">str</span>, <span class="type">Optional</span>[<span class="built_in">bool</span>], <span class="type">Optional</span>[<span class="built_in">bool</span>]) -&gt; Tensor</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>参数解释如下：</p><ul><li>input(Tensor)：输入张量数据；</li><li>size： 输出的尺寸，数据类型为 tuple： ([optional D_out], [optional H_out], W_out)，和 scale_factor 二选一；</li><li>scale_factor：在高度、宽度和深度上面的放大倍数。数据类型既可以是 int——表明高度、宽度、深度都扩大同一倍数；也可是tuple——指定高度、宽度、深度等维度的扩大倍数；</li><li>mode： 上采样的方法，包括最近邻（nearest），线性插值（linear），双线性插值（bilinear），三次线性插值（trilinear），默认是最近邻（nearest）；</li><li>align_corners： 如果设为True，输入图像和输出图像角点的像素将会被对齐（aligned），这只在mode = linear, bilinear, or trilinear才有效，默认为False。</li></ul><p>例子程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">y = F.interpolate(x * <span class="number">2</span>, scale_factor=(<span class="number">2</span>, <span class="number">2</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)   <span class="comment"># torch.Size([3, 224, 224])</span></span><br></pre></td></tr></table></figure><h2 id="53-nnconvtranspose2d-反卷积"><a class="markdownIt-Anchor" href="#53-nnconvtranspose2d-反卷积"></a> <strong>5.3 nn.ConvTranspose2d 反卷积</strong></h2><p>转置卷积（有时候也称为反卷积，个人觉得这种叫法不是很规范），它是一种特殊的卷积，先 padding 来扩大图像尺寸，紧接着跟正向卷积一样，旋转卷积核 180 度，再进行卷积计算。</p><h1 id="引用"><a class="markdownIt-Anchor" href="#引用"></a> 引用</h1><p>[0] <a href="http://zhuanlan.zhihu.com/p/">zhuanlan.zhihu.com/p/</a><br />[1] <a href="https://blog.csdn.net/Flag_ing/article/details/109129752">https://blog.csdn.net/Flag_ing/article/details/109129752</a><br />[2] <a href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch tensor 计算</title>
      <link href="/2023/12/19/AILearning/pytorch/tensor%E8%AE%A1%E7%AE%97/"/>
      <url>/2023/12/19/AILearning/pytorch/tensor%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h4 id="torchmean"><a class="markdownIt-Anchor" href="#torchmean"></a> torch.mean()</h4><blockquote><p>mean()函数的参数：dim=0,按行求平均值，返回的形状是（1，列数）；dim=1,按列求平均值，返回的形状是（行数，1）,默认不设置dim的时候，返回的是所有元素的平均值。</p></blockquote><h4 id="torchpow"><a class="markdownIt-Anchor" href="#torchpow"></a> torch.pow()</h4><blockquote><p>功能: 实现张量和标量之间逐元素求指数操作, 或者在可广播的张量之间逐元素求指数操作.</p></blockquote><h4 id="torchstack"><a class="markdownIt-Anchor" href="#torchstack"></a> torch.stack()</h4><blockquote><p>官方解释：沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><p>注：<code>python</code>的序列数据只有<code>list</code>和<code>tuple</code>。</p><p>浅显说法：把多个2维的张量凑成一个3维的张量；多个3维的凑成一个4维的张量…以此类推，也就是在增加新的维度进行堆叠。</p><p>outputs = torch.stack(inputs, dim=?) → Tensor</p></blockquote><h4 id="torchclamp"><a class="markdownIt-Anchor" href="#torchclamp"></a> torch.clamp()</h4><blockquote><p>torch.clamp(input, min, max, out=None) → Tensor</p><p>将输入<code>input</code>张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。</p></blockquote><h4 id="torchbmm"><a class="markdownIt-Anchor" href="#torchbmm"></a> torch.bmm()</h4><blockquote><p>计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,m) 也就是说两个tensor的第一维是相等的，然后第一个数组的第三维和第二个数组的第二维度要求一样，对于剩下的则不做要求，输出维度 （b,h,m）;</p></blockquote><h4 id="torchsqueeze函数"><a class="markdownIt-Anchor" href="#torchsqueeze函数"></a> torch.squeeze()函数</h4><blockquote><p>torch.squeeze(input, dim=None, out=None)</p><p>squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中 input 中大小为1的所有维都已删除。</p><p>举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)</p><p>当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作。</p><p>举个例子：如果 input 的形状为 (A×1×B)，squeeze(input, 0)后，返回的tensor不变；squeeze(input, 1)后，返回的tensor将被压缩为 (A×B)</p></blockquote><h4 id="torchunsqueeze"><a class="markdownIt-Anchor" href="#torchunsqueeze"></a> torch.unsqueeze()</h4><blockquote></blockquote><h4 id="torchspmm"><a class="markdownIt-Anchor" href="#torchspmm"></a> torch.spmm</h4><blockquote><p>torch.spmm只支持 sparse 在前，dense 在后的矩阵乘法，两个sparse相乘或者dense在前的乘法不支持，当然两个dense矩阵相乘是支持的。</p></blockquote><h4 id="torchsum"><a class="markdownIt-Anchor" href="#torchsum"></a> torch.sum</h4><blockquote><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191625912.png" alt="img" /></p><p>在dim这个维度上，对里面的tesnor 进行加和，如果keepdim=False，返回结果会删去dim这个维度。因为在dim上加和之后，dim=1，所以可以直接删去。</p></blockquote><h4 id="torchdiag"><a class="markdownIt-Anchor" href="#torchdiag"></a> torch.diag</h4><blockquote><p>对角矩阵</p></blockquote><h4 id="torchconcat"><a class="markdownIt-Anchor" href="#torchconcat"></a> torch.concat</h4><blockquote><p>torch.cat ( (A, B), dim=0)接受一个由两个（或多个）tensor组成的元组，按行拼接，所以两个（多个）tensor的列数要相同。</p><p>torch.cat ( (A, B), dim=1)是按列拼接，所以两个tensor的行数要相同。</p></blockquote><h4 id="torchview"><a class="markdownIt-Anchor" href="#torchview"></a> torch.view</h4><blockquote><p>在PyTorch中<strong>view</strong>函数作用为重构张量的维度，相当于numpy中的resize()的功能，但是用法不太一样;</p><p>torch.view(参数a,参数b,…)，其中参数a=3,参数b=2决定了将一维的tt1重构成3*2维的张量。<br />有时候会出现torch.view(-1)或者torch.view(参数a,-1)这种情况。则-1参数是需要估算的。</p><p><strong>view()函数的功能与reshape类似，用来转换size大小。x = x.view(batchsize, -1)中batchsize指转换后有几行，而-1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数。</strong></p><p>之前对于pytorch的网络编程学习都是大致理解每一层的概念，有些语法语句没有从原理上弄清楚，就比如标题的x = x.view(x.size(0), -1)  。</p><p>这句话一般出现在model类的forward函数中，具体位置一般都是在调用分类器之前。分类器是一个简单的nn.Linear()结构，输入输出都是维度为一的值，x = x.view(x.size(0), -1)  这句话的出现就是为了将前面多维度的tensor展平成一维。</p></blockquote><h4 id="torchpermute"><a class="markdownIt-Anchor" href="#torchpermute"></a> torch.permute</h4><blockquote><p>permute（dims）<br />参数dims用矩阵的维数代入，一般默认从0开始。即第0维，第1维等等<br />也可以理解为，第0块，第1块等等。当然矩阵最少是两维才能使用permute<br />如是两维，dims分别为是0和1<br />可以写成permute（0,1）这里不做任何变化，维数与之前相同<br />如果写成permute（1,0）得到的就是矩阵的转置<br />如果三维是permute(0,1,2)<br />0代表共有几块维度：本例中0对应着3块矩阵<br />1代表每一块中有多少行：本例中1对应着每块有2行<br />2代表每一块中有多少列：本例中2对应着每块有5列<br />所以是3块2行5列的三维矩阵</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习率衰减方法</title>
      <link href="/2023/12/19/AILearning/pytorch/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/"/>
      <url>/2023/12/19/AILearning/pytorch/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="pytorch"><a class="markdownIt-Anchor" href="#pytorch"></a> <a href="https://so.csdn.net/so/search?q=Pytorch&amp;spm=1001.2101.3001.7020">Pytorch</a> 学习率衰减方法</h3><h1 id="1什么是学习率衰减"><a class="markdownIt-Anchor" href="#1什么是学习率衰减"></a> 1.什么是学习率衰减</h1><p><a href="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&amp;spm=1001.2101.3001.7020">梯度下降</a>算法需要我们指定一个学习率作为权重更新步幅的控制因子，常用的学习率有0.01、0.001以及0.0001等，学习率越大则权重更新。一般来说，<strong>我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。</strong><br />Pytorch中有两种学习率调整(衰减)方法：<br />（1）使用<a href="https://so.csdn.net/so/search?q=%E5%BA%93%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">库函数</a>进行调整；<br />（2）手动调整。</p><h1 id="2使用库函数进行调整"><a class="markdownIt-Anchor" href="#2使用库函数进行调整"></a> 2.使用库函数进行调整</h1><p>Pytorch学习率调整策略通过 <a href="https://so.csdn.net/so/search?q=torch&amp;spm=1001.2101.3001.7020">torch</a>.optim.lr_sheduler 接口实现。pytorch提供的学习率调整策略分为三大类，分别是：<br />（1）有序调整：等间隔调整(Step)，多间隔调整(MultiStep)，指数衰减(Exponential)，余弦退火(CosineAnnealing);<br />（2）自适应调整：依训练状况伺机而变，通过监测某个指标的变化情况(loss、accuracy)，当该指标不怎么变化时，就是调整学习率的时机(ReduceLROnPlateau);<br />（3）自定义调整：通过自定义关于epoch的lambda函数调整学习率(LambdaLR)。<br />在每个epoch的训练中，使用scheduler.step()语句进行学习率更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        ......</span><br><span class="line">        y_ = model(x)</span><br><span class="line">        loss = criterion(y_,y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        ......</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    train(...)</span><br><span class="line">    test(...)</span><br><span class="line">    scheduler.step()</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></table></figure><h2 id="21有序调整"><a class="markdownIt-Anchor" href="#21有序调整"></a> 2.1.有序调整</h2><h3 id="211等间隔调整学习率"><a class="markdownIt-Anchor" href="#211等间隔调整学习率"></a> 2.1.1等间隔调整学习率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>每训练step_size个epoch，学习率调整为lr=lr*gamma.<br />以下内容中都将epoch和step对等，因为每个epoch中只进行一次scheduler.step()，实则该step指scheduler.step()中的step, 即step_size指scheduler.step()进行的次数。<br />参数</p><ul><li>optimizer: 神经网络训练中使用的优化器，如optimizer=torch.optim.SGD(…)</li><li>step_size(int): 学习率下降间隔数，单位是epoch，而不是iteration.</li><li>gamma(float):学习率调整倍数，默认为0.1</li><li>last_epoch(int)：上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整；当为-1时，学习率设置为初始值。<br /><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635978.png" alt="img" /></li></ul><h3 id="212多间隔调整学习率"><a class="markdownIt-Anchor" href="#212多间隔调整学习率"></a> 2.1.2.多间隔调整学习率</h3><p>跟2.1类似，但学习率调整的间隔并不是相等的，如epoch=10时调整一次，epoch=30时调整一次，epoch=80时调整一次…</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_shceduler.MultiStepLR(optimizer, milestones, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>milestone(list): 一个列表参数，表示多个学习率需要调整的epoch值，如milestones=[10, 30, 80].</li><li>其它参数同(1)。<br /><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635994.png" alt="img" /></li></ul><h3 id="213指数衰减调整学习率-exponentiallr"><a class="markdownIt-Anchor" href="#213指数衰减调整学习率-exponentiallr"></a> 2.1.3.指数衰减调整学习率 ExponentialLR</h3><p>学习率呈指数型衰减，每训练一个epoch，lr=lr×γepoch</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>gamma(float)：学习率调整倍数的底数，指数为epoch，初始值我lr, 倍数为γepoch</li><li>其它参数同上。<br /><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635065.png" alt="img" /></li></ul><h3 id="214余弦退火函数调整学习率"><a class="markdownIt-Anchor" href="#214余弦退火函数调整学习率"></a> 2.1.4.余弦退火函数调整学习率</h3><p>学习率呈余弦函数型衰减，并以2×Tmax为余弦函数周期，epoch=0对应余弦型学习率调整曲线的x=0，ymax=lr，epoch=Tmax对应余弦型学习率调整曲线的x=Π，ymin=etamin处，随着epoch&gt;Tmax，学习率随epoch增加逐渐上升，整个走势同cos(x)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=<span class="number">0</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>Tmax(int):学习率下降到最小值时的epoch数，即当epoch=T_max时，学习率下降到余弦函数最小值，当epoch&gt;T_max时，学习率将增大；</li><li>etamin: 学习率调整的最小值，即epoch=Tmax时，lrmin=etamin, 默认为0.</li><li>其它参数同上。<br /><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635073.png" alt="img" /></li></ul><h2 id="22根据指标调整学习率reducelronplateau"><a class="markdownIt-Anchor" href="#22根据指标调整学习率reducelronplateau"></a> 2.2.根据指标调整学习率ReduceLROnPlateau</h2><p>当<strong>某指标(loss或accuracy)在最近几个epoch中都没有变化(下降或升高超过给定阈值)时</strong>，调整学习率。<br />如当验证集的loss不再下降是，调整学习率；或监察验证集的accuracy不再升高时，调整学习率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">&#x27;min&#x27;</span>, factor=<span class="number">0.1</span>,</span><br><span class="line"> patience=<span class="number">10</span>,verbose=<span class="literal">False</span>, threshold=<span class="number">0.0001</span>, threshold_mode=<span class="string">&#x27;rel&#x27;</span>, cooldown=<span class="number">0</span>, </span><br><span class="line"> min_lr=<span class="number">0</span>, eps=<span class="number">1e-08</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>mode(str): 模式选择，有min和max两种模式，min表示当指标不再降低(如监测loss)，max表示当指标不再升高(如监测accuracy)。</li><li>factor(float): 学习率调整倍数，同前面的gamma，当监测指标达到要求时，lr=lr×factor。</li><li>patience(int): 忍受该指标多少个epoch不变化，当忍无可忍时，调整学习率。</li><li>verbose(bool): 是否打印学习率信息，print( ‘Epoch {:5d} reducing learning rate of group {} to {:.4e}.’.format(epoch, i, new_lr), 默认为False, 即不打印该信息。</li><li>threshold_mode (str): 选择判断指标是否达最优的模式，有两种模式：rel 和 abs.<br />当threshold_mode == rel, 并且 mode == max时，dynamic_threshold = best * (1 + threshold);<br />当threshold_mode == rel, 并且 mode == min时，dynamic_threshold = best * (1 - threshold);<br />当threshold_mode == abs, 并且 mode == max时，dynamic_threshold = best + threshold;<br />当threshold_mode == abs, 并且 mode == min时，dynamic_threshold = best - threshold;<br />threshold(float): 配合threshold_mode使用。</li><li>cooldown(int): “冷却时间”，当调整学习率之后，让学习率调整策略冷静一下，让模型在训练一段时间，再重启监测模式</li><li>min_lr(float or list): 学习率下限，可为float，或者list，当有多个参数组时，可用list进行设置。</li><li>eps(float): 学习率衰减的最小值，当学习率的变化值小于eps时，则不调整学习率。</li></ul><h2 id="23自定义调整学习率"><a class="markdownIt-Anchor" href="#23自定义调整学习率"></a> 2.3.自定义调整学习率</h2><p>为不同参数组设定不同学习率调整策略。调整规则为：<br />lr = base_lr * lambda(self.last_epoch)<br />在fine-tune中特别有用，<strong>我们不仅可以为不同层设置不同的学习率，还可以为不同层设置不同的学习率调整策略。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>lr_lambda(function or list): 自定义计算学习率调整倍数的函数，通常时epoch的函数，当有多个参数组时，设为list.</li><li>其它参数同上。</li></ul><h1 id="3手动调整学习率"><a class="markdownIt-Anchor" href="#3手动调整学习率"></a> 3.手动调整学习率</h1><p>手动调整学习率，通常可以定义如下函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sets the learning rate to the initial LR decayed by 10 every 30 epochs&quot;&quot;&quot;</span></span><br><span class="line">    lr = args.lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>))</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>又如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_learning_rate</span>(<span class="params">epoch, lr</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch &lt;= <span class="number">81</span>:  <span class="comment"># 32k iterations</span></span><br><span class="line">      <span class="keyword">return</span> lr</span><br><span class="line">    <span class="keyword">elif</span> epoch &lt;= <span class="number">122</span>:  <span class="comment"># 48k iterations</span></span><br><span class="line">      <span class="keyword">return</span> lr/<span class="number">10</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> lr/<span class="number">100</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>该函数通过修改每个epoch下，各参数组中的lr来进行学习率手动调整，用法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    lr = adjust_learning_rate(optimizer, epoch)  <span class="comment"># 调整学习率</span></span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">    ......</span><br><span class="line">    optimizer.step()  <span class="comment"># 采用新的学习率进行参数更新</span></span><br></pre></td></tr></table></figure><p>梯度下降算法需要我们指定一个学习率作为权重更新步幅的控制因子，常用的学习率有0.01、0.001以及0.0001等，学习率越大则权重更新。一般来说，<strong>我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些</strong>，使得网络更好的收敛到最优解。下图展示了随着迭代的进行动态调整学习率的4种策略曲线：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635068.jpg" alt="img" /></p><p>上述4种策略为自己根据资料整理得到的衰减类型：指数衰减、固定步长的衰减、多步长衰、余弦退火衰减。下面逐一介绍其性质，及pytorch对应的使用方式，需要注意学习率衰减策略很大程度上是<strong>依赖于经验与具体问题的</strong>，不能照搬参数。</p><p>*<strong>1、指数衰减*</strong></p><p>学习率按照指数的形式衰减是比较常用的策略，我们首先需要确定需要针对哪个优化器执行学习率动态调整策略，也就是首先定义一个优化器：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_ExpLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br></pre></td></tr></table></figure><p>定义好优化器以后，就可以给这个优化器绑定一个指数衰减学习率控制器：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ExpLR = torch.optim.lr_scheduler.ExponentialLR(optimizer_ExpLR, gamma=0.98)</span><br></pre></td></tr></table></figure><p>其中<strong>参数gamma表示衰减的底数，选择不同的gamma值可以获得幅度不同的衰减曲线</strong>，如下：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635071.jpg" alt="img" /></p><p>*<strong>2、固定步长衰减*</strong></p><p>有时我们希望学习率每隔一定步数（或者epoch）就减少为原来的gamma分之一，使用固定步长衰减依旧先定义优化器，再给优化器绑定StepLR对象：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_StepLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">StepLR = torch.optim.lr_scheduler.StepLR(optimizer_StepLR, step_size=step_size, gamma=0.65)</span><br></pre></td></tr></table></figure><p>其中gamma参数表示衰减的程度，step_size参数表示每隔多少个step进行一次学习率调整，下面对比了不同gamma值下的学习率变化情况：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635399.jpg" alt="img" /></p><p>*<strong>3、多步长衰减*</strong></p><p>上述固定步长的衰减的虽然能够按照固定的区间长度进行学习率更新**，但是有时我们希望不同的区间采用不同的更新频率，或者是有的区间更新学习率，有的区间不更新学习率**，这就需要使用MultiStepLR来实现动态区间长度控制：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_MultiStepLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">torch.optim.lr_scheduler.MultiStepLR(optimizer_MultiStepLR,</span><br><span class="line">                    milestones=[200, 300, 320, 340, 200], gamma=0.8)</span><br></pre></td></tr></table></figure><p>其中milestones参数为表示学习率更新的起止区间，在区间[0. 200]内学习率不更新，而在[200, 300]、[300, 320]…[340, 400]的右侧值都进行一次更新；gamma参数表示学习率衰减为上次的gamma分之一。其图示如下：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635419.jpg" alt="img" /></p><p>从图中可以看出，学习率在区间[200， 400]内快速的下降，这就是milestones参数所控制的，在milestones以外的区间学习率始终保持不变。</p><p>*<strong>4、余弦退火衰减*</strong></p><p>严格的说，余弦退火策略不应该算是学习率衰减策略，因为它使得学习率按照周期变化，其定义方式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_CosineLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">CosineLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_CosineLR, T_max=150, eta_min=0)</span><br></pre></td></tr></table></figure><p>其包含的参数和余弦知识一致，参数T_max表示余弦函数周期；eta_min表示学习率的最小值，默认它是0表示学习率至少为正值。确定一个余弦函数需要知道最值和周期，其中周期就是T_max，最值是初试学习率。下图展示了不同周期下的余弦学习率更新曲线：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635437.jpg" alt="img" /></p><p>*<strong>5、上述4种学习率动态更新策略的说明*</strong></p><p>4个负责学习率调整的类：StepLR、ExponentialLR、MultiStepLR和CosineAnnealingLR，其完整对学习率的更新都是在其step()函数被调用以后完成的，这个step表达的含义可以是一次迭代，当然更多情况下应该是一个epoch以后进行一次scheduler.step()，这根据具体问题来确定。此外，根据pytorch官网上给出的说明，scheduler.step()函数的调用应该在训练代码以后：</p><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">scheduler = ...</span><br><span class="line">&gt;&gt;&gt; for epoch in range(100):</span><br><span class="line">&gt;&gt;&gt;     train(...)</span><br><span class="line">&gt;&gt;&gt;     validate(...)</span><br><span class="line">&gt;&gt;&gt;     scheduler.step()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch环境</title>
      <link href="/2023/12/19/Programmer/python/torch%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
      <url>/2023/12/19/Programmer/python/torch%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191226420.png" alt="image-20231219122612389" /></p><h1 id="1安装对应的torch-torchvision"><a class="markdownIt-Anchor" href="#1安装对应的torch-torchvision"></a> 1.安装对应的torch、torchvision</h1><p>网址：<a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a></p><p>搜索对应CUDA版本的安装命令（cu110代表CUDA11.0），在终端中复制命令安装。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191135284.png" alt="image-20231219113547226" /></p><p>查看是否安装成功</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__) </span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) </span><br></pre></td></tr></table></figure><h1 id="2安装torch-geometric"><a class="markdownIt-Anchor" href="#2安装torch-geometric"></a> 2.安装torch-geometric</h1><p>网址：<a href="https://pytorch-geometric.com/whl/">https://pytorch-geometric.com/whl/</a></p><p>找到对应pytorch版本：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191137168.png" alt="image-20231219113754141" /></p><p>四个库（cluster,scatter,sparse,spline-conv）分别：wget 网页中对应的链接并 pip install 下载好的whl包，即完成安装：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191139386.png" alt="image-20231219113927345" /></p><p>注意自己环境的python版本以及linux/win就行</p><p>安装完上面四个库后执行 pip install torch-geometric</p><p>以上安装完成。</p><p>完成之后 import torch-geometric 发现报错，报错信息：<strong>“No module named 'torch.profiler”</strong></p><p>原因是torch1.10以上的版本才有<strong>torch.profiler</strong>这个库，但是Torch网址CUDA11.0兼容的选项没有torch1.10以上，那怎么办呢？</p><p>解决：</p><p>找到报错路径里的文件<strong><a href="http://profile.py">profile.py</a></strong></p><p>作如下修改：（原文件是第八行，改成了第九行）</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191140609.png" alt="image-20231219114012578" /></p><h1 id="3-dgl安装"><a class="markdownIt-Anchor" href="#3-dgl安装"></a> 3 DGL安装</h1><p>安装DGL无需安装torch-geometric，需要安装那四个依赖库</p><p><a href="https://www.dgl.ai/pages/start.html">Deep Graph Library (dgl.ai)</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n mVul python=3.7</span><br><span class="line"></span><br><span class="line">pip install torch==1.5.0+cu102 torchvision==0.6.0+cu102 torchaudio==0.5.0 -f https://download.pytorch.org/whl/cu102/torch_stable.html</span><br><span class="line"></span><br><span class="line">torch 1.5.0</span><br><span class="line">torchgeometric</span><br><span class="line">pip install networkx==2.5</span><br><span class="line">pip install dgl -f https://data.dgl.ai/wheels/cu102/repo.html</span><br><span class="line">https://data.dgl.ai/wheels/cu113/repo.html</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python环境</title>
      <link href="/2023/12/19/Programmer/python/python%E7%8E%AF%E5%A2%83/"/>
      <url>/2023/12/19/Programmer/python/python%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h1 id="1-conda虚拟环境"><a class="markdownIt-Anchor" href="#1-conda虚拟环境"></a> 1 conda虚拟环境</h1><h4 id="conda常用命令"><a class="markdownIt-Anchor" href="#conda常用命令"></a> conda常用命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda list # 查看当前虚拟环境已经安装的包（激活虚拟环境后使用）</span><br><span class="line">conda env list # 查看当前存在哪些虚拟环境</span><br><span class="line">conda update # conda 检查更新当前conda</span><br></pre></td></tr></table></figure><h4 id="conda创建虚拟环境"><a class="markdownIt-Anchor" href="#conda创建虚拟环境"></a> conda创建虚拟环境</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n xxx python=3.6</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx为所创建虚拟环境的名字</span></span><br></pre></td></tr></table></figure><h4 id="conda激活和退出虚拟环境windows"><a class="markdownIt-Anchor" href="#conda激活和退出虚拟环境windows"></a> conda激活和退出虚拟环境（windows）</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate xx # (虚拟环境名称）</span><br><span class="line"></span><br><span class="line">conda deactivate </span><br></pre></td></tr></table></figure><h4 id="conda为当前虚拟环境安装新的包"><a class="markdownIt-Anchor" href="#conda为当前虚拟环境安装新的包"></a> conda为当前虚拟环境安装新的包</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install -n package_name==所需版本 #（版本不指定则默认最新版）</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可使用临时镜像安装加快速度，例如安装numpy：</span></span><br><span class="line"></span><br><span class="line">conda install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy</span><br></pre></td></tr></table></figure><h4 id="conda删除虚拟环境或者虚拟环境中的某个包"><a class="markdownIt-Anchor" href="#conda删除虚拟环境或者虚拟环境中的某个包"></a> conda删除虚拟环境或者虚拟环境中的某个包</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove -n name --all</span><br><span class="line">conda remove --name env_name package_name </span><br></pre></td></tr></table></figure><h4 id="conda环境复制"><a class="markdownIt-Anchor" href="#conda环境复制"></a> conda环境复制</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n new_name --clone path</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">path为所需要复制的环境路径，可根据conda <span class="built_in">env</span> list查看路径</span></span><br></pre></td></tr></table></figure><h1 id="2-安装依赖库"><a class="markdownIt-Anchor" href="#2-安装依赖库"></a> 2 安装依赖库</h1><h2 id="pip"><a class="markdownIt-Anchor" href="#pip"></a> pip</h2><p>pip 是最为广泛使用的 Python 包管理器，可以帮助我们获得最新的 Python 包并进行管理。常用命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install [package-name]              # 安装名为[package-name]的包</span><br><span class="line">pip install [package-name]==X.X         # 安装名为[package-name]的包并指定版本X.X</span><br><span class="line">pip install [package-name] --proxy=代理服务器IP:端口号         # 使用代理服务器安装</span><br><span class="line">pip install [package-name] --upgrade    # 更新名为[package-name]的包</span><br><span class="line">pip uninstall [package-name]            # 删除名为[package-name]的包</span><br><span class="line">pip list                                # 列出当前环境下已安装的所有包</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">代码示例：</span></span><br><span class="line">pip install spyder -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">-i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下面介绍常见的国内源镜像：</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清华：https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里云：http://mirrors.aliyun.com/pypi/simple/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">华中理工大学：http://pypi.hustunique.com/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">山东理工大学：http://pypi.sdutlinux.org/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">豆瓣：http://pypi.douban.com/simple/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="conda"><a class="markdownIt-Anchor" href="#conda"></a> conda</h2><p>conda 包管理器是 Anaconda 自带的包管理器，可以帮助我们在 conda 环境下轻松地安装各种包。相较于 pip 而言，conda 的通用性更强（不仅是 Python 包，其他包如 CUDA Toolkit 和 cuDNN 也可以安装），但 conda 源的版本更新往往较慢。常用命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install [package-name]        # 安装名为[package-name]的包</span><br><span class="line">conda install [package-name]=X.X    # 安装名为[package-name]的包并指定版本X.X</span><br><span class="line">conda update [package-name]         # 更新名为[package-name]的包</span><br><span class="line">conda remove [package-name]         # 删除名为[package-name]的包</span><br><span class="line">conda list                          # 列出当前环境下已安装的所有包</span><br><span class="line">conda search [package-name]         # 列出名为[package-name]的包在conda源中的所有可用版本</span><br></pre></td></tr></table></figure><p><strong>conda镜像</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看conda当前设置</span></span><br><span class="line">conda config --show channels</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重置默认镜像源</span></span><br><span class="line">conda config --remove-key channels</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除单个镜像源</span></span><br><span class="line">conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">国内镜像</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清华大学镜像</span></span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">中科大镜像</span></span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里镜像</span></span><br><span class="line">conda config --add channels http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 环境 </tag>
            
            <tag> pip </tag>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex OCR</title>
      <link href="/2023/12/18/Tools/Latex-OCR/"/>
      <url>/2023/12/18/Tools/Latex-OCR/</url>
      
        <content type="html"><![CDATA[<blockquote><p>LaTeX-OCR 是一个开源的光学字符识别（OCR）软件，专为 LaTeX 文档提供支持。其主要目的是帮助用户将扫描的文档转换为 LaTeX 编辑器可以使用的可编辑文本，从而方便进行修改、编辑和排版。</p></blockquote><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202206456.png" alt="image-20231219121553484" /></p><h1 id="1安装"><a class="markdownIt-Anchor" href="#1安装"></a> 1.安装</h1><p>LaTeX-OCR可以从源码进行安装，也可以直接用pip来安装，源码地址：<a href="https://github.com/lukas-blecher/LaTeX-OCR">https://github.com/lukas-blecher/LaTeX-OCR</a> ，这里直接使用pip安装，为了方便管理环境，使用conda创建虚拟环境。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n latex python=3.10</span><br><span class="line">conda activate latex</span><br><span class="line">pip install &quot;pix2tex[gui]&quot;</span><br><span class="line">pip install &quot;pix2tex[gui]&quot; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>注：使用pip清华镜像源更快哦~</p><h1 id="2启动与使用"><a class="markdownIt-Anchor" href="#2启动与使用"></a> 2.启动与使用</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在虚拟环境下执行</span><br><span class="line">pix2tex</span><br></pre></td></tr></table></figure><p>首次执行会下载依赖模型；</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202206433.png" alt="image-20231219000535903" /></p><p>期间可能报错，连接断开，尝试重试；</p><p>使用：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202206418.png" alt="image-20231219000831872" /></p><p>输入h 回车查看帮助：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202206473.png" alt="image-20231219000913782" /></p><p>可以看到windows或macos下可以非常丝滑地使用，只需要：</p><ul><li>截图或复制一个图片到memory，可以理解为复制到剪贴板；</li><li>回到终端按回车，即可看到公式：<img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202206447.png" alt="image-20231219001159694" /></li><li>复制内容到LaTex块即可；</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>V</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m i n\sum_{i=1}^{n}{\mathcal{L}}(f(G_{i},Y_{i}|V_{i}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">Ω</mi><mi>j</mi></msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">W</mi><mi>h</mi></msub><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">r</mi><mi>j</mi></msub><mo>⊙</mo><msub><mi mathvariant="normal">Ω</mi><mi>S</mi></msub><mo fence="true">)</mo></mrow><mo>+</mo><mrow><mo fence="true">[</mo><msub><mi mathvariant="normal">∇</mi><mi>h</mi></msub><msub><mi mathvariant="normal">e</mi><mi>j</mi></msub><mo>+</mo><msub><mi mathvariant="normal">b</mi><mi>h</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega_{j}=tanh({\bf W}_{h}\left({\bf r}_{j}\odot\Omega_{S}\right)+\left[\nabla_{h}\mathrm{e}_{j}+\mathrm{b}_{h}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord">Ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">r</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord">Ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathrm">e</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span></p><p>工具很好用，无限制，非常良心，简直是福祉。</p>]]></content>
      
      
      <categories>
          
          <category> Tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git修改remote地址</title>
      <link href="/2023/12/18/Programmer/git/git%E4%BF%AE%E6%94%B9remote%E5%9C%B0%E5%9D%80/"/>
      <url>/2023/12/18/Programmer/git/git%E4%BF%AE%E6%94%B9remote%E5%9C%B0%E5%9D%80/</url>
      
        <content type="html"><![CDATA[<p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312202212006.png" alt="image-20231220221159980" /></p><h1 id="git修改remote地址"><a class="markdownIt-Anchor" href="#git修改remote地址"></a> git修改remote地址</h1><p>方式1、直接修改：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote set-url origin xxxxx.git</span><br></pre></td></tr></table></figure><p>方式2、先删后加 ：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm origin</span><br><span class="line">git remote add origin xxxxx.git</span><br></pre></td></tr></table></figure><p>修改默认pull和push分支：</p><p>git branch --set-upstream-to=origin/develop develop<br /><code>origin/develop develop</code>为要设置的默认分支</p><h4 id="给本地和远程仓库重命名"><a class="markdownIt-Anchor" href="#给本地和远程仓库重命名"></a> 给本地和远程仓库重命名</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 1.重命名本地分支</span><br><span class="line">git branch -m new-name  #如果当前在要重命名的分支</span><br><span class="line">git branch -m old-name new-name #如果当前不在要重命名的分支</span><br><span class="line"></span><br><span class="line">### 2.删除远程旧名称分支并且push新名称分支</span><br><span class="line">git push origin :old-name new-name</span><br><span class="line"></span><br><span class="line">### 3.关联新名称的本地分支和远程分支</span><br><span class="line"> git push origin -u new-name123456789</span><br></pre></td></tr></table></figure><h3 id="修改远程仓库地址"><a class="markdownIt-Anchor" href="#修改远程仓库地址"></a> 修改远程仓库地址</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote set-url origin [url]1</span><br></pre></td></tr></table></figure><h4 id="分别查看仓库-local-global-system-的配置信息"><a class="markdownIt-Anchor" href="#分别查看仓库-local-global-system-的配置信息"></a> 分别查看仓库 local global system 的配置信息</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --local --list</span><br><span class="line">git config --global --list</span><br><span class="line">git config --system --list123</span><br></pre></td></tr></table></figure><h4 id="仓库配置增加用户"><a class="markdownIt-Anchor" href="#仓库配置增加用户"></a> 仓库配置增加用户</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --local --add user.name yourname</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LineVD_Statement-level Vulnerability Detection using Graph Neural Networks</title>
      <link href="/2023/12/18/Papers/Vul/LineVD-Statement-level-Vulnerability-Detection-using-Graph-Neural-Networks/"/>
      <url>/2023/12/18/Papers/Vul/LineVD-Statement-level-Vulnerability-Detection-using-Graph-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<h2 id="0-abstract"><a class="markdownIt-Anchor" href="#0-abstract"></a> 0 Abstract</h2><p>当前基于机器学习的软件漏洞检测方法主要在功能级别进行。然而，这些方法的一个关键限制是，它们没有指示导致漏洞的特定代码行。这限制了开发人员有效检查和解释学习模型预测的能力，这对于将基于机器学习的工具集成到软件开发工作流中至关重要。基于图的模型在功能级漏洞检测方面表现出了良好的性能，但其在语句级漏洞检测中的能力尚未得到广泛探索。**虽然通过可解释的人工智能解释功能级预测是一个很有前途的方向，但我们在这里从完全监督学习的角度来考虑语句级软件漏洞检测任务。**我们提出了一种新的深度学习框架LineVD，它将语句级漏洞检测定义为节点分类任务。**LineVD利用图神经网络和基于转换器的模型对原始源代码标记进行编码，从而利用语句之间的控制和数据依赖性。**特别是，通过解决函数级和语句级信息之间的冲突输出，LineVD显著提高了函数代码在没有漏洞状态的情况下的预测性能。我们针对从多个真实世界项目中获得的大量真实世界C/C++漏洞进行了广泛的实验，并证明F1分数比当前最先进的技术提高了105%.</p><h2 id="1-intro-or-overview"><a class="markdownIt-Anchor" href="#1-intro-or-overview"></a> 1 Intro or Overview</h2><h4 id="11-problem-and-challenge"><a class="markdownIt-Anchor" href="#11-problem-and-challenge"></a> 1.1 Problem and Challenge</h4><p>自动化SVD大致可分为两类：</p><p>（1）传统方法，包括静态和动态分析；</p><p>（2）数据驱动解决方案，利用数据挖掘和机器学习来预测软件漏洞；</p><p>尽管当前的数据驱动方法在识别软件漏洞方面取得了成功，但它们往往局限于粗粒度水平。模型输出通常为开发人员提供有限的预测结果验证和解释信息，导致在评估和缓解软件漏洞时付出额外努力。</p><p>许多SVD解决方案已经从文件级过渡到函数级或切片级预测，其他一些工作进一步利用补充信息，如提交级别的代码更改以及附带的日志消息，来构建预测模型。虽然目标是帮助从业者对有缺陷的代码进行优先级排序，但漏洞通常可以局限于几个关键行。因此，审查大型函数仍然可能是一个相当大的负担。</p><h4 id="12-motivation"><a class="markdownIt-Anchor" href="#12-motivation"></a> 1.2 Motivation</h4><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312182057555.png" alt="image-20231218205705483" style="zoom:67%;" /><p>为了节省空间，我们从一个较小的函数中选择一个漏洞，该函数包含Linux内核（CVE-2018-12896）中的整数溢出漏洞，该漏洞最终可能被利用来导致拒绝服务。通过显式语句级别的预测，可以更容易地解释为什么函数被预测为易受攻击（或者，验证预测是否错误）。语句级SVD模型将第22行上的加法赋值操作标记为最可疑的，该操作包含易受攻击的整数强制转换操作，使开发人员能够更有效地验证和减轻该漏洞。</p><p>先前的工作利用GNNExplainer来导出易受攻击的语句作为模型的解释，以展示在语句级的工作。然而，在我们的工作中，我们发现在对潜在的脆弱性语句进行分类和排序时，性能是不充分和有效的。或者，我们旨在探索在语句级别直接训练和预测漏洞以进行SVD粒度细化的可行性和有效性，这将允许数据驱动的解决方案以完全监督的方式直接利用任何可用的语句级别信息。</p><h4 id="13-contribution"><a class="markdownIt-Anchor" href="#13-contribution"></a> 1.3 Contribution</h4><ul><li>提出了一种新颖有效的语句级SVD方法，LineVD实现了显著的改进，F1得分增加了105%；</li><li>研究了构建基于GNN的语句级SVD模型的每个阶段的性能影响，包括节点嵌入方法和GNN模型选择。根据研究结果，开发LineVD是为了通过同时学习功能和语句级别的信息，在很大程度上提高性能。</li><li>LineVD是第一种通过图神经网络联合学习函数级和语句级信息以提高SVD性能的方法，在经验评估中，它显著优于仅使用一种类型信息的传统模型。</li><li>发布了数据集、源代码和带有支持脚本的模型，这为未来的基准测试和比较工作提供了一个现成的实现解决方案。<a href="https://github.com/davidhin/linevd">https://github.com/davidhin/linevd</a></li></ul><h2 id="2-architecture-method"><a class="markdownIt-Anchor" href="#2-architecture-method"></a> 2 Architecture &amp; Method</h2><h4 id="21-system-overview"><a class="markdownIt-Anchor" href="#21-system-overview"></a> 2.1 System Overview</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312182110490.png" alt="image-20231218211008424" /></p><p>首先将问题定义为，节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">V\rightarrow Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>的映射，也就是语句是否易受攻击。</p><p>通过学习最小化损失loss:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>V</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">min\sum_{i=1}^nL(f(G_i, Y_i|V_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p><strong>Feature Extraction</strong></p><p>LineVD将源代码的单个函数作为原始输入。通过处理函数并将其拆分为单独的语句Vi，首先通过CodeBERT的预训练BPE标记器对每个样本进行标记。在V＝{V1，V2，…，Vn}的集合之后，整个函数和包括该函数的各个语句被传递到CodeBERT中。因此，可以获得函数级和语句级的代码表示。</p><p>具体而言，LineVD分别嵌入了函数级和语句级代码，而不是为函数级嵌入聚合语句级嵌入。CodeBERT是一个双峰模型，这意味着除了函数代码本身之外，它还基于函数的自然语言描述进行了训练。作为输入，它使用一个特殊的分隔符标记来区分自然语言描述和函数代码。虽然函数的自然语言描述是不可访问的，但在这项工作中应用了文献中规定的一般操作，在每个输入前添加一个额外的分隔符标记，使描述为空白。对于CodeBERT的输出，我们使用了分类标记的嵌入，这适用于代码摘要任务。这使我们能够更好地利用CodeBERT模型强大的预训练源代码摘要功能。</p><p>总体而言，使用CodeBERT的LineVD的特征提取组件产生n+1个特征嵌入：一个嵌入用于整个函数，n个嵌入用于每个语句，我们分别表示为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mi>v</mi><mo>=</mo><mrow><msubsup><mi>x</mi><mn>1</mn><mi>v</mi></msubsup><mi mathvariant="normal">，</mi><msubsup><mi>x</mi><mn>2</mn><mi>v</mi></msubsup><mi mathvariant="normal">，</mi><mo>…</mo><mi mathvariant="normal">，</mi><msubsup><mi>x</mi><mi>n</mi><mi>v</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">Xv={x^v_1，x^v_2，…，x^v_n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9125em;vertical-align:-0.24810799999999997em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">，</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>。</p><p><strong>Graph Construction</strong></p><p>在LineVD中，我们专注于数据和控制相关性信息，为此我们引入了图注意力网络（GAT）模型。如第2.2节所述，图神经网络（GNN）基于信息扩散机制学习图结构数据，而不是将信息压缩成平面向量，平面向量根据图的连通性更新节点状态，以保留重要信息，即拓扑依赖信息。</p><p><strong>Classifier Learning</strong></p><p>目标是训练一个可以同时从函数级和语句级代码中联合学习的模型。为了实现这一点，我们认为函数级和语句级代码片段对预测结果的贡献相等。因此，我们利用函数级CodeBERT嵌入的输入和从GAT层获得的语句嵌入，构建了一组共享的线性层和dropout层。</p><p>虽然易受攻击的语句可能足以指示函数易受攻击，但我们使用元素乘法构建LineVD，以进一步利用函数级别的信息进行训练。此外，这种操作和谐地平衡了函数级和语句级嵌入之间的冲突输出，并将证明某些场景的决策是合理的，即，如果函数级嵌入的输出类为零，那么所有语句级输出也为零。对此的直觉是，非易受攻击的函数不可能具有易受攻击的代码行。</p><h2 id="3-experiment-and-evaluation"><a class="markdownIt-Anchor" href="#3-experiment-and-evaluation"></a> 3 Experiment and Evaluation</h2><h4 id="31-dataset-and-process"><a class="markdownIt-Anchor" href="#31-dataset-and-process"></a> 3.1 DataSet and Process</h4><p>最近的研究表明，SVD模型应该根据能够代表真实世界漏洞不同特征的数据进行评估[10]。这意味着评估从真实世界项目中提取的源代码（即非合成的），同时保持不平衡的比率，这是软件项目中漏洞固有的。在现实世界场景中应用时，使用不满足这些条件的数据集会导致模型性能的不一致。另一个数据集要求是足够多的样本，理想情况下跨越多个项目，以便获得一个可以很好地推广到看不见的代码的模型。最后一个要求是在语句级别访问基本事实标签，或可追溯到修复前代码，即原始gitcommit。</p><p><strong>Ground-Truth labels</strong></p><p>为了获得易受攻击和非易受攻击线路的基本事实标签，我们遵循文献[19，32]中的断言，而不是提出我们自己的启发式方法：（1）漏洞修复提交中删除的线路用作易受攻击的线路的指标，以及（2）所有依赖于添加线路的控制或数据的线路也被视为易受攻击。第二点的理由是，在漏洞修复提交中添加的任何行都是为了帮助修补漏洞。因此，在漏洞修复提交中未修改但与这些添加的行相关的行可以被视为与漏洞相关。为了获得与依赖于添加行的行相对应的标签，我们首先获得样本前后版本的代码变化，其中Big Vul中的样本指的是函数级代码片段。对于之前的版本，我们删除所有添加的行，对于之后的版本，删除所有删除的行。在这两种情况下，我们都保留空白占位符行，以确保行号的一致性。从后版本中提取的代码图可用于查找所有依赖于添加行的控制或数据行，这些行的行号对应于前版本。这组线可以与删除的线组合，以获得单个样本的最终脆弱线集。注释行被排除在代码图中，因此不用于训练或预测。这可以从图1和图2中看出；在这种情况下，只有一个修改的行，它被视为删除的行（22）和添加的行（23）。在这种情况下，前后版本的控制和数据依赖边恰好相同，因此我们可以使用图2来识别依赖于第23行的控制/数据的行，即第3、19和21行。</p><p><strong>Cleaning</strong></p><p>原始数据集中的一些样本被错误地截断，导致代码样本无法解析且无效。例如，一个原本是50行的函数可能会因为没有明显原因而被错误地截断为40行。原因可能是数据集最初是如何构建的错误；然而，在整个数据集中只有30个这样的样本。我们使用80:10:10的随机训练/验证/测试分割比。对于训练集，我们对不可破坏样本的数量进行了不足采样，以在函数级别生成近似平衡的数据集，而测试和验证集保持原始的不平衡比例。我们选择在函数级别上平衡样本，因为在语句级别上进行平衡，同时保持函数中语句之间的上下文依赖关系是非常重要的。</p><h4 id="32-evaluation"><a class="markdownIt-Anchor" href="#32-evaluation"></a> 3.2 <strong>Evaluation</strong></h4><ul><li><p>RQ1：与最先进的基于解释的SVD模型相比，LineVD可以实现多大的性能提升？</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190937348.png" alt="image-20231219093737314" /></p></li><li><p>RQ2：不同的代码嵌入方法如何影响语句级漏洞检测？与其他粒度级别的SVD相比，语句级SVD的代码嵌入方法尚未得到探索。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190940483.png" alt="" /></p></li><li><p>RQ3：图神经网络和函数级信息如何对LineVD性能做出贡献？使用图神经网络的信息传播对语句级SVD的影响还有待探索。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190942923.png" alt="" /></p></li><li><p>RQ4：LineVD在跨项目分类场景中的表现如何？虽然在包含多个项目的数据集上进行训练已经减少了对模型通用性的歪曲，但来自同一项目的样本仍然有可能出现在训练集和测试集中。使用跨项目场景可以更好地表示模型在完全看不见的项目上的表现，而不仅仅是看不到的样本。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190943968.png" alt="image-20231219094311928" /></p></li><li><p>RQ5：对于真实世界的数据，LineVD最能区分哪些语句类型？从语句类型的角度研究模型预测结果，特别是对于真实世界的数据，可以帮助了解模型在哪里表现最好，在哪里失败，这可以指导未来的工作和语句级SVD的改进。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190943357.png" alt="image-20231219094343320" /></p></li></ul><h2 id="4-conclusion"><a class="markdownIt-Anchor" href="#4-conclusion"></a> 4 Conclusion</h2><p>LineVD，一种用于语句级漏洞检测的新型深度学习方法，它可以让开发人员更有效地评估潜在的漏洞功能。LineVD通过在训练过程中利用图神经网络和语句级信息，在真实世界的开源项目中实现了最先进的语句级漏洞检测。与最新的基于细粒度机器学习的模型相比，这一显著改进表明了直接利用语句级信息进行语句级SVD的有效性。最后，LineVD实现了合理的跨项目性能，表明即使对于完全看不见的软件项目，它也具有有效性和泛化能力。未来的方向将包括探索替代的预训练特征嵌入方法和新的GNN架构，这些架构可以更好地适应软件源代码的底层性质和漏洞。</p><h2 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h2><aside> 💡 Others<hr />]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> Vulnerabilities </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安全 </tag>
            
            <tag> Vulnerabilities </tag>
            
            <tag> 代码行级检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo bug</title>
      <link href="/2023/12/16/hello-bug/"/>
      <url>/2023/12/16/hello-bug/</url>
      
        <content type="html"><![CDATA[<p>这是一个bug指南，在使用Hexo的时候遇到了一些bug，自己在这里对解决过的bug做以记录。</p><h4 id="nunjucks-error-line-26-column-109-parseaggregate-expected-comma-after-expression"><a class="markdownIt-Anchor" href="#nunjucks-error-line-26-column-109-parseaggregate-expected-comma-after-expression"></a> Nunjucks Error: [Line 26, Column 109] parseAggregate: expected comma after expression</h4><p>这个自己是hexo在render公式的时候产生的问题，不能两个”{“紧接着放在一起，应该{ {…} }在<strong>之间加空格</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 指南 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/16/hello-world/"/>
      <url>/2023/12/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>这是一个指南</p><p><a href="https://gavinblog.github.io/anzhiyu-docs/">安知鱼主题指南 (gavinblog.github.io)</a></p><h3 id="布局layout"><a class="markdownIt-Anchor" href="#布局layout"></a> 布局（Layout）</h3><p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和 <code>draft</code>。在创建这三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 <code>post</code> 相同，都将储存到 <code>source/_posts</code> 文件夹。</p><table><thead><tr><th style="text-align:left">布局</th><th style="text-align:left">路径</th></tr></thead><tbody><tr><td style="text-align:left"><code>post</code></td><td style="text-align:left"><code>source/_posts</code></td></tr><tr><td style="text-align:left"><code>page</code></td><td style="text-align:left"><code>source</code></td></tr><tr><td style="text-align:left"><code>draft</code></td><td style="text-align:left"><code>source/_drafts</code></td></tr></tbody></table><h1 id="1-常用命令"><a class="markdownIt-Anchor" href="#1-常用命令"></a> 1 常用命令</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># draft</span><br><span class="line">hexo new draft &quot;test&quot;</span><br><span class="line"></span><br><span class="line">hexo publish draft &quot;test&quot;</span><br></pre></td></tr></table></figure><h1 id="2-公式"><a class="markdownIt-Anchor" href="#2-公式"></a> 2 公式</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm un hexo-renderer-marked --save</span><br><span class="line"># or</span><br><span class="line">npm un hexo-renderer-kramed --save</span><br><span class="line"># 安装 `hexo-renderer-markdown-it-plus`</span><br><span class="line">npm i @upupming/hexo-renderer-markdown-it-plus --save</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在根目录的 _config.yml 中使用下面的配置将 strict 设置为 false</span></span><br><span class="line"><span class="attr">markdown_it_plus:</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">plugin:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">&#x27;@neilsustc/markdown-it-katex&#x27;</span></span><br><span class="line">      <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">strict:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 指南 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的审稿意见</title>
      <link href="/2023/12/16/Manuscripts/review/"/>
      <url>/2023/12/16/Manuscripts/review/</url>
      
        <content type="html"><![CDATA[<h2 id="intelligent-vulnerability-detector-using-deep-sequence-and-graph-based-hybrid-feature-extraction"><a class="markdownIt-Anchor" href="#intelligent-vulnerability-detector-using-deep-sequence-and-graph-based-hybrid-feature-extraction"></a> Intelligent Vulnerability Detector using deep sequence and graph based Hybrid Feature Extraction</h2><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191506692.png" alt="image-20231210113010116" /></p><p>This manuscript proposed a graph-based and sequence-based neural network model for detecting vulnerabilities in Java code, utilizing multiple program features，which addresses the detection problem of a range of vulnerabilities collected from the Common Weakness Enumeration (CWE) . It introduces GCN-RFEMLP for extracting graph-based features and employs CodeBERT for extracting sequence-based features. However, there are some critical issues outlined in the manuscript making the referee has to reject it.</p><p>Comments：</p><ol><li>The COVID-19 pandemic, which is unrelated to the research and should not have been mentioned.</li><li>The manuscript presents a list of seven contributions; however, they lack conciseness and do not effectively emphasize the primary contributions.</li><li>Certain figures in the manuscript are not appropriate. Figure 1 appears to be more focused on the classification of machine learning methods and lacks contextual relevance, considering that the manuscript is specifically about vulnerability detection. Other figures also suffer from similar issues, as they seem to be detached from vulnerability detection and lack any meaningful connection. Figures 3 and 4 depict the node2vec process and GCN, respectively. However, these figures are not relevant to the vulnerability detection discussed in the paper and do not contribute to the study. Instead, the figures should focus on illustrating the transformation process from source code to code property graph, highlighting the comprehensive model proposed in the manuscript.</li><li>In the experimental section, the formatting of the tables presenting the experimental results lacks consistency. And, it is customary to report experimental results with two decimal places, such as 98.90. It is important to ensure that other result comparison data follow the same formatting convention.</li><li>The dataset description in the manuscript lacks clarity, and there is no mention of the labeling process for the data. Additionally, the comparison with other benchmarks does not indicate the dataset that was utilized.</li><li>Moreover, the manuscript lacks relevant explanations and approaches for addressing data imbalance, which can pose a risk of overfitting.</li></ol><h2 id="vuldet-bc-binary-software-vulnerability-detection-based-on-bigru-and-cnn"><a class="markdownIt-Anchor" href="#vuldet-bc-binary-software-vulnerability-detection-based-on-bigru-and-cnn"></a> VulDet-BC: Binary Software Vulnerability Detection Based on BiGRU and CNN</h2><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191506699.png" alt="image-20231210113205646" /></p><p>this manuscript 提出了一种二进制漏洞检测方法VulDet-BC，从二进制机器指令级别，结合BiGRU与CNN构建二进制漏洞检测模型，其中利用了注意力机制，并且通过与一些基线的对比在一些指标上优于基线，However, there are some critical issues outlined in the manuscript making the referee has to reject it.</p><p>Comments:</p><ol><li>文章在相关工作部分对漏洞检测这一研究领域的介绍不够充分，近期的许多新颖的工作并没有被提及。</li><li>深度学习方法常常基于一定的漏洞模式，来实现漏洞检测。文稿中的方法将二进制机器指令转换为数字形式，再转换过程中并没有提出任何与漏洞模式相关的概念，无论在语义或是语法上；</li><li>文中提到的BiGRU结合注意力模块并不新颖，且文中实验部分所提出的一系列RQ，缺乏对漏洞成因的思考，仅仅是从深度学习的角度在进行消融实验；</li><li>BinVulDet，在文稿中是第40个引用，是比较新的工作，在伪代码级别检测二进制软件的漏洞，文稿既没有对其工作进行介绍（related work)也没有与其进行对比研究。源代码漏洞检测方法VulDeePecker使用了code-gadget结合BiLSTM构建漏洞检测模型，但是文中似乎使用后半部分的BiLSTM进行对比，这样的对比实验设计已经不是同VulDeePecker工作进行对比了，这显然是错误的；</li><li>文稿中缺乏对漏洞检测任务的误报和漏报的分析，即FNR和FPR，这在漏洞检测的工作中非常重要，且缺乏对真实世界的软件漏洞进行检测的实验研究，使实验中提出的RQ变得更加单薄，对论文的研究缺乏支撑度；</li></ol><p>This manuscript proposed a binary software vulnerability detection method called VulDet-BC, which operates at the binary machine instruction level. It employs a combination of BiGRU and CNN along with attention mechanisms to build a vulnerability detection model. The manuscript claims superiority over baselines in certain metrics. However, the manuscript has several critical issues outlined below, which led the referee to reject it.</p><ol><li>The related work of the paper lacks a comprehensive review of the research field of vulnerability detection. Many recent and innovative works in the field have not been mentioned.</li></ol><p>2.Deep learning methods often rely on some vulnerability patterns to achieve effective vulnerability detection. The proposed method in the manuscript converts binary machine instructions into numeric representations without introducing any concepts related to vulnerability patterns, either semantically or syntactically.</p><p>3.The combination of BiGRU and attention mechanisms mentioned in the paper is not novel. Additionally, the series of research questions(RQ) proposed in the experimental section lacks contemplation on the causes of vulnerabilities. The experiments conducted only focus on the impact of deep learning techniques.</p><p>4.“BinVulDet” is referenced as the 40th citation in the manuscript and represents a relatively recent work that focuses on detecting vulnerabilities in binary software at the pseudo code level. However, the manuscript fails to provide an introduction to this work in the related work section and does not compare it with the proposed method. The source code vulnerability detection method “VulDeePecker” utilizes code-gadgets combined with BiLSTM to build a vulnerability detection model. However, it seems that the manuscript incorrectly compares its method with only the latter part, BiLSTM, which is not a valid comparison to the original VulDeePecker work. This discrepancy in the experimental design is evidently an error.</p><p>5.The manuscript lacks an analysis of false negatives (FNR) and false positives (FPR), which are crucial in vulnerability detection. Furthermore, there is a lack of experiments on detecting real-world software vulnerabilities, making the proposed research questions less substantiated.</p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 审稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c语言混淆技术</title>
      <link href="/2023/12/16/Security/c%E8%AF%AD%E8%A8%80%E6%B7%B7%E6%B7%86%E6%8A%80%E6%9C%AF/"/>
      <url>/2023/12/16/Security/c%E8%AF%AD%E8%A8%80%E6%B7%B7%E6%B7%86%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<h4 id="代码混淆定义"><a class="markdownIt-Anchor" href="#代码混淆定义"></a> 代码混淆定义：</h4><p>原代码 P 通过某种变换变成代码 P’，若 P 和 P’运行结果与过程行为保持一致，该种变换就称之为混淆变换。</p><p>具体来说，当混淆转换满足以下两种情况时，这种混淆变化称之为合法的转换：</p><ul><li>（1）如果源程序 P 无法停止运行或报错结束运行，则变换后的程序 P’可以结束运行也可以继续运行。</li><li>（2）否则，目标程序 P’也结束运行并且输出与源程序相同的结果。</li><li><mark>两个程序之间操作并不一定完全相同，且不一定有相同的效率。</mark></li></ul><p>实际上，混淆工具预先设定若干混淆规则，并使用其它更为复杂的代码取代源代码中符合条件的代码语句，<strong>虽然源代码语义并未改变但混淆后的程序运行过程中空间复杂度往往更高，执行时间也更长，甚至有可能改变系统环境</strong>等。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/imgs202312171007669.png" alt="image-20231217100733637" /></p><p>图 2.1 展示了混淆编译的整体流程，</p><ul><li>首先混淆工具会对输入的源代码进行代码预处理得到<mark>程序控制流图 CFG、抽象语法树 AST</mark> 等信息，</li><li>然后对<mark>数据流、控制流</mark>等进行分析，并根据输入的混淆参数选择对应的混淆算法处理源代码，</li><li>最后输出混淆编译后的程序。</li></ul><p>尽管混淆策略多种多样，但通常按 Collberg 提出的方法将其大致分为四类[16]：</p><ul><li>布局混淆</li><li>数据流混淆</li><li>控制流混淆</li><li>预防混淆</li></ul><p>接下来将对这几类混淆策略进行详细分析。</p><h4 id="布局混淆"><a class="markdownIt-Anchor" href="#布局混淆"></a> 布局混淆</h4><p>布局混淆是一种在不影响源程序正常运行的情况下，即<strong>不修改程序核心控制流和数据流</strong>，对程序包含有用信息的非核心代码做出修改的一种混淆策略；此处的非核心代码一般包括注释语句、多余代码片段、用于调试的代码语句以及自定义的变量名。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C/C++ </tag>
            
            <tag> 软件安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overleaf 葵花宝典</title>
      <link href="/2023/12/16/Manuscripts/overleaf/overleaf%E7%BB%8F%E9%AA%8C/"/>
      <url>/2023/12/16/Manuscripts/overleaf/overleaf%E7%BB%8F%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="1列表"><a class="markdownIt-Anchor" href="#1列表"></a> 1.列表</h1><h2 id="itemize命令无序列表"><a class="markdownIt-Anchor" href="#itemize命令无序列表"></a> <strong>{itemize}命令【无序列表】</strong></h2><blockquote><p>{itemize}命令对文本进行简单的排列，不是采用序号，默认是用实心圆点符号进行排列。这个命令需要和\item配合使用。</p></blockquote><p>默认为实心圆点符号</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">    <span class="keyword">\item</span> one</span><br><span class="line">    <span class="keyword">\item</span> two</span><br><span class="line">    <span class="keyword">\item</span> ...</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508804.png" alt="image-20231210154216456" /></p><p>使用其他符号进行排列</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">    <span class="keyword">\item</span>[*] one</span><br><span class="line">    <span class="keyword">\item</span>[*] two</span><br><span class="line">    <span class="keyword">\item</span>[*] ...</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508806.png" alt="image-20231210155010036" /></p><h2 id="enumerate命令有序列表"><a class="markdownIt-Anchor" href="#enumerate命令有序列表"></a> <strong>{enumerate}命令【有序列表】</strong></h2><blockquote><p>{enumerate}命令采用序号对文本进行简单的排列，默认是用1，2，3进行排列。这个命令需要和\item配合使用。</p></blockquote><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;enumerate&#125;</span><br><span class="line">    <span class="keyword">\item</span> one</span><br><span class="line">    <span class="keyword">\item</span> two</span><br><span class="line">    <span class="keyword">\item</span> ...</span><br><span class="line"><span class="keyword">\end</span>&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508812.png" alt="image-20231210155244000" /></p><p><strong>使用其他形式的编号</strong>：</p><blockquote><p>{enumerate}产生所需要的编号，默认是采用数字1,2,3……进行排列。</p><p><strong>使用命令\usepackage{enumerate}</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;[i)]</span><br><span class="line">    \item one</span><br><span class="line">    \item two</span><br><span class="line">    \item ...</span><br><span class="line">\end&#123;enumerate&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;enumerate&#125;[1)]</span><br><span class="line">    \item one</span><br><span class="line">    \item two</span><br><span class="line">    \item ...</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 自定义编号形式</span><br><span class="line"></span><br><span class="line">\begin&#123;description&#125;</span><br><span class="line">    \item[Step1] one</span><br><span class="line">    \item[Step2] two</span><br><span class="line">    \item[Step3] ...</span><br><span class="line">\end&#123;description&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191509540.png" alt="image-20231219150916511" /></p><h1 id="2三线表"><a class="markdownIt-Anchor" href="#2三线表"></a> 2.三线表</h1><p>使用方法1或者2都可以，两种latex编辑器WinEdt和TexStudio各有优点，看你选择，我用的是方法1，使用WinEdt。</p><p>直接显示latex 代码，然后你们根据自己的情况进行修改即可</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[h] %h表示三线表在当前位置插入</span><br><span class="line">\setlength&#123;\abovecaptionskip&#125;&#123;<span class="number">0.</span>05cm&#125; %设置三线表标题与第一条线间距</span><br><span class="line">\centering</span><br><span class="line">\caption&#123;\textbf&#123;The characteristics of various methods&#125;&#125;</span><br><span class="line">%表头文本加黑，但不加黑Table <span class="number">1.</span>字样，引入包即可：\usepackage[labelfont=bf]&#123;caption&#125;</span><br><span class="line">\arrayrulecolor&#123;black&#125; %设置三线表线条颜色：黑色</span><br><span class="line">\begin&#123;tabular*&#125;&#123;\hsize&#125;&#123;@&#123;\extracolsep&#123;\fill&#125;&#125;c c c c&#125; %&#123;\hsize&#125;使三线表自适应宽度，c表示文本居中</span><br><span class="line">  \hline</span><br><span class="line">  <span class="number">1</span> &amp; <span class="number">2</span> &amp; <span class="number">3</span> &amp; <span class="number">4</span>\\</span><br><span class="line">  \hline</span><br><span class="line">  <span class="number">11</span> &amp; <span class="number">22</span> &amp; <span class="number">33</span> &amp; <span class="number">44</span> \\</span><br><span class="line">  <span class="number">111</span> &amp; <span class="number">222</span> &amp; <span class="number">333</span> &amp; <span class="number">444</span> \\</span><br><span class="line">  <span class="number">1111</span> &amp; <span class="number">2222</span> &amp; <span class="number">3333</span> &amp; <span class="number">4444</span> \\</span><br><span class="line">  <span class="number">11111</span> &amp; <span class="number">22222</span> &amp; <span class="number">33333</span> &amp; <span class="number">44444</span> \\</span><br><span class="line">  <span class="number">111111</span> &amp; <span class="number">222222</span> &amp; <span class="number">333333</span> &amp; <span class="number">444444</span> \\</span><br><span class="line">  \hline</span><br><span class="line">\end&#123;tabular*&#125;</span><br><span class="line">\end&#123;table&#125;</span><br></pre></td></tr></table></figure><p>添加包：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;booktabs&#125;</span><br><span class="line">\usepackage&#123;amsmath&#125;</span><br><span class="line">\usepackage&#123;setspace&#125;</span><br><span class="line">\usepackage&#123;array,caption&#125;</span><br><span class="line">\usepackage[labelfont=bf]&#123;caption&#125;</span><br></pre></td></tr></table></figure><h1 id="3图片过大处理"><a class="markdownIt-Anchor" href="#3图片过大处理"></a> 3.图片过大处理</h1><p>在<a href="https://so.csdn.net/so/search?q=LaTeX%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87&amp;spm=1001.2101.3001.7020">LaTeX插入图片</a>的时候，经常需要调整图片的大小。我们可以通过如下代码来完成：</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[htb]</span><br><span class="line">  <span class="keyword">\centering</span></span><br><span class="line">  <span class="keyword">\includegraphics</span>[width=0.5<span class="keyword">\linewidth</span>]&#123;fig2.png&#125;</span><br><span class="line">  <span class="keyword">\caption</span>&#123;图片的解释&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>其中，width=0.5[linewidth](<a href="https://so.csdn.net/so/search?q=linewidth&amp;spm=1001.2101.3001.7020">https://so.csdn.net/so/search?q=linewidth&amp;spm=1001.2101.3001.7020</a>) 表明将插入的图像等比例缩小至0.6倍。经验证，调整比例后图像成功地缩小了。</p><p>这样可以适应模板 自动地调整大小 不用手动去调整长宽 非常好用</p><h1 id="4空格"><a class="markdownIt-Anchor" href="#4空格"></a> 4.空格</h1><p>quad空格a \quad b一个m的宽度<br />大空格a\ b1/3m宽度<br />中等空格a;b2/7m宽度<br />小空格a,b1/6m宽度<br />没有空格ab<br />紧贴a!b缩进1/6m宽度</p><h1 id="5latex的粗体"><a class="markdownIt-Anchor" href="#5latex的粗体"></a> 5.latex的粗体</h1><p>latTx的粗体一般用以下命令：</p><p>\textbf{}：文本环境加粗。在数学环境使用的话，会使斜体效果消失。并且无法输出加粗的希腊字母。</p><p>\mathbf{}：会变为粗体，但同样会导致数学字母斜体形式的丢失。 \boldmath{}：数学环境里可以加粗且不会使斜体消失。需要添加amsmath宏包。 \boldsymbol{}：可以对希腊字母加粗。需要添加amsmath宏包。 在数学环境中，比较推荐的方式是添加宏包\usepackage{bm}, 使用\bm{}命令加粗。</p><p>但是在xelatex或Luatex引擎的unicode-math环境中中，\bm{}会报错。此时，可以使用以下命令：</p><p>\symbfit{}：加粗，且有斜体效果 \symbf{}：加粗，没有斜体效果 \mathbfcal{}：加粗的\mathcal字体</p><p>[<a href="https://blog.csdn.net/xovee/article/details/106325136">翻译] [Overleaf] LaTeX 中的粗体、斜体、下划线_latex 斜体-CSDN博客</a></p><h1 id="6图片与引用"><a class="markdownIt-Anchor" href="#6图片与引用"></a> 6.图片与引用</h1><p>示例：</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure*&#125;</span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[scale=0.45]&#123;double<span class="built_in">_</span>single.eps&#125; <span class="comment">%scale=缩小比例，或者用width=2in</span></span><br><span class="line"><span class="keyword">\caption</span>&#123;Search&#125;     <span class="keyword">\label</span>&#123;fig:ss&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure*&#125;</span><br></pre></td></tr></table></figure><p>引用注意</p><p>\label{} 必须写在 \caption{} 的后面。</p><p>\ref{}:引用</p><p>\ref{fig:ss}, 即\ref{}, {}内为标签名称,我这里的标签名称是：fig:ss</p><h1 id="7宽度问题"><a class="markdownIt-Anchor" href="#7宽度问题"></a> 7.宽度问题</h1><p>\hsize: 是 Latex中定义的长度，是一种叫做水平盒子的长度，它的主要作用是告诉TeX系统什么时候换行。所以大部分时候和\textwidth是一致的，但是在分栏状况下，\hsize只是栏的宽度；</p><p>\textwidth: 是 Latex中定义的长度，等效于\hsize，并且是固定不变的，可以理解为一行文字的宽度。</p><p>\pagewidth: 包含了页边的宽度，比\textwidth要大</p><p>\linewidth: 这指得是目前环境的宽度，是依赖于上下文的一个宽度值，例如新建了一个box，在这个box中，</p><p>\linewidth是box中文字的宽度。再例如minipage环境中，\linewidth就和这个minipage的大小有关.</p><p>\columnwidth: 如果文章分栏的话，这个宽度就是每一栏的宽度。</p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> overleaf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overleaf 问题</title>
      <link href="/2023/12/16/Manuscripts/overleaf/overleaf%E9%97%AE%E9%A2%98%20/"/>
      <url>/2023/12/16/Manuscripts/overleaf/overleaf%E9%97%AE%E9%A2%98%20/</url>
      
        <content type="html"><![CDATA[<h1 id="algorithm最后出现0"><a class="markdownIt-Anchor" href="#algorithm最后出现0"></a> algorithm最后出现=0</h1><p><strong>解决方法：</strong></p><p>注释掉&quot;\usepackage{algpseudocode}&quot;</p><p>因为<code>\usepackage&#123;algpseudocode&#125; %This introduces extra zero at the end of algorithm</code></p><h1 id="table位置问题"><a class="markdownIt-Anchor" href="#table位置问题"></a> table位置问题</h1><p>如果table默认置顶，在.sty文件中定义了table环境，那么可尝试将</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[h] </span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[pos=h] </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> overleaf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computers&amp;Security</title>
      <link href="/2023/12/16/Manuscripts/sci/Computers&amp;Security/"/>
      <url>/2023/12/16/Manuscripts/sci/Computers&amp;Security/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www2.cloud.editorialmanager.com/cose/default2.aspx">Editorial Manager®</a></p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sci投稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ lamda表达式</title>
      <link href="/2023/12/16/Programmer/c-c++/lambda/"/>
      <url>/2023/12/16/Programmer/c-c++/lambda/</url>
      
        <content type="html"><![CDATA[<p>创建一个匿名函数并执行。Objective-C采用的是上尖号^，而C++ 11采用的是配对的方括号[]。实例如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    []&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Hello,Worldn&quot;</span>; </span><br><span class="line">    &#125;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们也可以方便的将这个创建的匿名函数赋值出来调用：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [](<span class="type">int</span> i) &#123; <span class="comment">// (int i) 是指传入改匿名函数的参数</span></span><br><span class="line">        cout &lt;&lt; i;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">func</span>(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="捕获选项"><a class="markdownIt-Anchor" href="#捕获选项"></a> 捕获选项</h1><ul><li>[] Capture nothing (or, a scorched earth strategy?)</li><li>[&amp;] Capture any referenced variable by reference</li><li>[=] Capture any referenced variable by making a copy</li><li>[=, &amp;foo] Capture any referenced variable by making a copy, but capture variable foo by reference</li><li>[bar] Capture bar by making a copy; don’t copy anything else</li><li>[this] Capture the this pointer of the enclosing class</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-[]不捕获任何变量</span><br><span class="line"></span><br><span class="line">-[&amp;]通过引用捕获任何引用变量</span><br><span class="line"></span><br><span class="line">-[=]通过复制捕获任何引用变量</span><br><span class="line"></span><br><span class="line">-[=，&amp;foo]通过复制捕获任何引用的变量，但通过引用捕获变量foo</span><br><span class="line"></span><br><span class="line">-[bar]通过复制来捕获bar；不要复制其他任何东西</span><br><span class="line"></span><br><span class="line">-[this]捕获封闭类的this指针</span><br></pre></td></tr></table></figure><h1 id="不捕获任何变量"><a class="markdownIt-Anchor" href="#不捕获任何变量"></a> [] 不捕获任何变量</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [] &#123; cout &lt;&lt; i; &#125;;</span><br><span class="line">    <span class="built_in">func</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>vs 报错<br />error C3493: 无法隐式捕获“i”，因为尚未指定默认捕获模式<br />error C2064: 项不会计算为接受 0 个参数的函数</p><p>g++ 报错：<br />error: ‘i’ is not captured</p><p>要直接沿用外部的变量需要在 [] 中指名捕获。</p><h1 id="拷贝捕获"><a class="markdownIt-Anchor" href="#拷贝捕获"></a> [=] 拷贝捕获</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [=]&#123;  <span class="comment">// [=] 表明将外部的所有变量拷贝一份到该函数内部</span></span><br><span class="line">        cout &lt;&lt; i;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">func</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br />1024</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [=]&#123;</span><br><span class="line">        <span class="comment">// fun1 内存在 i</span></span><br><span class="line">        cout &lt;&lt; i; <span class="comment">// 1024</span></span><br><span class="line">        <span class="keyword">auto</span> fun2 = []&#123; <span class="comment">// 未指名捕获， i 不存在</span></span><br><span class="line">            cout &lt;&lt; i;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="built_in">fun2</span>();</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="引用捕获"><a class="markdownIt-Anchor" href="#引用捕获"></a> [&amp;] 引用捕获</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    cout &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [&amp;]&#123;</span><br><span class="line">        cout &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果:<br />0x28ff0c<br />0x28ff0c</p><h1 id="拷贝与引用混合"><a class="markdownIt-Anchor" href="#拷贝与引用混合"></a> [=, &amp;] 拷贝与引用混合</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [=, &amp;i]&#123; <span class="comment">// 默认拷贝外部所有变量，但引用变量 i</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果<br />outside i:0x28ff0c<br />outside j:0x28ff08<br />inside i:0x28ff0c<br />inside j:0x28ff04</p><h1 id="bar-指定引用或拷贝"><a class="markdownIt-Anchor" href="#bar-指定引用或拷贝"></a> [bar] 指定引用或拷贝</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [i]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; j &lt;&lt; endl; // j 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br />outside i value:1024 addr:0x28ff08<br />inside i value:1024 addr:0x28ff04</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [&amp;i]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; j &lt;&lt; endl; // j 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br />outside i value:1024 addr:0x28ff08<br />inside i value:1024 addr:0x28ff08</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>, k;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [i, &amp;j]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; k; // k 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br />outside i:0x28ff0c<br />outside j:0x28ff08<br />inside i:0x28ff00<br />inside j:0x28ff08</p><h1 id="this-捕获-this-指针"><a class="markdownIt-Anchor" href="#this-捕获-this-指针"></a> [this] 捕获 this 指针</h1><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#include &lt;iostream&gt;</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="title">std</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title">test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">hello</span>()</span> &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;test hello!n&quot;</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">lambda</span>()</span> &#123;</span><br><span class="line">        auto fun = [<span class="keyword">this</span>]&#123; <span class="comment">// 捕获了 this 指针</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;hello(); <span class="comment">// 这里 this 调用的就是 class test 的对象了</span></span><br><span class="line">        &#125;;</span><br><span class="line">        fun();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">int</span> <span class="title">main</span>()</span></span><br><span class="line">&#123;</span><br><span class="line">    test t;</span><br><span class="line">    t.lambda();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C/C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C/C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Siamese Network</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/siamese/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/siamese/</url>
      
        <content type="html"><![CDATA[<h2 id="siamese网络"><a class="markdownIt-Anchor" href="#siamese网络"></a> Siamese网络</h2><h4 id="问题背景"><a class="markdownIt-Anchor" href="#问题背景"></a> 问题背景</h4><p>分类问题：</p><ul><li><p>分类数量较少，每一类的数据量较多，比如ImageNet、VOC等。这种分类问题可以使用神经网络或者SVM解决，只要事先知道了所有的类。</p></li><li><p>分类数量较多（或者说无法确认具体数量），每一类的数据量较少，比如人脸识别、人脸验证任务。</p></li></ul><p>解决方法：</p><ul><li>提出一种思路：将输入映射为一个特征向量，使用两个向量之间的距离来表示输入之间的差异，如图像语义上的差异。</li><li>Siamese网络，每次需要输入两个样本作为一个sample对计算损失函数。</li><li>提出Contrastive Loss用于训练。</li></ul><h4 id="应用场景"><a class="markdownIt-Anchor" href="#应用场景"></a> 应用场景</h4><p>孪生神经网络用于处理两个输入&quot;比较类似&quot;的情况。伪孪生神经网络适用于处理两个输入&quot;有一定差别&quot;的情况。比如，我们要计算两个句子或者词汇的语义相似度，使用siamese network比较适合；如果验证标题与正文的描述是否一致（标题和正文长度差别很大），或者文字是否描述了一幅图片（一个是图片，一个是文字），就应该使用pseudo-siamese network。也就是说，要根据具体的应用，判断应该使用哪一种结构，哪一种Loss。</p><h4 id="siamese创新点"><a class="markdownIt-Anchor" href="#siamese创新点"></a> Siamese创新点</h4><p>网络的创新点是淡化了标签，是的网络具有很好的扩展性，可以对那些没有训练过的类别进行分类，这一点优于很多算法。</p><p>该算法对一些小数据量的数据集也适用，变相地增加了整个数据集的大小，使得数据量相对较小的数据集也能用深度神经网络训练出不错的效果。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528397.png" alt="image-20220104195309340" /></p><p>不同输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">X_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>通过统一<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">G_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>得到两个向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>W</mi></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>G</mi><mi>W</mi></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G_W(X_1),G_W(X_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，计算两个向量之间的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">L1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord">1</span></span></span></span>距离获得<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">E_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。<br />其中，两个network是两个共享权值的网络，实际上就是两个完全相同的网络。孪生神经网络有两个输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">X1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">1</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">X2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">2</span></span></span></span>,将两个输入feed进入两个神经网络（Network1 and Network2），这两个神经网络分别将输入映射到新的空间，形成输入在新的空间中的表示。通过<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">Loss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span></span></span></span>的计算，评价两个输入的相似度。</p><p>如果左右两边不共享权值，而是两个不同的神经网络，叫做<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mi>s</mi><mi>e</mi><mi>u</mi><mi>d</mi><mi>o</mi><mo>−</mo><mi>s</mi><mi>i</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">pseudo-siamese network</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">u</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>，伪孪生神经网络。对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mi>s</mi><mi>e</mi><mi>u</mi><mi>d</mi><mi>o</mi><mo>−</mo><mi>s</mi><mi>i</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">pseudo-siamese network</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">u</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>，两边可以是不同的神经网络（如一个是lstm，一个是cnn），也可以是相同类型的神经网络。</p><h2 id="siamese的损失函数"><a class="markdownIt-Anchor" href="#siamese的损失函数"></a> Siamese的损失函数</h2><blockquote><p>Contrastive Loss</p></blockquote><h4 id="损失函数的选择"><a class="markdownIt-Anchor" href="#损失函数的选择"></a> 损失函数的选择</h4><p>Softmax当然是一种好的选择，但不一定是最优选择，即使是在分类问题中。传统的siamese network使用Contrastive Loss。损失函数还有更多的选择，siamese network的初衷是计算两个输入的相似度,。左右两个神经网络分别将输入转换成一个&quot;向量&quot;，在新的空间中，通过判断cosine距离就能得到相似度了。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Cosine</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span></span></span></span>是一个选择，<code>exp function</code>也是一种选择，欧式距离什么的都可以，<strong>训练的目标是让两个相似的输入距离尽可能的小，两个不同类别的输入距离尽可能的大。</strong></p><h4 id="论文中contrastive-loss"><a class="markdownIt-Anchor" href="#论文中contrastive-loss"></a> 论文中Contrastive Loss</h4><p>论文中的损失函数定义如下：<br />Y代表<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">X_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是否属于同一类别。输入同一类别为0，不属于同一类别为1。<br />P代表输入数据数量。<br />i表示当前输入数据下标。<br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">L_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表两个输入数据属于同一类别时的损失函数（G，genuine）。<br /><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>I</mi></msub></mrow><annotation encoding="application/x-tex">L_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表两个输入数据不属于同一类别的损失函数（I，imposter）。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>P</mi></munderover><mi>L</mi><mo stretchy="false">(</mo><mi>W</mi><mo separator="true">,</mo><mo stretchy="false">(</mo><mi>Y</mi><mo separator="true">,</mo><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mi>i</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(W)=\sum_{i=1}^{P}L(W,(Y,X_{1},X_{2})^{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.106005em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8746639999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.15999999999999992em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>W</mi><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mi>i</mi></msup><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>Y</mi><mo stretchy="false">)</mo><msub><mi>L</mi><mi>G</mi></msub><mrow><mo fence="true">(</mo><msub><mi>E</mi><mi>W</mi></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mi>i</mi></msup><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mrow></mrow><mrow><mo>+</mo><mi>Y</mi><msub><mi>L</mi><mi>I</mi></msub><mrow><mo fence="true">(</mo><msub><mi>E</mi><mi>W</mi></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mi>i</mi></msup><mo fence="true">)</mo></mrow></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array} {c}{ {L(W,(X_{1},X_{1},X_{2})^{i})=(1-Y)L_{G}\left(E_{W}(X_{1},X_{2})^{i}\right)} }\\ { { } } { {+ {Y}L_{I}\left(E_{W}(X_{1},{ { {X} } }_{2})^{i}\right)} }\end{array}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.42em;vertical-align:-0.96em;"></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.46em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span><span style="top:-2.4em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"></span></span><span class="mord"><span class="mord"><span class="mord">+</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.96em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span></span></span></span></span></p><p>根据我们对两个向量间举例的定义，可以得到以下条件：<br />即不同类别向量间的距离比相同类别向量间距离大。<br />两个向量之间距离越小，属于同一类别的可能性就越大。</p><h4 id="目前的contrastive-loss"><a class="markdownIt-Anchor" href="#目前的contrastive-loss"></a> 目前的Contrastive Loss</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528403.png" alt="img" /></p><p>其中：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528406.png" alt="img" /></p><p>代表两个样本特征X1和X2 的欧氏距离（二范数）P 表示样本的特征维数，Y 为两个样本是否匹配的标签，Y=1 代表两个样本相似或者匹配，Y=0 则代表不匹配，m 为设定的阈值，N 为样本个数。</p><p>观察上述的contrastive loss的表达式可以发现，这种损失函数可以很好</p><p>的表达成对样本的匹配程度，也能够很好用于训练提取特征的模型。</p><p>当 Y=1（即样本相似时），损失函数只剩下</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528402.png" alt="在这里插入图片描述" /></p><p>当 Y=0 (即样本不相似时），损失函数为</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528405.png" alt="在这里插入图片描述" /></p><p>即当样本不相似时，其特征空间的欧式距离反而小的话，损失值会变大，这也正好符号我们的要求。<br /><strong>注意：</strong><br />这里设置了一个阈值<strong>ｍargin</strong>，表示我们只考虑不相似特征欧式距离在<strong>０～ｍargin</strong>之间的，当距离超过ｍargin的，则把其loss看做为０**(即不相似的特征离的很远，其loss应该是很低的；而对于相似的特征反而离的很远，我们就需要增加其loss，从而不断更新成对样本的匹配程度)**</p><h2 id="siamese的思想总结"><a class="markdownIt-Anchor" href="#siamese的思想总结"></a> Siamese的思想总结</h2><p>其实讲了这么多，主要思想就是三点：</p><ul><li>输入不再是单个样本，而是一对样本，不再给单个的样本确切的标签，而且给定一对样本是否来自同一个类的标签，是就是0，不是就是1</li><li>设计了两个一模一样的网络，网络共享权值W，对输出进行了距离度量，可以说l1、l2等。</li><li>针对输入的样本对是否来自同一个类别设计了损失函数，损失函数形式有点类似交叉熵损失：<img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528415.png" alt="" /></li></ul><p>最后使用获得的损失函数，使用反向传播梯度下降去更新两个网络共享的权值W。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>低秩分解</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="低秩分解的几何解释"><a class="markdownIt-Anchor" href="#低秩分解的几何解释"></a> 低秩分解的几何解释</h1><p>低秩分解（Low-rank factorization）也可以通过几何的方式来解释，帮助我们理解其含义和应用。</p><p>假设我们有一个m×n的矩阵A，我们希望对其进行低秩分解，即将其分解为两个低秩矩阵的乘积：A ≈ UV^T。其中，U是一个m×k的矩阵，V是一个n×k的矩阵，k远远小于m和n。</p><p>几何上，可以将矩阵A视为描述一个向量空间中的点集。每一列可以看作是一个向量，而这些向量组成了一个n维的向量空间。低秩分解可以理解为通过两个低维的向量空间的点集的线性组合来近似表示原始向量空间中的点集。</p><p>具体地说，U矩阵的列向量可以看作是原始向量空间的基向量，它们将原始向量空间中的点集映射到一个低维的子空间。V矩阵的列向量则表示这个低维子空间中的基向量。通过对这两个子空间的基向量的线性组合，我们可以近似表示原始向量空间中的点集。</p><p>这个分解可以理解为以下几个几何步骤：</p><ol><li>U矩阵的列向量将原始向量空间中的点集映射到一个低维的子空间。这个子空间具有较低的维度k。</li><li>V矩阵的列向量表示这个低维子空间中的基向量，用于描述子空间中的点集。</li><li>通过对U和V的线性组合，将低维子空间中的点集映射回原始向量空间，近似重构出原始的点集。</li></ol><p>通过低秩分解，我们可以利用较低维度的向量空间来近似表示原始向量空间中的点集。这种近似表示可以在降低存储和计算成本的同时，尽可能地保留原始数据的主要结构和特征。</p><p>综上所述，几何视角可以帮助我们将低秩分解理解为通过两个低维子空间的基向量的线性组合来近似表示原始向量空间中的点集，从而实现对原始数据的降维和近似表示。这种几何解释有助于我们理解低秩分解的概念和原理。</p><h1 id="奇异值分解的几何理解"><a class="markdownIt-Anchor" href="#奇异值分解的几何理解"></a> <a href="https://www.cnblogs.com/lukairui/p/17475145.html">奇异值分解的几何理解</a></h1><p>奇异值分解（SVD）可以通过几何的方式来解释，从而帮助我们理解其含义和应用。</p><p>首先，我们可以将一个矩阵视为对向量空间的一种变换。假设有一个m×n的矩阵A，其中每一列可以看作是一个向量，而这些向量组成了一个n维的向量空间。奇异值分解可以将这个向量空间的变换分解为三个基本的几何操作：旋转、缩放和再次旋转。</p><p>具体地说，奇异值分解将矩阵A分解为三个矩阵的乘积：A = UΣVT。其中，U是一个正交矩阵，表示一个旋转操作；Σ是一个对角矩阵，对角线上的元素是奇异值，表示一个缩放操作；VT是另一个正交矩阵，表示另一个旋转操作。</p><p>这个分解可以理解为以下几个几何步骤：</p><ol><li>U对应的旋转矩阵将原始向量空间进行旋转操作，使其与新的基向量相对应。</li><li>Σ对应的对角矩阵进行缩放操作，将每个基向量的长度进行缩放，即改变了向量空间的比例关系。</li><li>V^T对应的旋转矩阵将缩放后的向量空间进行进一步旋转操作，以使其与原始向量空间对齐。</li></ol><p>通过奇异值分解，我们可以将原始矩阵A分解为这三个操作的组合，从而更好地理解和描述原始矩阵A的结构和特征。</p><p>此外，奇异值分解还提供了一种基于奇异值的重要性排序。奇异值的大小表示了每个基向量在变换中的重要性。较大的奇异值对应的基向量在变换中具有更大的影响力，而较小的奇异值对应的基向量在变换中贡献较小。</p><p>综上所述，几何视角可以帮助我们将奇异值分解理解为对向量空间的旋转、缩放和再次旋转等几何操作的组合，从而更好地理解和应用奇异值分解的概念和原理。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释机器学习</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="可解释机器学习"><a class="markdownIt-Anchor" href="#可解释机器学习"></a> 可解释机器学习</h1><p>作者：SkylaSun</p><p>链接：<a href="https://zhuanlan.zhihu.com/p/570926717">https://zhuanlan.zhihu.com/p/570926717</a></p><p>来源：知乎</p><p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h2 id="可解释机器学习-2"><a class="markdownIt-Anchor" href="#可解释机器学习-2"></a> 可解释机器学习</h2><p>可解释性方法将机器学习模型的决策过程转变成人类更能理解的结果。通用可解释性方法一般仅基于特征进行解释，无法处理图结构信息，在处理漏洞发掘场景下的图数据时，作者将边的相关性分数传递到<a href="https://www.zhihu.com/search?q=%E9%82%BB%E6%8E%A5%E8%8A%82%E7%82%B9&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">邻接节点</a>中，从而产生一个节点级别的解释。</p><ul><li>CAM（<a href="https://www.zhihu.com/search?q=%E7%B1%BB%E5%88%AB%E6%BF%80%E6%B4%BB%E6%98%A0%E5%B0%84%E5%9B%BE&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">类别激活映射图</a>），一种特征<a href="https://www.zhihu.com/search?q=%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">可视化技术</a>，最初为解释CNN模型而设计，将深层网络中学习到的<a href="https://www.zhihu.com/search?q=%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">语义信息</a>，通过权重与输出节点联系起来。</li><li><a href="https://www.zhihu.com/search?q=%E7%BA%BF%E6%80%A7%E8%BF%91%E4%BC%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">线性近似</a>，通过梯度与输入，计算每个特征对分类输出的线性化贡献。</li><li>GradCAM，将<a href="https://www.zhihu.com/search?q=%E7%BA%BF%E6%80%A7%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">线性近似方法</a>应用于GNN层的中间<a href="https://www.zhihu.com/search?q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">激活函数</a>，而非输入激活，产生了类似CAM的优势。</li><li>SmoothGrad，对多个噪声输入进行节点特征梯度平均，并且产生了抗噪声的解释。</li><li>IG（<a href="https://www.zhihu.com/search?q=%E7%A7%AF%E5%88%86%E6%A2%AF%E5%BA%A6&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">积分梯度</a>），改进了线性近似，过程中参考了反事实基线输入G‘，并且使用延实际输入G的<a href="https://www.zhihu.com/search?q=%E7%9B%B4%E7%BA%BF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">直线</a>路径平均的梯度，以及G和G’之间的输入激活。</li><li>Gradient or Saliency Method，通过梯度衡量预测结果相对不同输入的变化情况。</li><li>GB（导向反向传播），对普通的<a href="https://www.zhihu.com/search?q=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">反向传播</a>加了指导，限制了小于0的梯度的回传。</li><li>LRP（逐层相关性传播），将预测结果反向传递会输入，创建相关性映射。</li><li>EB（<a href="https://www.zhihu.com/search?q=%E6%BF%80%E5%8A%B1%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">激励反向传播</a>），使用反向传播计算l-1层到第1层中<a href="https://www.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E5%85%83&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">神经元</a>激活的相对影响，而且仅考虑<a href="https://www.zhihu.com/search?q=%E6%AD%A3%E6%9D%83%E9%87%8D&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">正权重</a>。</li></ul><p>另外，本文关注的三种针对<a href="https://www.zhihu.com/search?q=GNN%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">GNN模型</a>的可解释性方法概述如下：</p><p><strong>GNNExplainer</strong>，<a href="https://www.zhihu.com/search?q=%E9%BB%91%E7%9B%92%E5%89%8D%E5%90%91%E8%A7%A3%E9%87%8A%E6%8A%80%E6%9C%AF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">黑盒前向解释技术</a>，最大化整体预测结果和可判别子图S以及节点特征子集的<a href="https://www.zhihu.com/search?q=%E4%BA%92%E4%BF%A1%E6%81%AF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">互信息</a>。</p><p><strong>PGExplainer</strong>，解决了GNNExplainer需要针对每个单独的图实例进行计算的问题，同样也是通过提取相关子图S进行全局解释，但支持归纳式学习方法。</p><p><strong>Graph-LRP</strong>使用<a href="https://www.zhihu.com/search?q=%E9%AB%98%E9%98%B6%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22570926717%22%7D">高阶泰勒展开</a>处理多层GNN上节点之间的消息传递。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习专业术语</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/</url>
      
        <content type="html"><![CDATA[<h4 id="激活函数activation-function"><a class="markdownIt-Anchor" href="#激活函数activation-function"></a> 激活函数（Activation Function）</h4><p>为了让神经网络能够学习复杂的决策边界（decision boundary），我们在其一些层应用一个非线性激活函数。最常用的函数包括 sigmoid、tanh、ReLU（Rectified Linear Unit 线性修正单元） 以及这些函数的变体。</p><h4 id="adadelta"><a class="markdownIt-Anchor" href="#adadelta"></a> Adadelta</h4><p>Adadelta 是一个基于梯度下降的学习算法，可以随时间调整适应每个参数的学习率。它是作为 Adagrad 的改进版提出的，它比超参数（hyperparameter）更敏感而且可能会太过严重地降低学习率。Adadelta 类似于 rmsprop，而且可被用来替代 vanilla SGD。</p><p>论文：Adadelta：一种自适应学习率方法（ADADELTA: An Adaptive Learning Rate Method）<br />技术博客：斯坦福 CS231n：优化算法（<a href="http://cs231n.github.io/neural-networks-3/%EF%BC%89">http://cs231n.github.io/neural-networks-3/）</a><br />技术博客：梯度下降优化算法概述（<a href="http://sebastianruder.com/optimizing-gradient-descent/%EF%BC%89">http://sebastianruder.com/optimizing-gradient-descent/）</a></p><h4 id="adagrad"><a class="markdownIt-Anchor" href="#adagrad"></a> Adagrad</h4><p>Adagrad 是一种自适应学习率算法，能够随时间跟踪平方梯度并自动适应每个参数的学习率。它可被用来替代vanilla SGD (<a href="http://www.wildml.com/deep-learning-glossary/#sgd">http://www.wildml.com/deep-learning-glossary/#sgd</a>)；而且在稀疏数据上更是特别有用，在其中它可以将更高的学习率分配给更新不频繁的参数。</p><p>论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）<br />技术博客：斯坦福 CS231n：优化算法（<a href="http://cs231n.github.io/neural-networks-3/%EF%BC%89">http://cs231n.github.io/neural-networks-3/）</a><br />技术博客：梯度下降优化算法概述（<a href="http://sebastianruder.com/optimizing-gradient-descent/%EF%BC%89">http://sebastianruder.com/optimizing-gradient-descent/）</a></p><h4 id="adam"><a class="markdownIt-Anchor" href="#adam"></a> Adam</h4><p>Adam 是一种类似于 rmsprop 的自适应学习率算法，但它的更新是通过使用梯度的第一和第二时刻的运行平均值（running average）直接估计的，而且还包括一个偏差校正项。</p><p>论文：Adam：一种随机优化方法（Adam: A Method for Stochastic Optimization）<br />技术博客：梯度下降优化算法概述（<a href="http://sebastianruder.com/optimizing-gradient-descent/%EF%BC%89">http://sebastianruder.com/optimizing-gradient-descent/）</a></p><h4 id="仿射层affine-layer"><a class="markdownIt-Anchor" href="#仿射层affine-layer"></a> 仿射层（Affine Layer）</h4><p>神经网络中的一个全连接层。仿射（Affine）的意思是前面一层中的每一个神经元都连接到当前层中的每一个神经元。在许多方面，这是神经网络的「标准」层。仿射层通常被加在卷积神经网络或循环神经网络做出最终预测前的输出的顶层。仿射层的一般形式为 y = f(Wx + b)，其中 x 是层输入，w 是参数，b 是一个偏差矢量，f 是一个非线性激活函数。</p><h4 id="注意机制attention-mechanism"><a class="markdownIt-Anchor" href="#注意机制attention-mechanism"></a> 注意机制（Attention Mechanism）</h4><p>注意机制是由人类视觉注意所启发的，是一种关注图像中特定部分的能力。注意机制可被整合到语言处理和图像识别的架构中以帮助网络学习在做出预测时应该「关注」什么。</p><p>技术博客：深度学习和自然语言处理中的注意和记忆（<a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/%EF%BC%89">http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/）</a></p><h4 id="alexnet"><a class="markdownIt-Anchor" href="#alexnet"></a> Alexnet</h4><p>Alexnet 是一种卷积神经网络架构的名字，这种架构曾在 2012 年 ILSVRC 挑战赛中以巨大优势获胜，而且它还导致了人们对用于图像识别的卷积神经网络（CNN）的兴趣的复苏。它由 5 个卷积层组成。其中一些后面跟随着最大池化（max-pooling）层和带有最终 1000 条路径的 softmax (1000-way softmax)的 3个全连接层。Alexnet 被引入到了使用深度卷积神经网络的 ImageNet 分类中。</p><h4 id="自编码器autoencoder"><a class="markdownIt-Anchor" href="#自编码器autoencoder"></a> 自编码器（Autoencoder）</h4><p>自编码器是一种神经网络模型，它的目标是预测输入自身，这通常通过网络中某个地方的「瓶颈（bottleneck）」实现。通过引入瓶颈，我们迫使网络学习输入更低维度的表征，从而有效地将输入压缩成一个好的表征。自编码器和 PCA 等降维技术相关，但因为它们的非线性本质，它们可以学习更为复杂的映射。目前已有一些范围涵盖较广的自编码器存在，包括 降噪自编码器（Denoising Autoencoders）、变自编码器（Variational Autoencoders）和序列自编码器（Sequence Autoencoders）。</p><p>降噪自编码器论文：Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion<br />变自编码器论文：Auto-Encoding Variational Bayes<br />序列自编码器论文：Semi-supervised Sequence Learning</p><h4 id="平均池化average-pooling"><a class="markdownIt-Anchor" href="#平均池化average-pooling"></a> 平均池化（Average-Pooling）</h4><p>平均池化是一种在卷积神经网络中用于图像识别的池化（Pooling）技术。它的工作原理是在特征的局部区域上滑动窗口，比如像素，然后再取窗口中所有值的平均。它将输入表征压缩成一种更低维度的表征。</p><h4 id="反向传播backpropagation"><a class="markdownIt-Anchor" href="#反向传播backpropagation"></a> 反向传播（Backpropagation）</h4><p>反向传播是一种在神经网络中用来有效地计算梯度的算法，或更一般而言，是一种前馈计算图（feedforward computational graph）。其可以归结成从网络输出开始应用分化的链式法则，然后向后传播梯度。反向传播的第一个应用可以追溯到 1960 年代的 Vapnik 等人，但论文 Learning representations by back-propagating errors常常被作为引用源。</p><p>技术博客：计算图上的微积分学：反向传播（<a href="http://colah.github.io/posts/2015-08-Backprop/%EF%BC%89">http://colah.github.io/posts/2015-08-Backprop/）</a></p><h4 id="通过时间的反向传播bpttbackpropagation-through-time"><a class="markdownIt-Anchor" href="#通过时间的反向传播bpttbackpropagation-through-time"></a> 通过时间的反向传播（BPTT：Backpropagation Through Time）</h4><p>通过时间的反向传播是应用于循环神经网络（RNN）的反向传播算法。BPTT 可被看作是应用于 RNN 的标准反向传播算法，其中的每一个时间步骤（time step）都代表一个计算层，而且它的参数是跨计算层共享的。因为 RNN 在所有的时间步骤中都共享了同样的参数，一个时间步骤的错误必然能「通过时间」反向到之前所有的时间步骤，该算法也因而得名。当处理长序列（数百个输入）时，为降低计算成本常常使用一种删节版的 BPTT。删节的 BPTT 会在固定数量的步骤之后停止反向传播错误。</p><p>论文：Backpropagation Through Time: What It Does and How to Do It</p><h4 id="分批标准化bnbatch-normalization"><a class="markdownIt-Anchor" href="#分批标准化bnbatch-normalization"></a> 分批标准化（BN：Batch Normalization）</h4><p>分批标准化是一种按小批量的方式标准化层输入的技术。它能加速训练过程，允许使用更高的学习率，还可用作规范器（regularizer）。人们发现，分批标准化在卷积和前馈神经网络中应用时非常高效，但尚未被成功应用到循环神经网络上。</p><p>论文：分批标准化：通过减少内部协变量位移（Covariate Shift）加速深度网络训练（Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）<br />论文：使用分批标准化的循环神经网络（Batch Normalized Recurrent Neural Networks）</p><h4 id="双向循环神经网络bidirectional-rnn"><a class="markdownIt-Anchor" href="#双向循环神经网络bidirectional-rnn"></a> 双向循环神经网络（Bidirectional RNN）</h4><p>双向循环神经网络是一类包含两个方向不同的 RNN 的神经网络。其中的前向 RNN 从起点向终点读取输入序列，而反向 RNN 则从终点向起点读取。这两个 RNN 互相彼此堆叠，它们的状态通常通过附加两个矢量的方式进行组合。双向 RNN 常被用在自然语言问题中，因为在自然语言中我们需要同时考虑话语的前后上下文以做出预测。</p><p>论文：双向循环神经网络（Bidirectional Recurrent Neural Networks）</p><h4 id="caffe"><a class="markdownIt-Anchor" href="#caffe"></a> Caffe</h4><p>Caffe 是由伯克利大学视觉和学习中心开发的一种深度学习框架。在视觉任务和卷积神经网络模型中，Caffe 格外受欢迎且性能优异。</p><h4 id="分类交叉熵损失categorical-cross-entropy-loss"><a class="markdownIt-Anchor" href="#分类交叉熵损失categorical-cross-entropy-loss"></a> 分类交叉熵损失（Categorical Cross-Entropy Loss）</h4><p>分类交叉熵损失也被称为负对数似然（negative log likelihood）。这是一种用于解决分类问题的流行的损失函数，可用于测量两种概率分布（通常是真实标签和预测标签）之间的相似性。它可用 L = -sum(y * log(y_prediction)) 表示，其中 y 是真实标签的概率分布（通常是一个one-hot vector），y_prediction 是预测标签的概率分布，通常来自于一个 softmax。</p><h4 id="信道channel"><a class="markdownIt-Anchor" href="#信道channel"></a> 信道（Channel）</h4><p>深度学习模型的输入数据可以有多个信道。图像就是个典型的例子，它有红、绿和蓝三个颜色信道。一个图像可以被表示成一个三维的张量（Tensor），其中的维度对应于信道、高度和宽度。自然语言数据也可以有多个信道，比如在不同类型的嵌入（embedding）形式中。</p><h4 id="卷积神经网络cnnconvnetconvolutional-neural-network"><a class="markdownIt-Anchor" href="#卷积神经网络cnnconvnetconvolutional-neural-network"></a> 卷积神经网络（CNN/ConvNet：Convolutional Neural Network）</h4><p>CNN 使用卷积连接从输入的局部区域中提取的特征。大部分 CNN 都包含了卷积层、池化层和仿射层的组合。CNN 尤其凭借其在视觉识别任务的卓越性能表现而获得了普及，它已经在该领域保持了好几年的领先。</p><p>技术博客：斯坦福CS231n类——用于视觉识别的卷积神经网络（<a href="http://cs231n.github.io/neural-networks-3/%EF%BC%89">http://cs231n.github.io/neural-networks-3/）</a><br />技术博客：理解用于自然语言处理的卷积神经网络（<a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/%EF%BC%89">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/）</a></p><h4 id="深度信念网络dbndeep-belief-network"><a class="markdownIt-Anchor" href="#深度信念网络dbndeep-belief-network"></a> 深度信念网络（DBN：Deep Belief Network）</h4><p>DBN 是一类以无监督的方式学习数据的分层表征的概率图形模型。DBN 由多个隐藏层组成，这些隐藏层的每一对连续层之间的神经元是相互连接的。DBN 通过彼此堆叠多个 RBN（限制波尔兹曼机）并一个接一个地训练而创建。</p><p>论文：深度信念网络的一种快速学习算法（A fast learning algorithm for deep belief nets）</p><h4 id="deep-dream"><a class="markdownIt-Anchor" href="#deep-dream"></a> Deep Dream</h4><p>这是谷歌发明的一种试图用来提炼深度卷积神经网络获取的知识的技术。这种技术可以生成新的图像或转换已有的图片从而给它们一种幻梦般的感觉，尤其是递归地应用时。</p><p>代码：Github 上的 Deep Dream（<a href="https://github.com/google/deepdream%EF%BC%89">https://github.com/google/deepdream）</a><br />技术博客：Inceptionism：向神经网络掘进更深（<a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html%EF%BC%89">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html）</a></p><h4 id="dropout"><a class="markdownIt-Anchor" href="#dropout"></a> Dropout</h4><p>Dropout(随机失活) 是一种用于神经网络防止过拟合的正则化技术。它通过在每次训练迭代中随机地设置神经元中的一小部分为 0 来阻止神经元共适应（co-adapting），Dropout 可以通过多种方式进行解读，比如从不同网络的指数数字中随机取样。Dropout 层首先通过它们在卷积神经网络中的应用而得到普及，但自那以后也被应用到了其它层上，包括输入嵌入或循环网络。</p><p>通俗讲随机失活（Dropout）在训练的过程中以一定比例随机失活神经元来达到提高模型泛化能力的效果。</p><p>论文：Dropout: 一种防止神经网络过拟合的简单方法（Dropout: A Simple Way to Prevent Neural Networks from Overfitting）<br />论文：循环神经网络正则化（Recurrent Neural Network Regularization）</p><h4 id="嵌入embedding"><a class="markdownIt-Anchor" href="#嵌入embedding"></a> 嵌入（Embedding）</h4><p>一个嵌入映射到一个输入表征，比如一个词或一句话映射到一个矢量。一种流行的嵌入是词语嵌入（word embedding，国内常用的说法是：词向量），如 word2vec 或 GloVe。我们也可以嵌入句子、段落或图像。比如说，通过将图像和他们的文本描述映射到一个共同的嵌入空间中并最小化它们之间的距离，我们可以将标签和图像进行匹配。嵌入可以被明确地学习到，比如在 word2vec 中；嵌入也可作为监督任务的一部分例如情感分析（Sentiment Analysis）。通常一个网络的输入层是通过预先训练的嵌入进行初始化，然后再根据当前任务进行微调（fine-tuned）。</p><h4 id="梯度爆炸问题exploding-gradient-problem"><a class="markdownIt-Anchor" href="#梯度爆炸问题exploding-gradient-problem"></a> 梯度爆炸问题（Exploding Gradient Problem）</h4><p>梯度爆炸问题是梯度消失问题（Vanishing Gradient Problem）的对立面。在深度神经网络中，梯度可能会在反向传播过程中爆炸，导致数字溢出。解决梯度爆炸的一个常见技术是执行梯度裁剪（Gradient Clipping）。</p><p>论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）</p><h4 id="微调fine-tuning"><a class="markdownIt-Anchor" href="#微调fine-tuning"></a> 微调（Fine-Tuning）</h4><p>Fine-Tuning 这种技术是指使用来自另一个任务（例如一个无监督训练网络）的参数初始化网络，然后再基于当前任务更新这些参数。比如，自然语言处理架构通常使用 word2vec 这样的预训练的词向量（word embeddings），然后这些词向量会在训练过程中基于特定的任务（如情感分析）进行更新。</p><h4 id="梯度裁剪gradient-clipping"><a class="markdownIt-Anchor" href="#梯度裁剪gradient-clipping"></a> 梯度裁剪（Gradient Clipping）</h4><p>梯度裁剪是一种在非常深度的网络（通常是循环神经网络）中用于防止梯度爆炸（exploding gradient）的技术。执行梯度裁剪的方法有很多，但常见的一种是当参数矢量的 L2 范数（L2 norm）超过一个特定阈值时对参数矢量的梯度进行标准化，这个特定阈值根据函数：新梯度=梯度*阈值/L2范数（梯度）{new_gradients = gradients * threshold / l2_norm(gradients)}确定。</p><p>论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）</p><h4 id="glove"><a class="markdownIt-Anchor" href="#glove"></a> GloVe</h4><p>Glove 是一种为话语获取矢量表征（嵌入）的无监督学习算法。GloVe 的使用目的和 word2vec 一样，但 GloVe 具有不同的矢量表征，因为它是在共现（co-occurrence）统计数据上训练的。</p><p>论文：GloVe：用于词汇表征（Word Representation）的全局矢量（Global Vector）（GloVe: Global Vectors for Word Representation ）</p><h4 id="googlelenet"><a class="markdownIt-Anchor" href="#googlelenet"></a> GoogleLeNet</h4><p>GoogleLeNet 是曾赢得了 2014 年 ILSVRC 挑战赛的一种卷积神经网络架构。这种网络使用 Inception 模块（Inception Module）以减少参数和提高网络中计算资源的利用率。</p><p>论文：使用卷积获得更深（Going Deeper with Convolutions）</p><h4 id="gru"><a class="markdownIt-Anchor" href="#gru"></a> GRU</h4><p>GRU（Gated Recurrent Unit：门控循环单元）是一种 LSTM 单元的简化版本，拥有更少的参数。和 LSTM 细胞（LSTM cell）一样，它使用门控机制，通过防止梯度消失问题（vanishing gradient problem）让循环神经网络可以有效学习长程依赖（long-range dependency）。GRU 包含一个复位和更新门，它们可以根据当前时间步骤的新值决定旧记忆中哪些部分需要保留或更新。</p><p>论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）<br />技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/%EF%BC%89">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）</a></p><h4 id="ground-truth"><a class="markdownIt-Anchor" href="#ground-truth"></a> ground truth</h4><p>在机器学习中， ground truth表示监督学习的训练集的分类准确性，用于证明或者推翻某个假设。有监督的机器学习会对训练数据打标记，试想一下如果训练标记错误，那么将会对测试数据的预测产生影响，因此这里将那些正确打标记的数据成为ground truth。</p><h4 id="highway-layer"><a class="markdownIt-Anchor" href="#highway-layer"></a> Highway Layer</h4><p>Highway Layer　是使用门控机制控制通过层的信息流的一种神经网络层。堆叠多个 Highway Layer 层可让训练非常深的网络成为可能。Highway Layer 的工作原理是通过学习一个选择输入的哪部分通过和哪部分通过一个变换函数（如标准的仿射层）的门控函数来进行学习。Highway Layer 的基本公式是 T * h(x) + (1 - T) * x；其中 T 是学习过的门控函数，取值在 0 到 1 之间；h(x) 是一个任意的输入变换，x 是输入。注意所有这些都必须具有相同的大小。</p><p>论文：Highway Networks</p><h4 id="icml"><a class="markdownIt-Anchor" href="#icml"></a> ICML</h4><p>即国际机器学习大会（International Conference for Machine Learning），一个顶级的机器学习会议。</p><h4 id="ilsvrc"><a class="markdownIt-Anchor" href="#ilsvrc"></a> ILSVRC</h4><p>即 ImageNet 大型视觉识别挑战赛（ImageNet Large Scale Visual Recognition Challenge），该比赛用于评估大规模对象检测和图像分类的算法。它是计算机视觉领域最受欢迎的学术挑战赛。过去几年中，深度学习让错误率出现了显著下降，从 30% 降到了不到 5%，在许多分类任务中击败了人类。</p><h4 id="inception模块inception-module"><a class="markdownIt-Anchor" href="#inception模块inception-module"></a> Inception模块（Inception Module）</h4><p>Inception模块被用在卷积神经网络中，通过堆叠 1×1 卷积的降维（dimensionality reduction）带来更高效的计算和更深度的网络。</p><p>论文：使用卷积获得更深（Going Deeper with Convolutions）</p><h4 id="keras"><a class="markdownIt-Anchor" href="#keras"></a> Keras</h4><p>Kears 是一个基于 Python 的深度学习库，其中包括许多用于深度神经网络的高层次构建模块。它可以运行在 TensorFlow 或 Theano 上。</p><h4 id="lstm"><a class="markdownIt-Anchor" href="#lstm"></a> LSTM</h4><p>长短期记忆（Long Short-Term Memory）网络通过使用内存门控机制防止循环神经网络（RNN）中的梯度消失问题（vanishing gradient problem）。使用 LSTM 单元计算 RNN 中的隐藏状态可以帮助该网络有效地传播梯度和学习长程依赖（long-range dependency）。</p><p>论文：长短期记忆（LONG SHORT-TERM MEMORY）<br />技术博客：理解 LSTM 网络（<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/%EF%BC%89">http://colah.github.io/posts/2015-08-Understanding-LSTMs/）</a><br />技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/%EF%BC%89">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）</a></p><h4 id="最大池化max-pooling"><a class="markdownIt-Anchor" href="#最大池化max-pooling"></a> 最大池化（Max-Pooling）</h4><p>池化（Pooling）操作通常被用在卷积神经网络中。一个最大池化层从一块特征中选取最大值。和卷积层一样，池化层也是通过窗口（块）大小和步幅尺寸进行参数化。比如，我们可能在一个 10×10 特征矩阵上以 2 的步幅滑动一个 2×2 的窗口，然后选取每个窗口的 4 个值中的最大值，得到一个 5×5 特征矩阵。池化层通过只保留最突出的信息来减少表征的维度；在这个图像输入的例子中，它们为转译提供了基本的不变性（即使图像偏移了几个像素，仍可选出同样的最大值）。池化层通常被安插在连续卷积层之间。</p><h4 id="mnist"><a class="markdownIt-Anchor" href="#mnist"></a> MNIST</h4><p>MNIST数据集可能是最常用的一个图像识别数据集。它包含 60,000 个手写数字的训练样本和 10,000 个测试样本。每一张图像的尺寸为 28×28像素。目前最先进的模型通常能在该测试集中达到 99.5% 或更高的准确度。</p><h4 id="动量momentum"><a class="markdownIt-Anchor" href="#动量momentum"></a> 动量（Momentum）</h4><p>动量是梯度下降算法（Gradient Descent Algorithm）的扩展，可以加速和阻抑参数更新。在实际应用中，在梯度下降更新中包含一个动量项可在深度网络中得到更好的收敛速度（convergence rate）。</p><p>论文：通过反向传播（back-propagating error）错误学习表征</p><h4 id="多层感知器mlpmultilayer-perceptron"><a class="markdownIt-Anchor" href="#多层感知器mlpmultilayer-perceptron"></a> 多层感知器（MLP：Multilayer Perceptron）</h4><p>多层感知器是一种带有多个全连接层的前馈神经网络，这些全连接层使用非线性激活函数（activation function）处理非线性可分的数据。MLP 是多层神经网络或有两层以上的深度神经网络的最基本形式。</p><p>负对数似然（NLL：Negative Log Likelihood）</p><p>参见分类交叉熵损失（Categorical Cross-Entropy Loss）。</p><h4 id="神经网络机器翻译nmtneural-machine-translation"><a class="markdownIt-Anchor" href="#神经网络机器翻译nmtneural-machine-translation"></a> 神经网络机器翻译（NMT：Neural Machine Translation）</h4><p>NMT 系统使用神经网络实现语言（如英语和法语）之间的翻译。NMT 系统可以使用双语语料库进行端到端的训练，这有别于需要手工打造特征和开发的传统机器翻译系统。NMT 系统通常使用编码器和解码器循环神经网络实现，它可以分别编码源句和生成目标句。</p><p>论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）<br />论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）</p><h4 id="神经图灵机ntmneural-turing-machine"><a class="markdownIt-Anchor" href="#神经图灵机ntmneural-turing-machine"></a> 神经图灵机（NTM：Neural Turing Machine）</h4><p>NTM 是可以从案例中推导简单算法的神经网络架构。比如，NTM 可以通过案例的输入和输出学习排序算法。NTM 通常学习记忆和注意机制的某些形式以处理程序执行过程中的状态。</p><p>论文：神经图灵机（Neural Turing Machines）</p><p>非线性（Nonlinearity）</p><p>参见激活函数（Activation Function）。</p><h4 id="噪音对比估计ncenoise-contrastive-estimation"><a class="markdownIt-Anchor" href="#噪音对比估计ncenoise-contrastive-estimation"></a> 噪音对比估计（NCE：noise-contrastive estimation）</h4><p>噪音对比估计是一种通常被用于训练带有大输出词汇的分类器的采样损失（sampling loss）。在大量的可能的类上计算 softmax 是异常昂贵的。使用 NCE，我们可以将问题降低成二元分类问题，这可以通过训练分类器区别对待取样和「真实」分布以及人工生成的噪声分布来实现。</p><p>论文：噪音对比估计：一种用于非标准化统计模型的新估计原理（Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ）<br />论文：使用噪音对比估计有效地学习词向量（Learning word embeddings efficiently with noise-contrastive estimation）</p><h4 id="池化"><a class="markdownIt-Anchor" href="#池化"></a> 池化</h4><p>参见最大池化（Max-Pooling）或平均池化（Average-Pooling）。</p><h4 id="受限玻尔兹曼机rbnrestricted-boltzmann-machine"><a class="markdownIt-Anchor" href="#受限玻尔兹曼机rbnrestricted-boltzmann-machine"></a> 受限玻尔兹曼机（RBN：Restricted Boltzmann Machine）</h4><p>RBN 是一种可被解释为一个随机人工神经网络的概率图形模型。RBN 以无监督的形式学习数据的表征。RBN 由可见层和隐藏层以及每一个这些层中的二元神经元的连接所构成。RBN 可以使用对比散度（contrastive divergence）进行有效的训练，这是梯度下降的一种近似。</p><p>第六章：动态系统中的信息处理：和谐理论基础<br />论文：受限玻尔兹曼机简介（An Introduction to Restricted Boltzmann Machines）</p><h4 id="循环神经网络rnnrecurrent-neural-network"><a class="markdownIt-Anchor" href="#循环神经网络rnnrecurrent-neural-network"></a> 循环神经网络（RNN：Recurrent Neural Network）</h4><p>RNN 模型通过隐藏状态（或称记忆）连续进行相互作用。它可以使用最多 N 个输入，并产生最多 N 个输出。比如，一个输入序列可能是一个句子，其输出为每个单词的词性标注（part-of-speech tag）（N 到 N）；一个输入可能是一个句子，其输出为该句子的情感分类（N 到 1）；一个输入可能是单个图像，其输出为描述该图像所对应一系列词语（1 到 N）。在每一个时间步骤中，RNN 会基于当前输入和之前的隐藏状态计算新的隐藏状态「记忆」。其中「循环（recurrent）」这个术语来自这个事实：在每一步中都是用了同样的参数，该网络根据不同的输入执行同样的计算。</p><p>技术博客：了解 LSTM 网络（<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/%EF%BC%89">http://colah.github.io/posts/2015-08-Understanding-LSTMs/）</a><br />技术博客：循环神经网络教程第1部分——介绍 RNN （<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/%EF%BC%89">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/）</a></p><h4 id="递归神经网络recursive-neural-network"><a class="markdownIt-Anchor" href="#递归神经网络recursive-neural-network"></a> 递归神经网络（Recursive Neural Network）</h4><p>递归神经网络是循环神经网络的树状结构的一种泛化（generalization）。每一次递归都使用相同的权重。就像 RNN 一样，递归神经网络可以使用向后传播（backpropagation）进行端到端的训练。尽管可以学习树结构以将其用作优化问题的一部分，但递归神经网络通常被用在已有预定义结构的问题中，如自然语言处理的解析树中。</p><p>论文：使用递归神经网络解析自然场景和自然语言（Parsing Natural Scenes and Natural Language with Recursive Neural Networks ）</p><h4 id="relu"><a class="markdownIt-Anchor" href="#relu"></a> ReLU</h4><p>即线性修正单元（Rectified Linear Unit）。ReLU 常在深度神经网络中被用作激活函数。它们的定义是 f(x) = max(0, x) 。ReLU 相对于 tanh 等函数的优势包括它们往往很稀疏（它们的活化可以很容易设置为 0），而且它们受到梯度消失问题的影响也更小。ReLU 主要被用在卷积神经网络中用作激活函数。ReLU 存在几种变体，如Leaky ReLUs、Parametric ReLU (PReLU) 或更为流畅的 softplus近似。</p><p>论文：深入研究修正器（Rectifiers）：在 ImageNet 分类上超越人类水平的性能（Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification）<br />论文：修正非线性改进神经网络声学模型（Rectifier Nonlinearities Improve Neural Network Acoustic Models ）<br />论文：线性修正单元改进受限玻尔兹曼机（Rectified Linear Units Improve Restricted Boltzmann Machines ）</p><h4 id="残差网络resnet"><a class="markdownIt-Anchor" href="#残差网络resnet"></a> 残差网络（ResNet）</h4><p>深度残差网络（Deep Residual Network）赢得了 2015 年的 ILSVRC 挑战赛。这些网络的工作方式是引入跨层堆栈的快捷连接，让优化器可以学习更「容易」的残差映射（residual mapping）而非更为复杂的原映射（original mapping）。这些快捷连接和 Highway Layer 类似，但它们与数据无关且不会引入额外的参数或训练复杂度。ResNet 在 ImageNet 测试集中实现了 3.57% 的错误率。</p><p>论文：用于图像识别的深度残差网络（Deep Residual Learning for Image Recognition）</p><h4 id="rmsprop"><a class="markdownIt-Anchor" href="#rmsprop"></a> RMSProp</h4><p>RMSProp 是一种基于梯度的优化算法。它与 Adagrad 类似，但引入了一个额外的衰减项抵消 Adagrad 在学习率上的快速下降。</p><p>PPT：用于机器学习的神经网络 讲座6a<br />技术博客：斯坦福CS231n：优化算法（<a href="http://cs231n.github.io/neural-networks-3/%EF%BC%89">http://cs231n.github.io/neural-networks-3/）</a><br />技术博客：梯度下降优化算法概述（<a href="http://sebastianruder.com/optimizing-gradient-descent/%EF%BC%89">http://sebastianruder.com/optimizing-gradient-descent/）</a></p><h4 id="序列到序列seq2seq"><a class="markdownIt-Anchor" href="#序列到序列seq2seq"></a> 序列到序列（Seq2Seq）</h4><p>序列到序列（Sequence-to-Sequence）模型读取一个序列（如一个句子）作为输入，然后产生另一个序列作为输出。它和标准的 RNN 不同；在标准的 RNN 中，输入序列会在网络开始产生任何输出之前被完整地读取。通常而言，Seq2Seq 通过两个分别作为编码器和解码器的 RNN 实现。神经网络机器翻译是一类典型的 Seq2Seq 模型。</p><p>论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）</p><h4 id="随机梯度下降sgdstochastic-gradient-descent"><a class="markdownIt-Anchor" href="#随机梯度下降sgdstochastic-gradient-descent"></a> 随机梯度下降（SGD：Stochastic Gradient Descent）</h4><p>随机梯度下降是一种被用在训练阶段学习网络参数的基于梯度的优化算法。梯度通常使用反向传播算法计算。在实际应用中，人们使用微小批量版本的 SGD，其中的参数更新基于批案例而非单个案例进行执行，这能增加计算效率。vanilla SGD 存在许多扩展，包括动量（Momentum）、Adagrad、rmsprop、Adadelta 或 Adam。</p><p>论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）<br />技术博客：斯坦福CS231n：优化算法（<a href="http://cs231n.github.io/neural-networks-3/%EF%BC%89">http://cs231n.github.io/neural-networks-3/）</a><br />技术博客：梯度下降优化算法概述（<a href="http://sebastianruder.com/optimizing-gradient-descent/%EF%BC%89">http://sebastianruder.com/optimizing-gradient-descent/）</a></p><h4 id="softmax"><a class="markdownIt-Anchor" href="#softmax"></a> Softmax</h4><p>Softmax 函数通常被用于将原始分数（raw score）的矢量转换成用于分类的神经网络的输出层上的类概率（class probability）。它通过对归一化常数（normalization constant）进行指数化和相除运算而对分数进行规范化。如果我们正在处理大量的类，例如机器翻译中的大量词汇，计算归一化常数是很昂贵的。有许多种可以让计算更高效的替代选择，包括分层 Softmax（Hierarchical Softmax）或使用基于取样的损失函数，如 NCE。</p><h4 id="tensorflow"><a class="markdownIt-Anchor" href="#tensorflow"></a> TensorFlow</h4><p>TensorFlow是一个开源 C ++ / Python 软件库，用于使用数据流图的数值计算，尤其是深度神经网络。它是由谷歌创建的。在设计方面，它最类似于 Theano，但比 Caffe 或 Keras 更低级。</p><h4 id="theano"><a class="markdownIt-Anchor" href="#theano"></a> Theano</h4><p>Theano 是一个让你可以定义、优化和评估数学表达式的 Python 库。它包含许多用于深度神经网络的构造模块。Theano 是类似于 TensorFlow 的低级别库。更高级别的库包括Keras 和 Caffe。</p><h4 id="梯度消失问题vanishing-gradient-problem"><a class="markdownIt-Anchor" href="#梯度消失问题vanishing-gradient-problem"></a> 梯度消失问题（Vanishing Gradient Problem）</h4><p>梯度消失问题出现在使用梯度很小（在 0 到 1 的范围内）的激活函数的非常深的神经网络中，通常是循环神经网络。因为这些小梯度会在反向传播中相乘，它们往往在这些层中传播时「消失」，从而让网络无法学习长程依赖。解决这一问题的常用方法是使用 ReLU 这样的不受小梯度影响的激活函数，或使用明确针对消失梯度问题的架构，如LSTM。这个问题的反面被称为梯度爆炸问题（exploding gradient problem）。</p><p>论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）</p><h4 id="vgg"><a class="markdownIt-Anchor" href="#vgg"></a> VGG</h4><p>VGG 是在 2014 年 ImageNet 定位和分类比赛中分别斩获第一和第二位置的卷积神经网络模型。这个 VGG 模型包含 16-19 个权重层，并使用了大小为 3×3 和 1×1 的小型卷积过滤器。</p><p>论文：用于大规模图像识别的非常深度的卷积网络（Very Deep Convolutional Networks for Large-Scale Image Recognition）</p><h4 id="word2vec"><a class="markdownIt-Anchor" href="#word2vec"></a> word2vec</h4><p>word2vec 是一种试图通过预测文档中话语的上下文来学习词向量（word embedding）的算法和工具 (<a href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a>)。最终得到的词矢量（word vector）有一些有趣的性质，例如vector(‘queen’) ~= vector(‘king’) - vector(‘man’) + vector(‘woman’) （女王~=国王-男人+女人）。两个不同的目标函数可以用来学习这些嵌入：Skip-Gram 目标函数尝试预测一个词的上下文，CBOW 目标函数则尝试从词上下文预测这个词。</p><p>论文：向量空间中词汇表征的有效评估（Efficient Estimation of Word Representations in Vector Space）<br />论文：分布式词汇和短语表征以及他们的组合性（Distributed Representations of Words and Phrases and their Compositionality）<br />论文：解释 word2vec 参数学习（word2vec Parameter Learning Explained）</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识点 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
