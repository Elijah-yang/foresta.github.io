<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Cross-domain vulnerability detection using graph embedding and domain adaptation</title>
      <link href="/2023/12/19/Papers/Vul/Cross-domain%20vulnerability%20detection%20using%20graph%20embedding%20and%20domain%20adaptation/"/>
      <url>/2023/12/19/Papers/Vul/Cross-domain%20vulnerability%20detection%20using%20graph%20embedding%20and%20domain%20adaptation/</url>
      
        <content type="html"><![CDATA[<p>a山东省计算机网络重点实验室，山东省计算机科学中心（济南国家超级计算机中心），齐鲁工业大学（山东科学院），济南250014，中国b北京邮电大学网络空间安全学院，北京100876，中国c公共大数据国家重点实验室，贵州大学计算机科学与技术学院，贵阳550025</p><h1 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0 Abstract"></a>0 Abstract</h1><p>漏洞检测是维护网络空间安全的有效手段。机器学习方法由于其准确性和自动化的优势，在软件安全领域引起了人们的广泛关注。<strong>然而，目前的研究主要集中在训练数据和测试数据属于同一域的域内漏洞检测上。</strong>由于应用场景、编码习惯等因素，不同软件项目中的漏洞可能服从不同的概率分布。当机器学习方法应用于一个全新的项目时，这种差异会影响它们的性能。为了解决这个冷启动问题，我们<strong>提出了一个使用图嵌入和深度域自适应（VulGDA）的跨域漏洞检测框架。</strong>它以多种跨域方式工作，包括<strong>零样本方式</strong>，即目标域中没有标记数据可用于训练。将VulGDA分解为图嵌入和域自适应。在图嵌入阶段，我们将源代码中的样本转换为图表示，其中元素根据其语法和语义关系直接连接。然后，我们将来自图中定义的邻居和边的信息聚合为实值向量。通过图形嵌入，VulGDA提取了全面的漏洞特征，并解决了长期依赖性的挑战。针对训练数据和测试数据之间的差异，使用域自适应来训练特征生成器。该特征生成器将嵌入的图映射到一个“深层”特征，该特征对漏洞检测具有鉴别性，并且对域之间的移动保持不变。我们进行了一项系统实验来验证VulGDA的有效性。结果表明，将图嵌入和深域自适应相结合，提高了VulGDA在跨域漏洞检测中的性能。与最先进的方法相比，我们的方法在冷启动条件下具有更好的性能。</p><h1 id="1-Contributions"><a href="#1-Contributions" class="headerlink" title="1 Contributions"></a>1 Contributions</h1><p>主要问题就是目前的研究针对某一数据集或是某些特定的漏洞，没有在跨域问题上提出适当的解决方法；</p><ul><li><p>提出了一个结合了图嵌入和域自适应的跨域漏洞检测框架VuLGDA，VulGDA以ZeroShot方式工作，这是跨域检测中最严格的方式。VulGDA通过将目标域中的监督信息添加到训练数据中，也适用于few-shot和域内方式。</p></li><li><p>提出了一种提取综合漏洞特征的图嵌入方法；减少噪声对漏洞特征的影响</p></li><li><p>提出了一种神经网络来学习特征生成器。该特征生成器通过减少分类损失和域差异来进行优化。最后，该特征生成器将学习到的源域中的漏洞模式传输到目标域。</p></li><li><p>实现了一个原型并进行了系统的实验。结果表明，图嵌入和域自适应的应用提高了VulGDA的性能。与现有方法相比，VulGDA在跨域漏洞检测方面取得了更好的性能。</p></li></ul><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2 Method"></a>2 Method</h1><p>VulGDA旨在检测现实世界软件中的漏洞。为了实用，本文考虑了最严格的条件。也就是说，应用在源域中训练的检测模型来检测目标域中的漏洞，并且该模型在训练期间无法从目标域获得任何监督信息（零样本）。这一假设使VulGDA具有良好的泛化能力，并易于应用于其他跨域检测情况，例如历史漏洞数据很少（很少发生）或丰富（Indomain）的项目。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191701536.png" alt="image-20231219170117488"></p><p>问题定义：假设我们的检测架构可以访问源域中的标记数据集$S&#x3D;{(x_{i},y_{i})}<em>{i&#x3D;1}^{n}\sim(D</em>{s})^{n}$  和目标域中的未标记数据集 $T&#x3D;{({x}<em>{i})}</em>{i&#x3D;n+1}^{N}\sim(D_{T}^{X})^{N-n}$。$N$是训练样本的总数，$D_T^X$是$D_T$在$X$上的边缘分布。</p><p>最终目标是建立一个预测$η：X→Y$：最小$R_{D_T}(η)&#x3D;\operatorname*{Pr}<em>{(x y)\sim D</em>{T}}(\eta(x)\neq y),$，而$D_T$中没有监督信息。</p><p><strong>VuLGDA有两个阶段组成：</strong></p><p><strong>图嵌入：</strong></p><p>图嵌入阶段的目的是将源代码中的漏洞样本转换为可由机器学习模型处理的实值向量。因此，通过句法和语义分析，图嵌入阶段首先将样本转换为CPG，即图结构中的中间表示。在CPG中，语法和语义相关的元素被直接连接在一起，这缓解了长期的依赖问题和域之间的分歧。然后，利用预训练的单词嵌入生成标记嵌入和节点嵌入。最后，我们使用GGNN通过传播和聚合来自CPG中定义的邻居的信息来获得嵌入向量。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191747401.png" alt="image-20231219174742348"></p><p><strong>域自适应：</strong></p><p>域自适应阶段旨在学习在域之间转换分布的特征生成器。提出了一种由特征生成器、分类器和瓶颈组成的神经网络来学习该特征生成器。将源域中的特征作为输入的分类器产生分类损失。瓶颈度量源域和目标域中的特征之间的差异。通过最小化分类损失和领域差异来优化特征生成器。检测结果由生成的“深层”特征训练的分类器报告。经过上述阶段后，VulGDA可以在目标域中检测跨域漏洞，而无需标记数据。</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> Vulnerabilities </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安全 </tag>
            
            <tag> Vulnerabilities </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conv2d</title>
      <link href="/2023/12/19/AILearning/pytorch/conv2d/"/>
      <url>/2023/12/19/AILearning/pytorch/conv2d/</url>
      
        <content type="html"><![CDATA[<h4 id="1-用法"><a href="#1-用法" class="headerlink" title="1 用法"></a>1 用法</h4><ul><li><h5 id="Conv2d-in-channels-out-channels-kernel-size-stride-1-padding-0-dilation-1-groups-1-bias-True-padding-mode-‘zeros’"><a href="#Conv2d-in-channels-out-channels-kernel-size-stride-1-padding-0-dilation-1-groups-1-bias-True-padding-mode-‘zeros’" class="headerlink" title="Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1,padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1,bias&#x3D;True, padding_mode&#x3D;‘zeros’)"></a>Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1,padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1,bias&#x3D;True, padding_mode&#x3D;‘zeros’)</h5></li></ul><h4 id="2-参数"><a href="#2-参数" class="headerlink" title="2 参数"></a>2 参数</h4><ul><li><h5 id="in-channels：输入的通道数目-【必选】"><a href="#in-channels：输入的通道数目-【必选】" class="headerlink" title="in_channels：输入的通道数目 【必选】"></a>in_channels：输入的通道数目 【必选】</h5></li><li><h5 id="out-channels：-输出的通道数目-【必选】"><a href="#out-channels：-输出的通道数目-【必选】" class="headerlink" title="out_channels： 输出的通道数目 【必选】"></a>out_channels： 输出的通道数目 【必选】</h5></li><li><h5 id="kernel-size：卷积核的大小，类型为int-或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示-高和宽。【必选】"><a href="#kernel-size：卷积核的大小，类型为int-或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示-高和宽。【必选】" class="headerlink" title="kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】"></a>kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】</h5></li><li><h5 id="stride：-卷积每次滑动的步长为多少，默认是-1-【可选】"><a href="#stride：-卷积每次滑动的步长为多少，默认是-1-【可选】" class="headerlink" title="stride： 卷积每次滑动的步长为多少，默认是 1 【可选】"></a>stride： 卷积每次滑动的步长为多少，默认是 1 【可选】</h5></li><li><h5 id="padding：-设置在所有边界增加-值为-0-的边距的大小（也就是在feature-map-外围增加几圈-0-），例如当-padding-1-的时候，如果原来大小为-3-×-3-，那么之后的大小为-5-×-5-。即在外围加了一圈-0-。【可选】"><a href="#padding：-设置在所有边界增加-值为-0-的边距的大小（也就是在feature-map-外围增加几圈-0-），例如当-padding-1-的时候，如果原来大小为-3-×-3-，那么之后的大小为-5-×-5-。即在外围加了一圈-0-。【可选】" class="headerlink" title="padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding &#x3D;1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】"></a>padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding &#x3D;1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】</h5></li><li><h5 id="dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】"><a href="#dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】" class="headerlink" title="dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】"></a>dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】</h5></li><li><p>groups：控制输入和输出之间的连接。（不常用）【可选】</p><p>举例来说：<br>比如 groups 为1，那么所有的输入都会连接到所有输出<br>当 groups 为 2的时候，相当于将输入分为两组，并排放置两层，每层看到一半的输入通道并产生一半的输出通道，并且两者都是串联在一起的。这也是参数字面的意思：“组” 的含义。<br>需要注意的是，in_channels 和 out_channels 必须都可以整除 groups，否则会报错（因为要分成这么多组啊，除不开你让人家程序怎么办？）</p></li><li><h5 id="bias：-是否将一个-学习到的-bias-增加输出中，默认是-True-。【可选】"><a href="#bias：-是否将一个-学习到的-bias-增加输出中，默认是-True-。【可选】" class="headerlink" title="bias： 是否将一个 学习到的 bias 增加输出中，默认是 True 。【可选】"></a>bias： 是否将一个 学习到的 bias 增加输出中，默认是 True 。【可选】</h5></li><li><p>padding_mode ： 字符串类型，接收的字符串只有 “zeros” 和 “circular”。【可选】</p></li></ul><h4 id="3-相关形状"><a href="#3-相关形状" class="headerlink" title="3 相关形状"></a>3 相关形状</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191632440.png" alt="image-20231219163239412"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch入门</title>
      <link href="/2023/12/19/AILearning/pytorch/pytorch/"/>
      <url>/2023/12/19/AILearning/pytorch/pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="1-torch-nn简介"><a href="#1-torch-nn简介" class="headerlink" title="1.torch.nn简介"></a>1.torch.nn简介</h1><p>1.1torch.nn相关库的导入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#环境准备</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np              <span class="comment"># numpy数组库</span></span><br><span class="line"><span class="keyword">import</span> math                     <span class="comment"># 数学运算库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图库</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch             <span class="comment"># torch基础库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn    <span class="comment"># torch神经网络库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-2-torch-nn概述"><a href="#1-2-torch-nn概述" class="headerlink" title="1.2 torch.nn概述"></a>1.2 torch.nn概述</h3><blockquote><p>Pytorch提供了几个设计得非常棒的模块和类，比如 torch.nn，torch.optim，Dataset 以及 DataLoader，来帮助程序员设计和训练神经网络。</p></blockquote><p>nn是Neural Network的简称，帮助程序员方便执行如下的与神经网络相关的行为：</p><ul><li><p>创建神经网络</p></li><li><p>训练神经网络</p></li><li><p>保存神经网络</p></li><li><p>恢复神经网络</p></li></ul><p>包括五大基本功能模块</p><ul><li>torch.nn是专门为神经网络设计的模块化接口</li><li>nn构建于autograd之上，可以用来定义和运行神经网络<ul><li>nn.Parameter</li><li>nn.Linear</li><li>nn.functional</li><li>nn.Module</li><li>nn.Sequential</li></ul></li></ul><h1 id="2-nn-Linear类（全连接层）"><a href="#2-nn-Linear类（全连接层）" class="headerlink" title="2.nn.Linear类（全连接层）"></a>2.nn.Linear类（全连接层）</h1><h3 id="2-1函数功能"><a href="#2-1函数功能" class="headerlink" title="2.1函数功能"></a>2.1函数功能</h3><p>用于创建一个多输入、多输出的全连接层。</p><p>nn.Linear本身并不包含激活函数（Functional）</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635002.png" alt="image-20231219163552911"></p><h3 id="2-2函数说明"><a href="#2-2函数说明" class="headerlink" title="2.2函数说明"></a>2.2函数说明</h3><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629041.png" alt="image-20211203155643310"></p><ul><li>in_featrues：<ul><li>指输入的二维张量的大小，即输入的[batch_size, size]中的size</li><li>in_features的数量，决定的参数的个数Y&#x3D;WX+b，X的维度就是in_features，X的维度决定W的维度，总参数的个数 &#x3D; in_features + 1</li></ul></li><li>out_featrues<ul><li>指的是输出的二维张量的大小，&#x3D;&#x3D;即输出的二维张量的形状为[batch_size output_size].&#x3D;&#x3D;</li><li>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。<strong>多少个输出，就需要多少个神经元</strong>。</li><li>&#x3D;&#x3D;从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。&#x3D;&#x3D;</li></ul></li></ul><h3 id="2-3多个全连接层构建全连接网络"><a href="#2-3多个全连接层构建全连接网络" class="headerlink" title="2.3多个全连接层构建全连接网络"></a>2.3多个全连接层构建全连接网络</h3><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629046.png" alt="image-20211203160715687"></p><ul><li>使用nn.Linear类创建全连接层</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.Linear</span></span><br><span class="line"><span class="comment"># 建立单层的多输入、多输出全连接层</span></span><br><span class="line"><span class="comment"># in_features由输入张量的形状决定，out_features则决定了输出张量的形状 </span></span><br><span class="line">full_connect_layer = nn.Linear(in_features = <span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>, out_features = <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;full_connect_layer:&quot;</span>, full_connect_layer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;parameters        :&quot;</span>, full_connect_layer.parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定输入的图像形状为[28,28,1]</span></span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将四维张量转换为二维张量之后，才能作为全连接层的输入</span></span><br><span class="line">x_input = x_input.view(<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用全连接层</span></span><br><span class="line">y_output = full_connect_layer(x_input) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output.shape:&quot;</span>, y_output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_output:&quot;</span>, y_output)</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629071.png" alt="image-20211203163313390"></p><h2 id="3-nn-functional（常见函数）"><a href="#3-nn-functional（常见函数）" class="headerlink" title="3 nn.functional（常见函数）"></a>3 nn.functional（常见函数）</h2><h3 id="3-1-nn-functional概述"><a href="#3-1-nn-functional概述" class="headerlink" title="3.1 nn.functional概述"></a>3.1 nn.functional概述</h3><blockquote><p>nn.functional定义了创建神经网络所需要的一些常见的处理函数。如没有激活函数的神经元，各种激活函数等。</p></blockquote><ul><li>包含torch.nn库中所有函数，包含大量loss和activation function<ul><li>torch.nn.functional.conv2d(input, weight, bias&#x3D;None, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1)</li><li>nn.functional.xxx是函数接口</li><li>nn.functional.xxx无法与nn.Sequential结合使用</li><li>没有学习参数的(eg. maxpool, loss_ func, activation func)等根据个人选择使用nn.functional.xxx或nn.Xxx</li><li>需要特别注意dropout层</li></ul></li></ul><h3 id="3-2-nn-functional函数分类"><a href="#3-2-nn-functional函数分类" class="headerlink" title="3.2 nn.functional函数分类"></a>3.2 nn.functional函数分类</h3><p>nn.functional包括神经网络前向和后向处理所需要到的常见函数</p><ul><li>神经元处理函数</li><li>激活函数</li></ul><h3 id="3-3-激活函数案例"><a href="#3-3-激活函数案例" class="headerlink" title="3.3 激活函数案例"></a>3.3 激活函数案例</h3><ul><li>relu案例</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.relu( )</span></span><br><span class="line"><span class="built_in">print</span>(y_output)</span><br><span class="line">out = nn.functional.relu(y_output)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><h2 id="4-nn-xxx和nn-functional-xxx比较"><a href="#4-nn-xxx和nn-functional-xxx比较" class="headerlink" title="4 nn.xxx和nn.functional.xxx比较"></a>4 nn.xxx和nn.functional.xxx比较</h2><h3 id="4-1-相同点"><a href="#4-1-相同点" class="headerlink" title="4.1 相同点"></a>4.1 相同点</h3><ul><li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout，。。。。。；</li><li>运行效率也是近乎相同。</li></ul><h3 id="4-2-不同点"><a href="#4-2-不同点" class="headerlink" title="4.2 不同点"></a>4.2 不同点</h3><ul><li><code>nn.functional.xxx</code>是API函数接口，而<code>nn.Xxx</code>是对原始API函数<code>nn.functional.xxx</code>的类封装。</li><li>所有<code>nn.Xxx</code>都继承于于共同祖先<code>nn.Module</code>。这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code> 等。</li><li><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</li><li><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。<code>nn.functional.xxx</code>同时传入输入数据和<code>weight, bias</code>等其他参数 。</li><li><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</li></ul><h2 id="5-nn-Parameter类"><a href="#5-nn-Parameter类" class="headerlink" title="5 nn.Parameter类"></a>5 nn.Parameter类</h2><h3 id="5-1-nn-Parameter概述"><a href="#5-1-nn-Parameter概述" class="headerlink" title="5.1 nn.Parameter概述"></a>5.1 nn.Parameter概述</h3><blockquote><p>Parameter实际上也是tensor，也就是说是一个多维矩阵，是Variable类的一个特殊类。</p><p>当我们创建一个model时，nn会自动创建相应的参数parameter，并会自动累加到模型的Parameter 成员列表中。</p></blockquote><h3 id="5-2-单个全连接层中参数的个数"><a href="#5-2-单个全连接层中参数的个数" class="headerlink" title="5.2 单个全连接层中参数的个数"></a>5.2 单个全连接层中参数的个数</h3><p>in_features的数量，决定的参数的个数   Y &#x3D; WX + b,  X的维度就是in_features，X的维度决定的W的维度， 总的参数个数 &#x3D; in_features + 1</p><p>out_features的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。</p><p>多少个输出，就需要多少个神经元。</p><p>总的W参数的个数&#x3D;  in_features * out_features</p><p>总的b参数的个数&#x3D;  1 * out_features</p><p>总的参数（W和B）的个数&#x3D;  (in_features + 1) * out_features</p><h3 id="5-3-使用参数创建全连接层"><a href="#5-3-使用参数创建全连接层" class="headerlink" title="5.3 使用参数创建全连接层"></a>5.3 使用参数创建全连接层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191629076.png" alt="image-20211203165921227"></p><h2 id="6-nn-Module类"><a href="#6-nn-Module类" class="headerlink" title="6 nn.Module类"></a>6 nn.Module类</h2><ul><li>抽象概念，既可以表示神经网络中的某个层layer，也可以表示一个包含很多层的神经网络</li><li>modle.parameters()</li><li>modle.buffers()</li><li>modle.state_dict()</li><li>modle.modules()</li><li>forward(),to()</li></ul><h2 id="7-利用nn-Sequential类创建神经网络（继承与nn-Module类）"><a href="#7-利用nn-Sequential类创建神经网络（继承与nn-Module类）" class="headerlink" title="7 利用nn.Sequential类创建神经网络（继承与nn.Module类）"></a>7 利用nn.Sequential类创建神经网络（继承与nn.Module类）</h2><blockquote><p>nn.Sequential是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在Sequential类对象的数据结构中，同时以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，Sequential可以看成是有多个函数运算对象，串联成的神经网络，其返回的是Module类型的神经网络对象。</p></blockquote><h2 id="8-自定义神经网络模型类（继承于Module类）"><a href="#8-自定义神经网络模型类（继承于Module类）" class="headerlink" title="8.自定义神经网络模型类（继承于Module类）"></a>8.自定义神经网络模型类（继承于Module类）</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络模型：带relu的两层全连接神经网络</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;自定义新的神经网络模型的类&quot;</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NetC</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#定义前向运算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># -1表示自动匹配</span></span><br><span class="line">        h1 = self.h1(x)</span><br><span class="line">        a1 =  self.relu1(h1)</span><br><span class="line">        out = self.out(a1)</span><br><span class="line">        a_out = self.softmax(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = NetC(<span class="number">28</span>*<span class="number">28</span>, <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch tensor操作</title>
      <link href="/2023/12/19/AILearning/pytorch/tensor%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/12/19/AILearning/pytorch/tensor%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="一、张量的基本操作"><a href="#一、张量的基本操作" class="headerlink" title="一、张量的基本操作"></a>一、张量的基本操作</h1><p>Pytorch 中，张量的操作分为<strong>结构操作和数学运算</strong>，其理解就如字面意思。结构操作就是改变张量本身的结构，数学运算就是对张量的元素值完成数学运算。</p><ul><li>常使用的张量结构操作：维度变换（tranpose、view 等）、合并分割（split、chunk等）、索引切片（index_select、gather 等）。</li><li>常使用的张量数学运算：标量运算、向量运算、矩阵运算。</li></ul><h1 id="二、维度变换"><a href="#二、维度变换" class="headerlink" title="二、维度变换"></a>二、维度变换</h1><h2 id="2-1-squeeze-vs-unsqueeze-维度增减"><a href="#2-1-squeeze-vs-unsqueeze-维度增减" class="headerlink" title="2.1 squeeze vs unsqueeze 维度增减"></a><strong>2.1 squeeze vs unsqueeze 维度增减</strong></h2><ul><li>**squeeze()**：对 tensor 进行维度的压缩，去掉维数为 1 的维度。用法：torch.squeeze(a) 将 a 中所有为 1 的维度都删除，或者 a.squeeze(1) 是去掉 a中指定的维数为 1 的维度。</li><li>**unsqueeze()**：对数据维度进行扩充，给指定位置加上维数为 1 的维度。用法：torch.unsqueeze(a, N)，或者 a.unsqueeze(N)，在 a 中指定位置 N 加上一个维数为 1 的维度。</li></ul><p>squeeze 用例程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.squeeze(a)</span><br><span class="line">c = a.squeeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br></pre></td></tr></table></figure><p>程序输出结果如下：</p><blockquote><p>torch.Size([3, 3]) torch.Size([1, 3, 3])</p></blockquote><p>unsqueeze 用例程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">y1 = torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line">y2 = x.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y1.shape)</span><br><span class="line"><span class="built_in">print</span>(y2.shape)</span><br></pre></td></tr></table></figure><p>程序输出结果如下：</p><blockquote><p>torch.Size([1, 3, 3]) torch.Size([1, 3, 3])</p></blockquote><h2 id="2-2-transpose-vs-permute-维度交换"><a href="#2-2-transpose-vs-permute-维度交换" class="headerlink" title="2.2 transpose vs permute 维度交换"></a><strong>2.2 transpose vs permute 维度交换</strong></h2><p>torch.transpose() 只能交换两个维度，而 .permute() 可以自由交换任意位置。函数定义如下：</p><ul><li>transpose(dim0, dim1) → Tensor # See torch.transpose()</li><li>permute(*dims) → Tensor # dim(int). Returns a view of the original tensor with its dimensions permuted.</li></ul><p>在 CNN 模型中，我们经常遇到交换维度的问题，举例：四个维度表示的 tensor：[batch, channel, h, w]（nchw），如果想把 channel 放到最后去，形成[batch, h, w, channel]（nhwc），如果使用 torch.transpose() 方法，至少要交换两次（先 1 3 交换再 1 2 交换），而使用 .permute() 方法只需一次操作，更加方便。例子程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">32</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 3, 28, 32]</span></span><br><span class="line"><span class="built_in">print</span>(b.transpose(<span class="number">1</span>, <span class="number">3</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 32, 28, 3])</span></span><br><span class="line"><span class="built_in">print</span>(b.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 32, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 28, 32, 3]</span></span><br></pre></td></tr></table></figure><h2 id="2-3-reshape-vs-view"><a href="#2-3-reshape-vs-view" class="headerlink" title="2.3 reshape vs view"></a><strong>2.3 reshape vs view</strong></h2><blockquote><p>view只适合对满足连续性条件（contiguous）的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。view能干的reshape都能干，如果view不能干就可以用reshape来处理。更多可看[1]</p></blockquote><h2 id="2-4-einsum"><a href="#2-4-einsum" class="headerlink" title="2.4 einsum"></a><strong>2.4 einsum</strong></h2><p>首先看下 einsum 实现矩阵乘法的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 语义解析：</span></span><br><span class="line"><span class="comment"># 输入a：2阶张量，下标为ik</span></span><br><span class="line"><span class="comment"># 输入b: 2阶张量，下标为kj</span></span><br><span class="line"><span class="comment"># 输出o: 2阶张量，下标为i和j</span></span><br><span class="line">c = torch.einsum(<span class="string">&quot;ik,kj-&gt;ij&quot;</span>, [a, b])</span><br><span class="line"><span class="comment"># 等价操作 torch.mm(a, b)</span></span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">60.</span>).reshape(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">b = np.arange(<span class="number">24.</span>).reshape(<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 语义解析：</span></span><br><span class="line"><span class="comment"># 输入a：3阶张量，下标为ijk</span></span><br><span class="line"><span class="comment"># 输入b: 3阶张量，下标为jil</span></span><br><span class="line"><span class="comment"># 输出o: 2阶张量，下标为k和l</span></span><br><span class="line">c = np.einsum(<span class="string">&#x27;ijk,jil-&gt;kl&#x27;</span>, a, b)</span><br></pre></td></tr></table></figure><p>这个方法可以实现矩阵乘法，但是也可以用来更换维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 语义解析:</span></span><br><span class="line"><span class="comment"># 当后面只有一个张量时，就是对自己维度进行变换</span></span><br><span class="line"><span class="comment"># 比较常用的就是将chanel换到最后</span></span><br><span class="line">x = torch.einsum(<span class="string">&#x27;nchw-&gt;nhwc&#x27;</span>, x)</span><br></pre></td></tr></table></figure><blockquote><p>更多可看[2]</p></blockquote><h1 id="三、索引切片"><a href="#三、索引切片" class="headerlink" title="三、索引切片"></a>三、索引切片</h1><h2 id="3-1-规则索引切片方式"><a href="#3-1-规则索引切片方式" class="headerlink" title="3.1 规则索引切片方式"></a><strong>3.1 规则索引切片方式</strong></h2><p>张量的索引切片方式和 numpy、python 多维列表几乎一致，都可以通过索引和切片对部分元素进行修改。切片时支持缺省参数和省略号。实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] <span class="comment"># 第 1 行数据</span></span><br><span class="line">tensor([<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">2</span>][<span class="number">2</span>]</span><br><span class="line">tensor(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>:<span class="number">3</span>,:]  <span class="comment"># 第1至第3行，全部列</span></span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>:<span class="number">2</span>,:]  <span class="comment"># 第1行至第2行</span></span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">1</span>:,-<span class="number">1</span>]  <span class="comment"># 第2行至最后行，最后一列</span></span><br><span class="line">tensor([<span class="number">9</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">1</span>:,::<span class="number">2</span>] <span class="comment"># 第1行至最后行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure><p>以上切片方式相对规则，对于不规则的切片提取,可以使用 torch.index_select, torch.take, torch.gather, torch.masked_select。</p><h2 id="3-2-gather-和-torch-index-select-算子"><a href="#3-2-gather-和-torch-index-select-算子" class="headerlink" title="3.2 gather 和 torch.index_select 算子"></a><strong>3.2 gather 和 torch.index_select 算子</strong></h2><blockquote><p>gather 算子的用法比较难以理解，在翻阅了官方文档和网上资料后，我有了一些自己的理解。</p></blockquote><p>1，gather 是不规则的切片提取算子（Gathers values along an axis specified by dim. 在指定维度上根据索引 index 来选取数据）。函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, *, sparse_grad=<span class="literal">False</span>, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p><strong>参数解释：</strong></p><ul><li>input (Tensor) – the source tensor.</li><li>dim (int) – the axis along which to index.</li><li>index (LongTensor) – the indices of elements to gather.</li></ul><p>对于 3D tensor，output 值的定义如下： gather 的官方定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]   <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure><p>下面结合 2D 和 3D tensor 的用例来直观理解算子用法。<br>（1）对于 2D tensor 的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">0</span>, <span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])  <span class="comment"># 选取对角线元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(a, <span class="number">0</span>, index)</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure><p>output 值定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照 index = tensor([[0, 1, 2, 3]])顺序作用在行上索引依次为0,1,2,3</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">a[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">5</span></span><br><span class="line">a[<span class="number">2</span>][<span class="number">2</span>] = <span class="number">10</span></span><br><span class="line">a[<span class="number">3</span>][<span class="number">3</span>] = <span class="number">15</span></span><br></pre></td></tr></table></figure><p>（2）索引更复杂的 2D tensor 例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>output 值的计算如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output[i][j] = <span class="built_in">input</span>[i][index[i][j]]  <span class="comment"># if dim = 1</span></span><br><span class="line">output[<span class="number">0</span>][<span class="number">0</span>] = <span class="built_in">input</span>[<span class="number">0</span>][index[<span class="number">0</span>][<span class="number">0</span>]] = <span class="built_in">input</span>[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">output[<span class="number">0</span>][<span class="number">1</span>] = <span class="built_in">input</span>[<span class="number">0</span>][index[<span class="number">0</span>][<span class="number">1</span>]] = <span class="built_in">input</span>[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">output[<span class="number">1</span>][<span class="number">0</span>] = <span class="built_in">input</span>[<span class="number">1</span>][index[<span class="number">1</span>][<span class="number">0</span>]] = <span class="built_in">input</span>[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">4</span></span><br><span class="line">output[<span class="number">1</span>][<span class="number">1</span>] = <span class="built_in">input</span>[<span class="number">1</span>][index[<span class="number">1</span>][<span class="number">1</span>]] = <span class="built_in">input</span>[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">3</span></span><br></pre></td></tr></table></figure><p>总结：<strong>可以看到 gather 是通过将索引在指定维度 dim 上的值替换为 index 的值，但是其他维度索引不变的情况下获取 tensor 数据</strong>。直观上可以理解为对矩阵进行重排，比如对每一行(dim&#x3D;1)的元素进行变换，比如 torch.gather(a, 1, torch.tensor([[1,2,0], [1,2,0]])) 的作用就是对 矩阵 a 每一行的元素，进行 permtute(1,2,0) 操作。</p><p>2，理解了 gather 再看 index_select 就很简单，函数作用是返回沿着输入张量的指定维度的指定索引号进行索引的张量子集。函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>函数返回一个新的张量，它使用数据类型为 LongTensor 的 index 中的条目沿维度 dim 索引输入张量。返回的张量具有与原始张量（输入）相同的维数。 维度尺寸与索引长度相同； 其他尺寸与原始张量中的尺寸相同。实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>,  <span class="number">0.2647</span>, -<span class="number">0.1228</span>, -<span class="number">1.1068</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">0</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">1</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>, -<span class="number">0.5414</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>, -<span class="number">0.1228</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>,  <span class="number">0.7230</span>]])</span><br></pre></td></tr></table></figure><h1 id="四、合并分割"><a href="#四、合并分割" class="headerlink" title="四、合并分割"></a>四、合并分割</h1><h2 id="4-1-torch-cat-和-torch-stack"><a href="#4-1-torch-cat-和-torch-stack" class="headerlink" title="4.1 torch.cat 和 torch.stack"></a><strong>4.1 torch.cat 和 torch.stack</strong></h2><p>可以用 torch.cat 方法和 torch.stack 方法将多个张量合并，也可以用 torch.split方法把一个张量分割成多个张量。torch.cat 和 torch.stack 有略微的区别，torch.cat 是连接，不会增加维度，而 torch.stack 是堆叠，会增加一个维度。两者函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</span></span><br><span class="line">torch.cat(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"><span class="comment"># Concatenates a sequence of tensors along **a new** dimension. All tensors need to be of the same size.</span></span><br><span class="line">torch.stack(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>torch.cat 和 torch.stack 用法实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">0</span>,<span class="number">9</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.arange(<span class="number">10</span>,<span class="number">19</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.arange(<span class="number">20</span>,<span class="number">29</span>).view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat_abc = torch.cat([a,b,c], dim=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(cat_abc.shape)</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(cat_abc)</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">        [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack_abc = torch.stack([a,b,c], axis=<span class="number">0</span>)  <span class="comment"># torch中dim和axis参数名可以混用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(stack_abc.shape)</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(stack_abc)</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">         [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">         [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chunk_abc = torch.chunk(cat_abc, <span class="number">3</span>, dim=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chunk_abc</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]]),</span><br><span class="line"> tensor([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]),</span><br><span class="line"> tensor([[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>],</span><br><span class="line">         [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>],</span><br><span class="line">         [<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]]))</span><br></pre></td></tr></table></figure><h2 id="4-2-torch-split-和-torch-chunk"><a href="#4-2-torch-split-和-torch-chunk" class="headerlink" title="4.2 torch.split 和 torch.chunk"></a><strong>4.2 torch.split 和 torch.chunk</strong></h2><p>torch.split() 和 torch.chunk() 可以看作是 torch.cat() 的逆运算。split() 作用是将张量拆分为多个块，每个块都是原始张量的视图。split() 函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Splits the tensor into chunks. Each chunk is a view of the original tensor.</span></span><br><span class="line"><span class="string">If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.</span></span><br><span class="line"><span class="string">If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>chunk() 作用是将 tensor 按 dim（行或列）分割成 chunks 个 tensor 块，返回的是一个元组。chunk() 函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>) → <span class="type">List</span> of Tensors</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.</span></span><br><span class="line"><span class="string">Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.</span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    input (Tensor) – the tensor to split</span></span><br><span class="line"><span class="string">    chunks (int) – number of chunks to return</span></span><br><span class="line"><span class="string">    dim (int) – dimension along which to split the tensor</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>实例代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">10</span>).reshape(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(a, <span class="number">2</span>)</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">3</span>]]),</span><br><span class="line"> tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">          [<span class="number">6</span>, <span class="number">7</span>]]),</span><br><span class="line"> tensor([[<span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(a, [<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>]]),</span><br><span class="line"> tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">(tensor([[<span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">8</span>]]), </span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">5</span>],</span><br><span class="line">        [<span class="number">7</span>],</span><br><span class="line">        [<span class="number">9</span>]]))</span><br></pre></td></tr></table></figure><h1 id="五、卷积相关算子"><a href="#五、卷积相关算子" class="headerlink" title="五、卷积相关算子"></a>五、卷积相关算子</h1><h2 id="5-1-上采样方法总结"><a href="#5-1-上采样方法总结" class="headerlink" title="5.1 上采样方法总结"></a><strong>5.1 上采样方法总结</strong></h2><p>上采样大致被总结成了三个类别：</p><ol><li>基于线性插值的上采样：最近邻算法（nearest）、双线性插值算法（bilinear）、双三次插值算法（bicubic）等，这是传统图像处理方法。</li><li>基于深度学习的上采样（转置卷积，也叫反卷积 Conv2dTranspose2d等）</li><li>Unpooling 的方法（简单的补零或者扩充操作）<br>计算效果：最近邻插值算法 &lt; 双线性插值 &lt; 双三次插值。计算速度：最近邻插值算法 &gt; 双线性插值 &gt; 双三次插值。</li></ol><h2 id="5-2-F-interpolate-采样函数"><a href="#5-2-F-interpolate-采样函数" class="headerlink" title="5.2 F.interpolate 采样函数"></a><strong>5.2 F.interpolate 采样函数</strong></h2><blockquote><p>Pytorch 老版本有 nn.Upsample 函数，新版本建议用 torch.nn.functional.interpolate，一个函数可实现定制化需求的上采样或者下采样功能，。</p></blockquote><p>F.interpolate() 函数全称是 torch.nn.functional.interpolate()，函数定义如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interpolate</span>(<span class="params"><span class="built_in">input</span>, size=<span class="literal">None</span>, scale_factor=<span class="literal">None</span>, mode=<span class="string">&#x27;nearest&#x27;</span>, align_corners=<span class="literal">None</span>, recompute_scale_factor=<span class="literal">None</span></span>):  <span class="comment"># noqa: F811</span></span><br><span class="line">    <span class="comment"># type: (Tensor, <span class="type">Optional</span>[<span class="built_in">int</span>], <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]], <span class="built_in">str</span>, <span class="type">Optional</span>[<span class="built_in">bool</span>], <span class="type">Optional</span>[<span class="built_in">bool</span>]) -&gt; Tensor</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>参数解释如下：</p><ul><li>input(Tensor)：输入张量数据；</li><li>size： 输出的尺寸，数据类型为 tuple： ([optional D_out], [optional H_out], W_out)，和 scale_factor 二选一；</li><li>scale_factor：在高度、宽度和深度上面的放大倍数。数据类型既可以是 int——表明高度、宽度、深度都扩大同一倍数；也可是tuple——指定高度、宽度、深度等维度的扩大倍数；</li><li>mode： 上采样的方法，包括最近邻（nearest），线性插值（linear），双线性插值（bilinear），三次线性插值（trilinear），默认是最近邻（nearest）；</li><li>align_corners： 如果设为True，输入图像和输出图像角点的像素将会被对齐（aligned），这只在mode &#x3D; linear, bilinear, or trilinear才有效，默认为False。</li></ul><p>例子程序如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">y = F.interpolate(x * <span class="number">2</span>, scale_factor=(<span class="number">2</span>, <span class="number">2</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)   <span class="comment"># torch.Size([3, 224, 224])</span></span><br></pre></td></tr></table></figure><h2 id="5-3-nn-ConvTranspose2d-反卷积"><a href="#5-3-nn-ConvTranspose2d-反卷积" class="headerlink" title="5.3 nn.ConvTranspose2d 反卷积"></a><strong>5.3 nn.ConvTranspose2d 反卷积</strong></h2><p>转置卷积（有时候也称为反卷积，个人觉得这种叫法不是很规范），它是一种特殊的卷积，先 padding 来扩大图像尺寸，紧接着跟正向卷积一样，旋转卷积核 180 度，再进行卷积计算。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[0] zhuanlan.zhihu.com&#x2F;p&#x2F;<br>[1] <a href="https://blog.csdn.net/Flag_ing/article/details/109129752">https://blog.csdn.net/Flag_ing/article/details/109129752</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch tensor 计算</title>
      <link href="/2023/12/19/AILearning/pytorch/tensor%E8%AE%A1%E7%AE%97/"/>
      <url>/2023/12/19/AILearning/pytorch/tensor%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h4 id="torch-mean"><a href="#torch-mean" class="headerlink" title="torch.mean()"></a>torch.mean()</h4><blockquote><p>mean()函数的参数：dim&#x3D;0,按行求平均值，返回的形状是（1，列数）；dim&#x3D;1,按列求平均值，返回的形状是（行数，1）,默认不设置dim的时候，返回的是所有元素的平均值。</p></blockquote><h4 id="torch-pow"><a href="#torch-pow" class="headerlink" title="torch.pow()"></a>torch.pow()</h4><blockquote><p>功能: 实现张量和标量之间逐元素求指数操作, 或者在可广播的张量之间逐元素求指数操作.</p></blockquote><h4 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h4><blockquote><p>官方解释：沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><p>注：<code>python</code>的序列数据只有<code>list</code>和<code>tuple</code>。</p><p>浅显说法：把多个2维的张量凑成一个3维的张量；多个3维的凑成一个4维的张量…以此类推，也就是在增加新的维度进行堆叠。</p><p>outputs &#x3D; torch.stack(inputs, dim&#x3D;?) → Tensor</p></blockquote><h4 id="torch-clamp"><a href="#torch-clamp" class="headerlink" title="torch.clamp()"></a>torch.clamp()</h4><blockquote><p>torch.clamp(input, min, max, out&#x3D;None) → Tensor</p><p>将输入<code>input</code>张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。</p></blockquote><h4 id="torch-bmm"><a href="#torch-bmm" class="headerlink" title="torch.bmm()"></a>torch.bmm()</h4><blockquote><p>计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,m) 也就是说两个tensor的第一维是相等的，然后第一个数组的第三维和第二个数组的第二维度要求一样，对于剩下的则不做要求，输出维度 （b,h,m）;</p></blockquote><h4 id="torch-squeeze-函数"><a href="#torch-squeeze-函数" class="headerlink" title="torch.squeeze()函数"></a>torch.squeeze()函数</h4><blockquote><p>torch.squeeze(input, dim&#x3D;None, out&#x3D;None) </p><p>squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中 input 中大小为1的所有维都已删除。</p><p>举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)</p><p>当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作。</p><p>举个例子：如果 input 的形状为 (A×1×B)，squeeze(input, 0)后，返回的tensor不变；squeeze(input, 1)后，返回的tensor将被压缩为 (A×B)</p></blockquote><h4 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a>torch.unsqueeze()</h4><blockquote></blockquote><h4 id="torch-spmm"><a href="#torch-spmm" class="headerlink" title="torch.spmm"></a>torch.spmm</h4><blockquote><p>torch.spmm只支持 sparse 在前，dense 在后的矩阵乘法，两个sparse相乘或者dense在前的乘法不支持，当然两个dense矩阵相乘是支持的。</p></blockquote><h4 id="torch-sum"><a href="#torch-sum" class="headerlink" title="torch.sum"></a>torch.sum</h4><blockquote><p> <img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191625912.png" alt="img"></p><p> 在dim这个维度上，对里面的tesnor 进行加和，如果keepdim&#x3D;False，返回结果会删去dim这个维度。因为在dim上加和之后，dim&#x3D;1，所以可以直接删去。</p></blockquote><h4 id="torch-diag"><a href="#torch-diag" class="headerlink" title="torch.diag"></a>torch.diag</h4><blockquote><p>对角矩阵</p></blockquote><h4 id="torch-concat"><a href="#torch-concat" class="headerlink" title="torch.concat"></a>torch.concat</h4><blockquote><p>torch.cat ( (A, B), dim&#x3D;0)接受一个由两个（或多个）tensor组成的元组，按行拼接，所以两个（多个）tensor的列数要相同。</p><p>torch.cat ( (A, B), dim&#x3D;1)是按列拼接，所以两个tensor的行数要相同。</p></blockquote><h4 id="torch-view"><a href="#torch-view" class="headerlink" title="torch.view"></a>torch.view</h4><blockquote><p>在PyTorch中<strong>view</strong>函数作用为重构张量的维度，相当于numpy中的resize()的功能，但是用法不太一样;</p><p>torch.view(参数a,参数b,…..)，其中参数a&#x3D;3,参数b&#x3D;2决定了将一维的tt1重构成3*2维的张量。<br>有时候会出现torch.view(-1)或者torch.view(参数a,-1)这种情况。则-1参数是需要估算的。</p><p><strong>view()函数的功能与reshape类似，用来转换size大小。x &#x3D; x.view(batchsize, -1)中batchsize指转换后有几行，而-1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数。</strong></p><p>之前对于pytorch的网络编程学习都是大致理解每一层的概念，有些语法语句没有从原理上弄清楚，就比如标题的x &#x3D; x.view(x.size(0), -1)  。</p><p>这句话一般出现在model类的forward函数中，具体位置一般都是在调用分类器之前。分类器是一个简单的nn.Linear()结构，输入输出都是维度为一的值，x &#x3D; x.view(x.size(0), -1)  这句话的出现就是为了将前面多维度的tensor展平成一维。</p></blockquote><h4 id="torch-permute"><a href="#torch-permute" class="headerlink" title="torch.permute"></a>torch.permute</h4><blockquote><p>permute（dims）<br>参数dims用矩阵的维数代入，一般默认从0开始。即第0维，第1维等等<br>也可以理解为，第0块，第1块等等。当然矩阵最少是两维才能使用permute<br>如是两维，dims分别为是0和1<br>可以写成permute（0,1）这里不做任何变化，维数与之前相同<br>如果写成permute（1,0）得到的就是矩阵的转置<br>如果三维是permute(0,1,2)<br>0代表共有几块维度：本例中0对应着3块矩阵<br>1代表每一块中有多少行：本例中1对应着每块有2行<br>2代表每一块中有多少列：本例中2对应着每块有5列<br>所以是3块2行5列的三维矩阵</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习率衰减方法</title>
      <link href="/2023/12/19/AILearning/pytorch/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/"/>
      <url>/2023/12/19/AILearning/pytorch/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="Pytorch-学习率衰减方法"><a href="#Pytorch-学习率衰减方法" class="headerlink" title="Pytorch 学习率衰减方法"></a><a href="https://so.csdn.net/so/search?q=Pytorch&spm=1001.2101.3001.7020">Pytorch</a> 学习率衰减方法</h3><h1 id="1-什么是学习率衰减"><a href="#1-什么是学习率衰减" class="headerlink" title="1.什么是学习率衰减"></a>1.什么是学习率衰减</h1><p><a href="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&spm=1001.2101.3001.7020">梯度下降</a>算法需要我们指定一个学习率作为权重更新步幅的控制因子，常用的学习率有0.01、0.001以及0.0001等，学习率越大则权重更新。一般来说，<strong>我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些，使得网络更好的收敛到最优解。</strong><br>Pytorch中有两种学习率调整(衰减)方法：<br>（1）使用<a href="https://so.csdn.net/so/search?q=%E5%BA%93%E5%87%BD%E6%95%B0&spm=1001.2101.3001.7020">库函数</a>进行调整；<br>（2）手动调整。</p><h1 id="2-使用库函数进行调整"><a href="#2-使用库函数进行调整" class="headerlink" title="2.使用库函数进行调整"></a>2.使用库函数进行调整</h1><p>Pytorch学习率调整策略通过 <a href="https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020">torch</a>.optim.lr_sheduler 接口实现。pytorch提供的学习率调整策略分为三大类，分别是：<br>（1）有序调整：等间隔调整(Step)，多间隔调整(MultiStep)，指数衰减(Exponential)，余弦退火(CosineAnnealing);<br>（2）自适应调整：依训练状况伺机而变，通过监测某个指标的变化情况(loss、accuracy)，当该指标不怎么变化时，就是调整学习率的时机(ReduceLROnPlateau);<br>（3）自定义调整：通过自定义关于epoch的lambda函数调整学习率(LambdaLR)。<br>在每个epoch的训练中，使用scheduler.step()语句进行学习率更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        ......</span><br><span class="line">        y_ = model(x)</span><br><span class="line">        loss = criterion(y_,y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        ......</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    train(...)</span><br><span class="line">    test(...)</span><br><span class="line">    scheduler.step()</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></table></figure><h2 id="2-1-有序调整"><a href="#2-1-有序调整" class="headerlink" title="2.1.有序调整"></a>2.1.有序调整</h2><h3 id="2-1-1等间隔调整学习率"><a href="#2-1-1等间隔调整学习率" class="headerlink" title="2.1.1等间隔调整学习率"></a>2.1.1等间隔调整学习率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>每训练step_size个epoch，学习率调整为lr&#x3D;lr*gamma.<br>以下内容中都将epoch和step对等，因为每个epoch中只进行一次scheduler.step()，实则该step指scheduler.step()中的step, 即step_size指scheduler.step()进行的次数。<br>参数</p><ul><li>optimizer: 神经网络训练中使用的优化器，如optimizer&#x3D;torch.optim.SGD(…)</li><li>step_size(int): 学习率下降间隔数，单位是epoch，而不是iteration.</li><li>gamma(float):学习率调整倍数，默认为0.1</li><li>last_epoch(int)：上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整；当为-1时，学习率设置为初始值。<br><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635978.png" alt="img"></li></ul><h3 id="2-1-2-多间隔调整学习率"><a href="#2-1-2-多间隔调整学习率" class="headerlink" title="2.1.2.多间隔调整学习率"></a>2.1.2.多间隔调整学习率</h3><p>跟2.1类似，但学习率调整的间隔并不是相等的，如epoch&#x3D;10时调整一次，epoch&#x3D;30时调整一次，epoch&#x3D;80时调整一次…</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_shceduler.MultiStepLR(optimizer, milestones, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>milestone(list): 一个列表参数，表示多个学习率需要调整的epoch值，如milestones&#x3D;[10, 30, 80].</li><li>其它参数同(1)。<br><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635994.png" alt="img"></li></ul><h3 id="2-1-3-指数衰减调整学习率-ExponentialLR"><a href="#2-1-3-指数衰减调整学习率-ExponentialLR" class="headerlink" title="2.1.3.指数衰减调整学习率 ExponentialLR"></a>2.1.3.指数衰减调整学习率 ExponentialLR</h3><p>学习率呈指数型衰减，每训练一个epoch，lr&#x3D;lr×γepoch</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>gamma(float)：学习率调整倍数的底数，指数为epoch，初始值我lr, 倍数为γepoch</li><li>其它参数同上。<br><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635065.png" alt="img"></li></ul><h3 id="2-1-4-余弦退火函数调整学习率"><a href="#2-1-4-余弦退火函数调整学习率" class="headerlink" title="2.1.4.余弦退火函数调整学习率"></a>2.1.4.余弦退火函数调整学习率</h3><p>学习率呈余弦函数型衰减，并以2×Tmax为余弦函数周期，epoch&#x3D;0对应余弦型学习率调整曲线的x&#x3D;0，ymax&#x3D;lr，epoch&#x3D;Tmax对应余弦型学习率调整曲线的x&#x3D;Π，ymin&#x3D;etamin处，随着epoch&gt;Tmax，学习率随epoch增加逐渐上升，整个走势同cos(x)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=<span class="number">0</span>, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>Tmax(int):学习率下降到最小值时的epoch数，即当epoch&#x3D;T_max时，学习率下降到余弦函数最小值，当epoch&gt;T_max时，学习率将增大；</li><li>etamin: 学习率调整的最小值，即epoch&#x3D;Tmax时，lrmin&#x3D;etamin, 默认为0.</li><li>其它参数同上。<br><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635073.png" alt="img"></li></ul><h2 id="2-2-根据指标调整学习率ReduceLROnPlateau"><a href="#2-2-根据指标调整学习率ReduceLROnPlateau" class="headerlink" title="2.2.根据指标调整学习率ReduceLROnPlateau"></a>2.2.根据指标调整学习率ReduceLROnPlateau</h2><p>当<strong>某指标(loss或accuracy)在最近几个epoch中都没有变化(下降或升高超过给定阈值)时</strong>，调整学习率。<br>如当验证集的loss不再下降是，调整学习率；或监察验证集的accuracy不再升高时，调整学习率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">&#x27;min&#x27;</span>, factor=<span class="number">0.1</span>,</span><br><span class="line"> patience=<span class="number">10</span>,verbose=<span class="literal">False</span>, threshold=<span class="number">0.0001</span>, threshold_mode=<span class="string">&#x27;rel&#x27;</span>, cooldown=<span class="number">0</span>, </span><br><span class="line"> min_lr=<span class="number">0</span>, eps=<span class="number">1e-08</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>mode(str): 模式选择，有min和max两种模式，min表示当指标不再降低(如监测loss)，max表示当指标不再升高(如监测accuracy)。</li><li>factor(float): 学习率调整倍数，同前面的gamma，当监测指标达到要求时，lr&#x3D;lr×factor。</li><li>patience(int): 忍受该指标多少个epoch不变化，当忍无可忍时，调整学习率。</li><li>verbose(bool): 是否打印学习率信息，print( ‘Epoch {:5d} reducing learning rate of group {} to {:.4e}.’.format(epoch, i, new_lr), 默认为False, 即不打印该信息。</li><li>threshold_mode (str): 选择判断指标是否达最优的模式，有两种模式：rel 和 abs.<br>当threshold_mode &#x3D;&#x3D; rel, 并且 mode &#x3D;&#x3D; max时，dynamic_threshold &#x3D; best * (1 + threshold);<br>当threshold_mode &#x3D;&#x3D; rel, 并且 mode &#x3D;&#x3D; min时，dynamic_threshold &#x3D; best * (1 - threshold);<br>当threshold_mode &#x3D;&#x3D; abs, 并且 mode &#x3D;&#x3D; max时，dynamic_threshold &#x3D; best + threshold;<br>当threshold_mode &#x3D;&#x3D; abs, 并且 mode &#x3D;&#x3D; min时，dynamic_threshold &#x3D; best - threshold;<br>threshold(float): 配合threshold_mode使用。</li><li>cooldown(int): “冷却时间”，当调整学习率之后，让学习率调整策略冷静一下，让模型在训练一段时间，再重启监测模式</li><li>min_lr(float or list): 学习率下限，可为float，或者list，当有多个参数组时，可用list进行设置。</li><li>eps(float): 学习率衰减的最小值，当学习率的变化值小于eps时，则不调整学习率。</li></ul><h2 id="2-3-自定义调整学习率"><a href="#2-3-自定义调整学习率" class="headerlink" title="2.3.自定义调整学习率"></a>2.3.自定义调整学习率</h2><p>为不同参数组设定不同学习率调整策略。调整规则为：<br>lr &#x3D; base_lr * lambda(self.last_epoch)<br>在fine-tune中特别有用，<strong>我们不仅可以为不同层设置不同的学习率，还可以为不同层设置不同的学习率调整策略。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>lr_lambda(function or list): 自定义计算学习率调整倍数的函数，通常时epoch的函数，当有多个参数组时，设为list.</li><li>其它参数同上。</li></ul><h1 id="3-手动调整学习率"><a href="#3-手动调整学习率" class="headerlink" title="3.手动调整学习率"></a>3.手动调整学习率</h1><p>手动调整学习率，通常可以定义如下函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sets the learning rate to the initial LR decayed by 10 every 30 epochs&quot;&quot;&quot;</span></span><br><span class="line">    lr = args.lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>))</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>又如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adjust_learning_rate</span>(<span class="params">epoch, lr</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch &lt;= <span class="number">81</span>:  <span class="comment"># 32k iterations</span></span><br><span class="line">      <span class="keyword">return</span> lr</span><br><span class="line">    <span class="keyword">elif</span> epoch &lt;= <span class="number">122</span>:  <span class="comment"># 48k iterations</span></span><br><span class="line">      <span class="keyword">return</span> lr/<span class="number">10</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> lr/<span class="number">100</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>该函数通过修改每个epoch下，各参数组中的lr来进行学习率手动调整，用法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    lr = adjust_learning_rate(optimizer, epoch)  <span class="comment"># 调整学习率</span></span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">    ......</span><br><span class="line">    optimizer.step()  <span class="comment"># 采用新的学习率进行参数更新</span></span><br></pre></td></tr></table></figure><p>梯度下降算法需要我们指定一个学习率作为权重更新步幅的控制因子，常用的学习率有0.01、0.001以及0.0001等，学习率越大则权重更新。一般来说，<strong>我们希望在训练初期学习率大一些，使得网络收敛迅速，在训练后期学习率小一些</strong>，使得网络更好的收敛到最优解。下图展示了随着迭代的进行动态调整学习率的4种策略曲线：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635068.jpg" alt="img"></p><p>上述4种策略为自己根据资料整理得到的衰减类型：指数衰减、固定步长的衰减、多步长衰、余弦退火衰减。下面逐一介绍其性质，及pytorch对应的使用方式，需要注意学习率衰减策略很大程度上是<strong>依赖于经验与具体问题的</strong>，不能照搬参数。</p><p>*<strong>1、指数衰减*</strong></p><p>学习率按照指数的形式衰减是比较常用的策略，我们首先需要确定需要针对哪个优化器执行学习率动态调整策略，也就是首先定义一个优化器：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_ExpLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br></pre></td></tr></table></figure><p>定义好优化器以后，就可以给这个优化器绑定一个指数衰减学习率控制器：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ExpLR = torch.optim.lr_scheduler.ExponentialLR(optimizer_ExpLR, gamma=0.98)</span><br></pre></td></tr></table></figure><p>其中<strong>参数gamma表示衰减的底数，选择不同的gamma值可以获得幅度不同的衰减曲线</strong>，如下：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635071.jpg" alt="img"></p><p>*<strong>2、固定步长衰减*</strong></p><p>有时我们希望学习率每隔一定步数（或者epoch）就减少为原来的gamma分之一，使用固定步长衰减依旧先定义优化器，再给优化器绑定StepLR对象：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_StepLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">StepLR = torch.optim.lr_scheduler.StepLR(optimizer_StepLR, step_size=step_size, gamma=0.65)</span><br></pre></td></tr></table></figure><p>其中gamma参数表示衰减的程度，step_size参数表示每隔多少个step进行一次学习率调整，下面对比了不同gamma值下的学习率变化情况：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635399.jpg" alt="img"></p><p>*<strong>3、多步长衰减*</strong></p><p>上述固定步长的衰减的虽然能够按照固定的区间长度进行学习率更新<strong>，但是有时我们希望不同的区间采用不同的更新频率，或者是有的区间更新学习率，有的区间不更新学习率</strong>，这就需要使用MultiStepLR来实现动态区间长度控制：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_MultiStepLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">torch.optim.lr_scheduler.MultiStepLR(optimizer_MultiStepLR,</span><br><span class="line">                    milestones=[200, 300, 320, 340, 200], gamma=0.8)</span><br></pre></td></tr></table></figure><p>其中milestones参数为表示学习率更新的起止区间，在区间[0. 200]内学习率不更新，而在[200, 300]、[300, 320]…..[340, 400]的右侧值都进行一次更新；gamma参数表示学习率衰减为上次的gamma分之一。其图示如下：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635419.jpg" alt="img"></p><p>从图中可以看出，学习率在区间[200， 400]内快速的下降，这就是milestones参数所控制的，在milestones以外的区间学习率始终保持不变。</p><p>*<strong>4、余弦退火衰减*</strong></p><p>严格的说，余弦退火策略不应该算是学习率衰减策略，因为它使得学习率按照周期变化，其定义方式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer_CosineLR = torch.optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line">CosineLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_CosineLR, T_max=150, eta_min=0)</span><br></pre></td></tr></table></figure><p>其包含的参数和余弦知识一致，参数T_max表示余弦函数周期；eta_min表示学习率的最小值，默认它是0表示学习率至少为正值。确定一个余弦函数需要知道最值和周期，其中周期就是T_max，最值是初试学习率。下图展示了不同周期下的余弦学习率更新曲线：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191635437.jpg" alt="img"></p><p>*<strong>5、上述4种学习率动态更新策略的说明*</strong></p><p>4个负责学习率调整的类：StepLR、ExponentialLR、MultiStepLR和CosineAnnealingLR，其完整对学习率的更新都是在其step()函数被调用以后完成的，这个step表达的含义可以是一次迭代，当然更多情况下应该是一个epoch以后进行一次scheduler.step()，这根据具体问题来确定。此外，根据pytorch官网上给出的说明，scheduler.step()函数的调用应该在训练代码以后：</p><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">scheduler = ...</span><br><span class="line">&gt;&gt;&gt; for epoch in range(100):</span><br><span class="line">&gt;&gt;&gt;     train(...)</span><br><span class="line">&gt;&gt;&gt;     validate(...)</span><br><span class="line">&gt;&gt;&gt;     scheduler.step()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch环境</title>
      <link href="/2023/12/19/Programmer/python/torch%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
      <url>/2023/12/19/Programmer/python/torch%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191226420.png" alt="image-20231219122612389"></p><h1 id="1-安装对应的torch、torchvision"><a href="#1-安装对应的torch、torchvision" class="headerlink" title="1.安装对应的torch、torchvision"></a>1.安装对应的torch、torchvision</h1><p>网址：<a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a></p><p>搜索对应CUDA版本的安装命令（cu110代表CUDA11.0），在终端中复制命令安装。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191135284.png" alt="image-20231219113547226"></p><p>查看是否安装成功</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__) </span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) </span><br></pre></td></tr></table></figure><h1 id="2-安装torch-geometric"><a href="#2-安装torch-geometric" class="headerlink" title="2.安装torch-geometric"></a>2.安装torch-geometric</h1><p>网址：<a href="https://pytorch-geometric.com/whl/">https://pytorch-geometric.com/whl/</a></p><p>找到对应pytorch版本：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191137168.png" alt="image-20231219113754141"></p><p>四个库（cluster,scatter,sparse,spline-conv）分别：wget 网页中对应的链接并 pip install 下载好的whl包，即完成安装：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191139386.png" alt="image-20231219113927345"></p><p>注意自己环境的python版本以及linux&#x2F;win就行 </p><p>安装完上面四个库后执行 pip install torch-geometric </p><p>以上安装完成。</p><p>完成之后 import torch-geometric 发现报错，报错信息：<strong>“No module named ‘torch.profiler”</strong></p><p>原因是torch1.10以上的版本才有<strong>torch.profiler</strong>这个库，但是Torch网址CUDA11.0兼容的选项没有torch1.10以上，那怎么办呢？</p><p>解决：</p><p>找到报错路径里的文件<strong>profile.py</strong></p><p>作如下修改：（原文件是第八行，改成了第九行）</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191140609.png" alt="image-20231219114012578"></p><h1 id="3-DGL安装"><a href="#3-DGL安装" class="headerlink" title="3 DGL安装"></a>3 DGL安装</h1><p>安装DGL无需安装torch-geometric，需要安装那四个依赖库</p><p><a href="https://www.dgl.ai/pages/start.html">Deep Graph Library (dgl.ai)</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n mVul python=3.7</span><br><span class="line"></span><br><span class="line">pip install torch==1.5.0+cu102 torchvision==0.6.0+cu102 torchaudio==0.5.0 -f https://download.pytorch.org/whl/cu102/torch_stable.html</span><br><span class="line"></span><br><span class="line">torch 1.5.0</span><br><span class="line">torchgeometric</span><br><span class="line">pip install networkx==2.5</span><br><span class="line">pip install dgl -f https://data.dgl.ai/wheels/cu102/repo.html</span><br><span class="line">https://data.dgl.ai/wheels/cu113/repo.html</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python环境</title>
      <link href="/2023/12/19/Programmer/python/python%E7%8E%AF%E5%A2%83/"/>
      <url>/2023/12/19/Programmer/python/python%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h1 id="1-conda虚拟环境"><a href="#1-conda虚拟环境" class="headerlink" title="1 conda虚拟环境"></a>1 conda虚拟环境</h1><h4 id="conda常用命令"><a href="#conda常用命令" class="headerlink" title="conda常用命令"></a>conda常用命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda list # 查看当前虚拟环境已经安装的包（激活虚拟环境后使用）</span><br><span class="line">conda env list # 查看当前存在哪些虚拟环境</span><br><span class="line">conda update # conda 检查更新当前conda</span><br></pre></td></tr></table></figure><h4 id="conda创建虚拟环境"><a href="#conda创建虚拟环境" class="headerlink" title="conda创建虚拟环境"></a>conda创建虚拟环境</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n xxx python=3.6</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxx为所创建虚拟环境的名字</span></span><br></pre></td></tr></table></figure><h4 id="conda激活和退出虚拟环境（windows）"><a href="#conda激活和退出虚拟环境（windows）" class="headerlink" title="conda激活和退出虚拟环境（windows）"></a>conda激活和退出虚拟环境（windows）</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate xx # (虚拟环境名称）</span><br><span class="line"></span><br><span class="line">conda deactivate </span><br></pre></td></tr></table></figure><h4 id="conda为当前虚拟环境安装新的包"><a href="#conda为当前虚拟环境安装新的包" class="headerlink" title="conda为当前虚拟环境安装新的包"></a>conda为当前虚拟环境安装新的包</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install -n package_name==所需版本 #（版本不指定则默认最新版）</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可使用临时镜像安装加快速度，例如安装numpy：</span></span><br><span class="line"></span><br><span class="line">conda install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy</span><br></pre></td></tr></table></figure><h4 id="conda删除虚拟环境或者虚拟环境中的某个包"><a href="#conda删除虚拟环境或者虚拟环境中的某个包" class="headerlink" title="conda删除虚拟环境或者虚拟环境中的某个包"></a>conda删除虚拟环境或者虚拟环境中的某个包</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove -n name --all</span><br><span class="line">conda remove --name env_name package_name </span><br></pre></td></tr></table></figure><h4 id="conda环境复制"><a href="#conda环境复制" class="headerlink" title="conda环境复制"></a>conda环境复制</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n new_name --clone path</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">path为所需要复制的环境路径，可根据conda <span class="built_in">env</span> list查看路径</span></span><br></pre></td></tr></table></figure><h1 id="2-安装依赖库"><a href="#2-安装依赖库" class="headerlink" title="2 安装依赖库"></a>2 安装依赖库</h1><h2 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h2><p>pip 是最为广泛使用的 Python 包管理器，可以帮助我们获得最新的 Python 包并进行管理。常用命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install [package-name]              # 安装名为[package-name]的包</span><br><span class="line">pip install [package-name]==X.X         # 安装名为[package-name]的包并指定版本X.X</span><br><span class="line">pip install [package-name] --proxy=代理服务器IP:端口号         # 使用代理服务器安装</span><br><span class="line">pip install [package-name] --upgrade    # 更新名为[package-name]的包</span><br><span class="line">pip uninstall [package-name]            # 删除名为[package-name]的包</span><br><span class="line">pip list                                # 列出当前环境下已安装的所有包</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">代码示例：</span></span><br><span class="line">pip install spyder -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">-i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下面介绍常见的国内源镜像：</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清华：https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里云：http://mirrors.aliyun.com/pypi/simple/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">华中理工大学：http://pypi.hustunique.com/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">山东理工大学：http://pypi.sdutlinux.org/</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">豆瓣：http://pypi.douban.com/simple/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h2><p>conda 包管理器是 Anaconda 自带的包管理器，可以帮助我们在 conda 环境下轻松地安装各种包。相较于 pip 而言，conda 的通用性更强（不仅是 Python 包，其他包如 CUDA Toolkit 和 cuDNN 也可以安装），但 conda 源的版本更新往往较慢。常用命令如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install [package-name]        # 安装名为[package-name]的包</span><br><span class="line">conda install [package-name]=X.X    # 安装名为[package-name]的包并指定版本X.X</span><br><span class="line">conda update [package-name]         # 更新名为[package-name]的包</span><br><span class="line">conda remove [package-name]         # 删除名为[package-name]的包</span><br><span class="line">conda list                          # 列出当前环境下已安装的所有包</span><br><span class="line">conda search [package-name]         # 列出名为[package-name]的包在conda源中的所有可用版本</span><br></pre></td></tr></table></figure><p><strong>conda镜像</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看conda当前设置</span></span><br><span class="line">conda config --show channels</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重置默认镜像源</span></span><br><span class="line">conda config --remove-key channels</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除单个镜像源</span></span><br><span class="line">conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">国内镜像</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清华大学镜像</span></span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">中科大镜像</span></span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">conda config --add channels http://mirrors.ustc.edu.cn/anaconda/cloud/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里镜像</span></span><br><span class="line">conda config --add channels http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 环境 </tag>
            
            <tag> pip </tag>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex OCR</title>
      <link href="/2023/12/18/Tools/Latex-OCR/"/>
      <url>/2023/12/18/Tools/Latex-OCR/</url>
      
        <content type="html"><![CDATA[<blockquote><p>LaTeX-OCR 是一个开源的光学字符识别（OCR）软件，专为 LaTeX 文档提供支持。其主要目的是帮助用户将扫描的文档转换为 LaTeX 编辑器可以使用的可编辑文本，从而方便进行修改、编辑和排版。</p></blockquote><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191215516.png" alt="image-20231219121553484"></p><h1 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h1><p>LaTeX-OCR可以从源码进行安装，也可以直接用pip来安装，源码地址：<a href="https://github.com/lukas-blecher/LaTeX-OCR">https://github.com/lukas-blecher/LaTeX-OCR</a> ，这里直接使用pip安装，为了方便管理环境，使用conda创建虚拟环境。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n latex python=3.10</span><br><span class="line">conda activate latex</span><br><span class="line">pip install &quot;pix2tex[gui]&quot;</span><br><span class="line">pip install &quot;pix2tex[gui]&quot; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>注：使用pip清华镜像源更快哦~</p><h1 id="2-启动与使用"><a href="#2-启动与使用" class="headerlink" title="2.启动与使用"></a>2.启动与使用</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在虚拟环境下执行</span><br><span class="line">pix2tex</span><br></pre></td></tr></table></figure><p>首次执行会下载依赖模型；</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190005945.png" alt="image-20231219000535903"></p><p>期间可能报错，连接断开，尝试重试；</p><p>使用：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190008897.png" alt="image-20231219000831872"></p><p>输入h 回车查看帮助：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190009822.png" alt="image-20231219000913782"></p><p>可以看到windows或macos下可以非常丝滑地使用，只需要：</p><ul><li>截图或复制一个图片到memory，可以理解为复制到剪贴板；</li><li>回到终端按回车，即可看到公式：<img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190012621.png" alt="image-20231219001159694"></li><li>复制内容到LaTex块即可；</li></ul><p>$$<br>m i n\sum_{i&#x3D;1}^{n}{\mathcal{L}}(f(G_{i},Y_{i}|V_{i}))<br>$$<br>$$<br>\Omega_{j}&#x3D;tanh({\bf W}<em>{h}\left({\bf r}</em>{j}\odot\Omega_{S}\right)+\left[\nabla_{h}\mathrm{e}<em>{j}+\mathrm{b}</em>{h}\right),<br>$$</p><p>是不是很好用呢，无限制的，非常良心的工具，简直是福祉。</p>]]></content>
      
      
      <categories>
          
          <category> Tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LineVD_Statement-level Vulnerability Detection using Graph Neural Networks</title>
      <link href="/2023/12/18/Papers/Vul/LineVD-Statement-level-Vulnerability-Detection-using-Graph-Neural-Networks/"/>
      <url>/2023/12/18/Papers/Vul/LineVD-Statement-level-Vulnerability-Detection-using-Graph-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<h2 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0 Abstract"></a>0 Abstract</h2><p>当前基于机器学习的软件漏洞检测方法主要在功能级别进行。然而，这些方法的一个关键限制是，它们没有指示导致漏洞的特定代码行。这限制了开发人员有效检查和解释学习模型预测的能力，这对于将基于机器学习的工具集成到软件开发工作流中至关重要。基于图的模型在功能级漏洞检测方面表现出了良好的性能，但其在语句级漏洞检测中的能力尚未得到广泛探索。<strong>虽然通过可解释的人工智能解释功能级预测是一个很有前途的方向，但我们在这里从完全监督学习的角度来考虑语句级软件漏洞检测任务。</strong>我们提出了一种新的深度学习框架LineVD，它将语句级漏洞检测定义为节点分类任务。<strong>LineVD利用图神经网络和基于转换器的模型对原始源代码标记进行编码，从而利用语句之间的控制和数据依赖性。</strong>特别是，通过解决函数级和语句级信息之间的冲突输出，LineVD显著提高了函数代码在没有漏洞状态的情况下的预测性能。我们针对从多个真实世界项目中获得的大量真实世界C&#x2F;C++漏洞进行了广泛的实验，并证明F1分数比当前最先进的技术提高了105%.</p><h2 id="1-Intro-or-Overview"><a href="#1-Intro-or-Overview" class="headerlink" title="1 Intro or Overview"></a>1 Intro or Overview</h2><h4 id="1-1-Problem-and-Challenge"><a href="#1-1-Problem-and-Challenge" class="headerlink" title="1.1 Problem and Challenge"></a>1.1 Problem and Challenge</h4><p>自动化SVD大致可分为两类：</p><p>（1）传统方法，包括静态和动态分析；</p><p>（2）数据驱动解决方案，利用数据挖掘和机器学习来预测软件漏洞；</p><p>尽管当前的数据驱动方法在识别软件漏洞方面取得了成功，但它们往往局限于粗粒度水平。模型输出通常为开发人员提供有限的预测结果验证和解释信息，导致在评估和缓解软件漏洞时付出额外努力。</p><p>许多SVD解决方案已经从文件级过渡到函数级或切片级预测，其他一些工作进一步利用补充信息，如提交级别的代码更改以及附带的日志消息，来构建预测模型。虽然目标是帮助从业者对有缺陷的代码进行优先级排序，但漏洞通常可以局限于几个关键行。因此，审查大型函数仍然可能是一个相当大的负担。</p><h4 id="1-2-Motivation"><a href="#1-2-Motivation" class="headerlink" title="1.2 Motivation"></a>1.2 Motivation</h4><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312182057555.png" alt="image-20231218205705483" style="zoom:67%;" /><p>为了节省空间，我们从一个较小的函数中选择一个漏洞，该函数包含Linux内核（CVE-2018-12896）中的整数溢出漏洞，该漏洞最终可能被利用来导致拒绝服务。通过显式语句级别的预测，可以更容易地解释为什么函数被预测为易受攻击（或者，验证预测是否错误）。语句级SVD模型将第22行上的加法赋值操作标记为最可疑的，该操作包含易受攻击的整数强制转换操作，使开发人员能够更有效地验证和减轻该漏洞。</p><p>先前的工作利用GNNExplainer来导出易受攻击的语句作为模型的解释，以展示在语句级的工作。然而，在我们的工作中，我们发现在对潜在的脆弱性语句进行分类和排序时，性能是不充分和有效的。或者，我们旨在探索在语句级别直接训练和预测漏洞以进行SVD粒度细化的可行性和有效性，这将允许数据驱动的解决方案以完全监督的方式直接利用任何可用的语句级别信息。</p><h4 id="1-3-Contribution"><a href="#1-3-Contribution" class="headerlink" title="1.3 Contribution"></a>1.3 Contribution</h4><ul><li>提出了一种新颖有效的语句级SVD方法，LineVD实现了显著的改进，F1得分增加了105%；</li><li>研究了构建基于GNN的语句级SVD模型的每个阶段的性能影响，包括节点嵌入方法和GNN模型选择。根据研究结果，开发LineVD是为了通过同时学习功能和语句级别的信息，在很大程度上提高性能。</li><li>LineVD是第一种通过图神经网络联合学习函数级和语句级信息以提高SVD性能的方法，在经验评估中，它显著优于仅使用一种类型信息的传统模型。</li><li>发布了数据集、源代码和带有支持脚本的模型，这为未来的基准测试和比较工作提供了一个现成的实现解决方案。<a href="https://github.com/davidhin/linevd">https://github.com/davidhin/linevd</a></li></ul><h2 id="2-Architecture-Method"><a href="#2-Architecture-Method" class="headerlink" title="2 Architecture &amp; Method"></a>2 Architecture &amp; Method</h2><h4 id="2-1-System-Overview"><a href="#2-1-System-Overview" class="headerlink" title="2.1 System Overview"></a>2.1 System Overview</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312182110490.png" alt="image-20231218211008424"></p><p>首先将问题定义为，节点$V\rightarrow Y$的映射，也就是语句是否易受攻击。</p><p>通过学习最小化损失loss:<br>$$<br>min\sum_{i&#x3D;1}^nL(f(G_i, Y_i|V_i))<br>$$<br><strong>Feature Extraction</strong></p><p>LineVD将源代码的单个函数作为原始输入。通过处理函数并将其拆分为单独的语句Vi，首先通过CodeBERT的预训练BPE标记器对每个样本进行标记。在V＝{V1，V2，…，Vn}的集合之后，整个函数和包括该函数的各个语句被传递到CodeBERT中。因此，可以获得函数级和语句级的代码表示。</p><p>具体而言，LineVD分别嵌入了函数级和语句级代码，而不是为函数级嵌入聚合语句级嵌入。CodeBERT是一个双峰模型，这意味着除了函数代码本身之外，它还基于函数的自然语言描述进行了训练。作为输入，它使用一个特殊的分隔符标记来区分自然语言描述和函数代码。虽然函数的自然语言描述是不可访问的，但在这项工作中应用了文献中规定的一般操作，在每个输入前添加一个额外的分隔符标记，使描述为空白。对于CodeBERT的输出，我们使用了分类标记的嵌入，这适用于代码摘要任务。这使我们能够更好地利用CodeBERT模型强大的预训练源代码摘要功能。</p><p>总体而言，使用CodeBERT的LineVD的特征提取组件产生n+1个特征嵌入：一个嵌入用于整个函数，n个嵌入用于每个语句，我们分别表示为$Xv&#x3D;{x^v_1，x^v_2，…，x^v_n}$。</p><p><strong>Graph Construction</strong></p><p>在LineVD中，我们专注于数据和控制相关性信息，为此我们引入了图注意力网络（GAT）模型。如第2.2节所述，图神经网络（GNN）基于信息扩散机制学习图结构数据，而不是将信息压缩成平面向量，平面向量根据图的连通性更新节点状态，以保留重要信息，即拓扑依赖信息。</p><p><strong>Classifier Learning</strong></p><p>目标是训练一个可以同时从函数级和语句级代码中联合学习的模型。为了实现这一点，我们认为函数级和语句级代码片段对预测结果的贡献相等。因此，我们利用函数级CodeBERT嵌入的输入和从GAT层获得的语句嵌入，构建了一组共享的线性层和dropout层。</p><p>虽然易受攻击的语句可能足以指示函数易受攻击，但我们使用元素乘法构建LineVD，以进一步利用函数级别的信息进行训练。此外，这种操作和谐地平衡了函数级和语句级嵌入之间的冲突输出，并将证明某些场景的决策是合理的，即，如果函数级嵌入的输出类为零，那么所有语句级输出也为零。对此的直觉是，非易受攻击的函数不可能具有易受攻击的代码行。</p><h2 id="3-Experiment-and-Evaluation"><a href="#3-Experiment-and-Evaluation" class="headerlink" title="3 Experiment and Evaluation"></a>3 Experiment and Evaluation</h2><h4 id="3-1-DataSet-and-Process"><a href="#3-1-DataSet-and-Process" class="headerlink" title="3.1 DataSet and Process"></a>3.1 DataSet and Process</h4><p>最近的研究表明，SVD模型应该根据能够代表真实世界漏洞不同特征的数据进行评估[10]。这意味着评估从真实世界项目中提取的源代码（即非合成的），同时保持不平衡的比率，这是软件项目中漏洞固有的。在现实世界场景中应用时，使用不满足这些条件的数据集会导致模型性能的不一致。另一个数据集要求是足够多的样本，理想情况下跨越多个项目，以便获得一个可以很好地推广到看不见的代码的模型。最后一个要求是在语句级别访问基本事实标签，或可追溯到修复前代码，即原始gitcommit。</p><p><strong>Ground-Truth labels</strong></p><p>为了获得易受攻击和非易受攻击线路的基本事实标签，我们遵循文献[19，32]中的断言，而不是提出我们自己的启发式方法：（1）漏洞修复提交中删除的线路用作易受攻击的线路的指标，以及（2）所有依赖于添加线路的控制或数据的线路也被视为易受攻击。第二点的理由是，在漏洞修复提交中添加的任何行都是为了帮助修补漏洞。因此，在漏洞修复提交中未修改但与这些添加的行相关的行可以被视为与漏洞相关。为了获得与依赖于添加行的行相对应的标签，我们首先获得样本前后版本的代码变化，其中Big Vul中的样本指的是函数级代码片段。对于之前的版本，我们删除所有添加的行，对于之后的版本，删除所有删除的行。在这两种情况下，我们都保留空白占位符行，以确保行号的一致性。从后版本中提取的代码图可用于查找所有依赖于添加行的控制或数据行，这些行的行号对应于前版本。这组线可以与删除的线组合，以获得单个样本的最终脆弱线集。注释行被排除在代码图中，因此不用于训练或预测。这可以从图1和图2中看出；在这种情况下，只有一个修改的行，它被视为删除的行（22）和添加的行（23）。在这种情况下，前后版本的控制和数据依赖边恰好相同，因此我们可以使用图2来识别依赖于第23行的控制&#x2F;数据的行，即第3、19和21行。</p><p><strong>Cleaning</strong></p><p>原始数据集中的一些样本被错误地截断，导致代码样本无法解析且无效。例如，一个原本是50行的函数可能会因为没有明显原因而被错误地截断为40行。原因可能是数据集最初是如何构建的错误；然而，在整个数据集中只有30个这样的样本。我们使用80:10:10的随机训练&#x2F;验证&#x2F;测试分割比。对于训练集，我们对不可破坏样本的数量进行了不足采样，以在函数级别生成近似平衡的数据集，而测试和验证集保持原始的不平衡比例。我们选择在函数级别上平衡样本，因为在语句级别上进行平衡，同时保持函数中语句之间的上下文依赖关系是非常重要的。</p><h4 id="3-2-Evaluation"><a href="#3-2-Evaluation" class="headerlink" title="3.2 Evaluation"></a>3.2 <strong>Evaluation</strong></h4><ul><li><p>RQ1：与最先进的基于解释的SVD模型相比，LineVD可以实现多大的性能提升？</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190937348.png" alt="image-20231219093737314"></p></li><li><p>RQ2：不同的代码嵌入方法如何影响语句级漏洞检测？与其他粒度级别的SVD相比，语句级SVD的代码嵌入方法尚未得到探索。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190940483.png"></p></li><li><p>RQ3：图神经网络和函数级信息如何对LineVD性能做出贡献？使用图神经网络的信息传播对语句级SVD的影响还有待探索。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190942923.png"></p></li><li><p>RQ4：LineVD在跨项目分类场景中的表现如何？虽然在包含多个项目的数据集上进行训练已经减少了对模型通用性的歪曲，但来自同一项目的样本仍然有可能出现在训练集和测试集中。使用跨项目场景可以更好地表示模型在完全看不见的项目上的表现，而不仅仅是看不到的样本。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190943968.png" alt="image-20231219094311928"></p></li><li><p>RQ5：对于真实世界的数据，LineVD最能区分哪些语句类型？从语句类型的角度研究模型预测结果，特别是对于真实世界的数据，可以帮助了解模型在哪里表现最好，在哪里失败，这可以指导未来的工作和语句级SVD的改进。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312190943357.png" alt="image-20231219094343320"></p></li></ul><h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4 Conclusion"></a>4 Conclusion</h2><p>LineVD，一种用于语句级漏洞检测的新型深度学习方法，它可以让开发人员更有效地评估潜在的漏洞功能。LineVD通过在训练过程中利用图神经网络和语句级信息，在真实世界的开源项目中实现了最先进的语句级漏洞检测。与最新的基于细粒度机器学习的模型相比，这一显著改进表明了直接利用语句级信息进行语句级SVD的有效性。最后，LineVD实现了合理的跨项目性能，表明即使对于完全看不见的软件项目，它也具有有效性和泛化能力。未来的方向将包括探索替代的预训练特征嵌入方法和新的GNN架构，这些架构可以更好地适应软件源代码的底层性质和漏洞。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><aside> 💡 Others<hr>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
          <category> Vulnerabilities </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安全 </tag>
            
            <tag> Vulnerabilities </tag>
            
            <tag> 代码行级检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo bug</title>
      <link href="/2023/12/16/hello-world%20(copy)/"/>
      <url>/2023/12/16/hello-world%20(copy)/</url>
      
        <content type="html"><![CDATA[<p>这是一个bug指南，在使用Hexo的时候遇到了一些bug，自己在这里对解决过的bug做以记录。</p><h4 id="Nunjucks-Error-Line-26-Column-109-parseAggregate-expected-comma-after-expression"><a href="#Nunjucks-Error-Line-26-Column-109-parseAggregate-expected-comma-after-expression" class="headerlink" title="Nunjucks Error: [Line 26, Column 109] parseAggregate: expected comma after expression"></a>Nunjucks Error: [Line 26, Column 109] parseAggregate: expected comma after expression</h4><p>这个自己是hexo在render公式的时候产生的问题，网上查了一下是在latex公式中，不能讲两个”{“紧接着放在一起，应该{ {…} }在<strong>之间加空格</strong>才行，也是醉了。</p>]]></content>
      
      
      <categories>
          
          <category> 指南 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/16/hello-world/"/>
      <url>/2023/12/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>这是一个指南</p><p><a href="https://gavinblog.github.io/anzhiyu-docs/">安知鱼主题指南 (gavinblog.github.io)</a></p><h3 id="布局（Layout）"><a href="#布局（Layout）" class="headerlink" title="布局（Layout）"></a>布局（Layout）</h3><p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和 <code>draft</code>。在创建这三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 <code>post</code> 相同，都将储存到 <code>source/_posts</code> 文件夹。</p><table><thead><tr><th align="left">布局</th><th align="left">路径</th></tr></thead><tbody><tr><td align="left"><code>post</code></td><td align="left"><code>source/_posts</code></td></tr><tr><td align="left"><code>page</code></td><td align="left"><code>source</code></td></tr><tr><td align="left"><code>draft</code></td><td align="left"><code>source/_drafts</code></td></tr></tbody></table><h1 id="1-常用命令"><a href="#1-常用命令" class="headerlink" title="1 常用命令"></a>1 常用命令</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># draft</span><br><span class="line">hexo new draft &quot;test&quot;</span><br><span class="line"></span><br><span class="line">hexo publish draft &quot;test&quot;</span><br></pre></td></tr></table></figure><h1 id="2-公式"><a href="#2-公式" class="headerlink" title="2 公式"></a>2 公式</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm un hexo-renderer-marked --save</span><br><span class="line"># or</span><br><span class="line">npm un hexo-renderer-kramed --save</span><br><span class="line"># 安装 `hexo-renderer-markdown-it-plus`</span><br><span class="line">npm i @upupming/hexo-renderer-markdown-it-plus --save</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在根目录的 _config.yml 中使用下面的配置将 strict 设置为 false</span></span><br><span class="line"><span class="attr">markdown_it_plus:</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">plugin:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">&#x27;@neilsustc/markdown-it-katex&#x27;</span></span><br><span class="line">      <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">strict:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 指南 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的审稿意见</title>
      <link href="/2023/12/16/Manuscripts/review/"/>
      <url>/2023/12/16/Manuscripts/review/</url>
      
        <content type="html"><![CDATA[<h2 id="Intelligent-Vulnerability-Detector-using-deep-sequence-and-graph-based-Hybrid-Feature-Extraction"><a href="#Intelligent-Vulnerability-Detector-using-deep-sequence-and-graph-based-Hybrid-Feature-Extraction" class="headerlink" title="Intelligent Vulnerability Detector using deep sequence and graph based Hybrid Feature Extraction"></a>Intelligent Vulnerability Detector using deep sequence and graph based Hybrid Feature Extraction</h2><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191506692.png" alt="image-20231210113010116"></p><p>This manuscript proposed a graph-based and sequence-based neural network model for detecting vulnerabilities in Java code, utilizing multiple program features，which addresses the detection problem of a range of vulnerabilities collected from the Common Weakness Enumeration (CWE) . It introduces GCN-RFEMLP for extracting graph-based features and employs CodeBERT for extracting sequence-based features. However, there are some critical issues outlined in the manuscript making the referee has to reject it.</p><p>Comments：</p><ol><li>The COVID-19 pandemic, which is unrelated to the research and should not have been mentioned.</li><li>The manuscript presents a list of seven contributions; however, they lack conciseness and do not effectively emphasize the primary contributions.</li><li>Certain figures in the manuscript are not appropriate. Figure 1 appears to be more focused on the classification of machine learning methods and lacks contextual relevance, considering that the manuscript is specifically about vulnerability detection. Other figures also suffer from similar issues, as they seem to be detached from vulnerability detection and lack any meaningful connection. Figures 3 and 4 depict the node2vec process and GCN, respectively. However, these figures are not relevant to the vulnerability detection discussed in the paper and do not contribute to the study. Instead, the figures should focus on illustrating the transformation process from source code to code property graph, highlighting the comprehensive model proposed in the manuscript.</li><li>In the experimental section, the formatting of the tables presenting the experimental results lacks consistency. And, it is customary to report experimental results with two decimal places, such as 98.90. It is important to ensure that other result comparison data follow the same formatting convention.</li><li>The dataset description in the manuscript lacks clarity, and there is no mention of the labeling process for the data. Additionally, the comparison with other benchmarks does not indicate the dataset that was utilized.</li><li>Moreover, the manuscript lacks relevant explanations and approaches for addressing data imbalance, which can pose a risk of overfitting.</li></ol><h2 id="VulDet-BC-Binary-Software-Vulnerability-Detection-Based-on-BiGRU-and-CNN"><a href="#VulDet-BC-Binary-Software-Vulnerability-Detection-Based-on-BiGRU-and-CNN" class="headerlink" title="VulDet-BC: Binary Software Vulnerability Detection Based on BiGRU and CNN"></a>VulDet-BC: Binary Software Vulnerability Detection Based on BiGRU and CNN</h2><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191506699.png" alt="image-20231210113205646"></p><p>this manuscript 提出了一种二进制漏洞检测方法VulDet-BC，从二进制机器指令级别，结合BiGRU与CNN构建二进制漏洞检测模型，其中利用了注意力机制，并且通过与一些基线的对比在一些指标上优于基线，However, there are some critical issues outlined in the manuscript making the referee has to reject it.</p><p>Comments:</p><ol><li>文章在相关工作部分对漏洞检测这一研究领域的介绍不够充分，近期的许多新颖的工作并没有被提及。</li><li>深度学习方法常常基于一定的漏洞模式，来实现漏洞检测。文稿中的方法将二进制机器指令转换为数字形式，再转换过程中并没有提出任何与漏洞模式相关的概念，无论在语义或是语法上；</li><li>文中提到的BiGRU结合注意力模块并不新颖，且文中实验部分所提出的一系列RQ，缺乏对漏洞成因的思考，仅仅是从深度学习的角度在进行消融实验；</li><li>BinVulDet，在文稿中是第40个引用，是比较新的工作，在伪代码级别检测二进制软件的漏洞，文稿既没有对其工作进行介绍（related work)也没有与其进行对比研究。源代码漏洞检测方法VulDeePecker使用了code-gadget结合BiLSTM构建漏洞检测模型，但是文中似乎使用后半部分的BiLSTM进行对比，这样的对比实验设计已经不是同VulDeePecker工作进行对比了，这显然是错误的；</li><li>文稿中缺乏对漏洞检测任务的误报和漏报的分析，即FNR和FPR，这在漏洞检测的工作中非常重要，且缺乏对真实世界的软件漏洞进行检测的实验研究，使实验中提出的RQ变得更加单薄，对论文的研究缺乏支撑度；</li></ol><p>This manuscript proposed a binary software vulnerability detection method called VulDet-BC, which operates at the binary machine instruction level. It employs a combination of BiGRU and CNN along with attention mechanisms to build a vulnerability detection model. The manuscript claims superiority over baselines in certain metrics. However, the manuscript has several critical issues outlined below, which led the referee to reject it.</p><ol><li>The related work of the paper lacks a comprehensive review of the research field of vulnerability detection. Many recent and innovative works in the field have not been mentioned.</li></ol><p>2.Deep learning methods often rely on some vulnerability patterns to achieve effective vulnerability detection. The proposed method in the manuscript converts binary machine instructions into numeric representations without introducing any concepts related to vulnerability patterns, either semantically or syntactically.</p><p>3.The combination of BiGRU and attention mechanisms mentioned in the paper is not novel. Additionally, the series of research questions(RQ) proposed in the experimental section lacks contemplation on the causes of vulnerabilities. The experiments conducted only focus on the impact of deep learning techniques.</p><p>4.”BinVulDet” is referenced as the 40th citation in the manuscript and represents a relatively recent work that focuses on detecting vulnerabilities in binary software at the pseudo code level. However, the manuscript fails to provide an introduction to this work in the related work section and does not compare it with the proposed method. The source code vulnerability detection method “VulDeePecker” utilizes code-gadgets combined with BiLSTM to build a vulnerability detection model. However, it seems that the manuscript incorrectly compares its method with only the latter part, BiLSTM, which is not a valid comparison to the original VulDeePecker work. This discrepancy in the experimental design is evidently an error.</p><p>5.The manuscript lacks an analysis of false negatives (FNR) and false positives (FPR), which are crucial in vulnerability detection. Furthermore, there is a lack of experiments on detecting real-world software vulnerabilities, making the proposed research questions less substantiated.</p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 审稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c语言混淆技术</title>
      <link href="/2023/12/16/Security/c%E8%AF%AD%E8%A8%80%E6%B7%B7%E6%B7%86%E6%8A%80%E6%9C%AF/"/>
      <url>/2023/12/16/Security/c%E8%AF%AD%E8%A8%80%E6%B7%B7%E6%B7%86%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<h4 id="代码混淆定义："><a href="#代码混淆定义：" class="headerlink" title="代码混淆定义："></a>代码混淆定义：</h4><p>原代码 P 通过某种变换变成代码 P’，若 P 和 P’运行结果与过程行为保持一致，该种变换就称之为混淆变换。</p><p>具体来说，当混淆转换满足以下两种情况时，这种混淆变化称之为合法的转换：</p><ul><li>（1）如果源程序 P 无法停止运行或报错结束运行，则变换后的程序 P’可以结束运行也可以继续运行。</li><li>（2）否则，目标程序 P’也结束运行并且输出与源程序相同的结果。</li><li>&#x3D;&#x3D;两个程序之间操作并不一定完全相同，且不一定有相同的效率。&#x3D;&#x3D;</li></ul><p>实际上，混淆工具预先设定若干混淆规则，并使用其它更为复杂的代码取代源代码中符合条件的代码语句，<strong>虽然源代码语义并未改变但混淆后的程序运行过程中空间复杂度往往更高，执行时间也更长，甚至有可能改变系统环境</strong>等。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/imgs202312171007669.png" alt="image-20231217100733637"></p><p>图 2.1 展示了混淆编译的整体流程，</p><ul><li>首先混淆工具会对输入的源代码进行代码预处理得到&#x3D;&#x3D;程序控制流图 CFG、抽象语法树 AST&#x3D;&#x3D; 等信息，</li><li>然后对&#x3D;&#x3D;数据流、控制流&#x3D;&#x3D;等进行分析，并根据输入的混淆参数选择对应的混淆算法处理源代码，</li><li>最后输出混淆编译后的程序。</li></ul><p>尽管混淆策略多种多样，但通常按 Collberg 提出的方法将其大致分为四类[16]：</p><ul><li>布局混淆</li><li>数据流混淆</li><li>控制流混淆</li><li>预防混淆</li></ul><p>接下来将对这几类混淆策略进行详细分析。</p><h4 id="布局混淆"><a href="#布局混淆" class="headerlink" title="布局混淆"></a>布局混淆</h4><p>布局混淆是一种在不影响源程序正常运行的情况下，即<strong>不修改程序核心控制流和数据流</strong>，对程序包含有用信息的非核心代码做出修改的一种混淆策略；此处的非核心代码一般包括注释语句、多余代码片段、用于调试的代码语句以及自定义的变量名。</p>]]></content>
      
      
      <categories>
          
          <category> 软件安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C/C++ </tag>
            
            <tag> 软件安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computers&amp;Security</title>
      <link href="/2023/12/16/Manuscripts/sci/Computers&amp;Security/"/>
      <url>/2023/12/16/Manuscripts/sci/Computers&amp;Security/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www2.cloud.editorialmanager.com/cose/default2.aspx">Editorial Manager®</a></p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sci投稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overleaf 葵花宝典</title>
      <link href="/2023/12/16/Manuscripts/overleaf/overleaf%E7%BB%8F%E9%AA%8C/"/>
      <url>/2023/12/16/Manuscripts/overleaf/overleaf%E7%BB%8F%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="1-列表"><a href="#1-列表" class="headerlink" title="1.列表"></a>1.列表</h1><h2 id="itemize-命令【无序列表】"><a href="#itemize-命令【无序列表】" class="headerlink" title="{itemize}命令【无序列表】"></a><strong>{itemize}命令【无序列表】</strong></h2><blockquote><p>{itemize}命令对文本进行简单的排列，不是采用序号，默认是用实心圆点符号进行排列。这个命令需要和\item配合使用。</p></blockquote><p>默认为实心圆点符号</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">    <span class="keyword">\item</span> one</span><br><span class="line">    <span class="keyword">\item</span> two</span><br><span class="line">    <span class="keyword">\item</span> ...</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508804.png" alt="image-20231210154216456"></p><p>使用其他符号进行排列</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;itemize&#125;</span><br><span class="line">    <span class="keyword">\item</span>[*] one</span><br><span class="line">    <span class="keyword">\item</span>[*] two</span><br><span class="line">    <span class="keyword">\item</span>[*] ...</span><br><span class="line"><span class="keyword">\end</span>&#123;itemize&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508806.png" alt="image-20231210155010036"></p><h2 id="enumerate-命令【有序列表】"><a href="#enumerate-命令【有序列表】" class="headerlink" title="{enumerate}命令【有序列表】"></a><strong>{enumerate}命令【有序列表】</strong></h2><blockquote><p>{enumerate}命令采用序号对文本进行简单的排列，默认是用1，2，3进行排列。这个命令需要和\item配合使用。</p></blockquote><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;enumerate&#125;</span><br><span class="line">    <span class="keyword">\item</span> one</span><br><span class="line">    <span class="keyword">\item</span> two</span><br><span class="line">    <span class="keyword">\item</span> ...</span><br><span class="line"><span class="keyword">\end</span>&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191508812.png" alt="image-20231210155244000"></p><p><strong>使用其他形式的编号</strong>：</p><blockquote><p>{enumerate}产生所需要的编号，默认是采用数字1,2,3……进行排列。</p><p><strong>使用命令\usepackage{enumerate}</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;[i)]</span><br><span class="line">    \item one</span><br><span class="line">    \item two</span><br><span class="line">    \item ...</span><br><span class="line">\end&#123;enumerate&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;enumerate&#125;[1)]</span><br><span class="line">    \item one</span><br><span class="line">    \item two</span><br><span class="line">    \item ...</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 自定义编号形式</span><br><span class="line"></span><br><span class="line">\begin&#123;description&#125;</span><br><span class="line">    \item[Step1] one</span><br><span class="line">    \item[Step2] two</span><br><span class="line">    \item[Step3] ...</span><br><span class="line">\end&#123;description&#125;</span><br></pre></td></tr></table></figure><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191509540.png" alt="image-20231219150916511"></p><h1 id="2-三线表"><a href="#2-三线表" class="headerlink" title="2.三线表"></a>2.三线表</h1><p>使用方法1或者2都可以，两种latex编辑器WinEdt和TexStudio各有优点，看你选择，我用的是方法1，使用WinEdt。</p><p>直接显示latex 代码，然后你们根据自己的情况进行修改即可</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[h] %h表示三线表在当前位置插入</span><br><span class="line">\setlength&#123;\abovecaptionskip&#125;&#123;<span class="number">0.</span>05cm&#125; %设置三线表标题与第一条线间距</span><br><span class="line">\centering</span><br><span class="line">\caption&#123;\textbf&#123;The characteristics of various methods&#125;&#125;</span><br><span class="line">%表头文本加黑，但不加黑Table <span class="number">1.</span>字样，引入包即可：\usepackage[labelfont=bf]&#123;caption&#125;</span><br><span class="line">\arrayrulecolor&#123;black&#125; %设置三线表线条颜色：黑色</span><br><span class="line">\begin&#123;tabular*&#125;&#123;\hsize&#125;&#123;@&#123;\extracolsep&#123;\fill&#125;&#125;c c c c&#125; %&#123;\hsize&#125;使三线表自适应宽度，c表示文本居中</span><br><span class="line">  \hline</span><br><span class="line">  <span class="number">1</span> &amp; <span class="number">2</span> &amp; <span class="number">3</span> &amp; <span class="number">4</span>\\</span><br><span class="line">  \hline</span><br><span class="line">  <span class="number">11</span> &amp; <span class="number">22</span> &amp; <span class="number">33</span> &amp; <span class="number">44</span> \\</span><br><span class="line">  <span class="number">111</span> &amp; <span class="number">222</span> &amp; <span class="number">333</span> &amp; <span class="number">444</span> \\</span><br><span class="line">  <span class="number">1111</span> &amp; <span class="number">2222</span> &amp; <span class="number">3333</span> &amp; <span class="number">4444</span> \\</span><br><span class="line">  <span class="number">11111</span> &amp; <span class="number">22222</span> &amp; <span class="number">33333</span> &amp; <span class="number">44444</span> \\</span><br><span class="line">  <span class="number">111111</span> &amp; <span class="number">222222</span> &amp; <span class="number">333333</span> &amp; <span class="number">444444</span> \\</span><br><span class="line">  \hline</span><br><span class="line">\end&#123;tabular*&#125;</span><br><span class="line">\end&#123;table&#125;</span><br></pre></td></tr></table></figure><p>添加包：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;booktabs&#125;</span><br><span class="line">\usepackage&#123;amsmath&#125;</span><br><span class="line">\usepackage&#123;setspace&#125;</span><br><span class="line">\usepackage&#123;array,caption&#125;</span><br><span class="line">\usepackage[labelfont=bf]&#123;caption&#125;</span><br></pre></td></tr></table></figure><h1 id="3-图片过大处理"><a href="#3-图片过大处理" class="headerlink" title="3.图片过大处理"></a>3.图片过大处理</h1><p>在<a href="https://so.csdn.net/so/search?q=LaTeX%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87&spm=1001.2101.3001.7020">LaTeX插入图片</a>的时候，经常需要调整图片的大小。我们可以通过如下代码来完成：</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[htb]</span><br><span class="line">  <span class="keyword">\centering</span></span><br><span class="line">  <span class="keyword">\includegraphics</span>[width=0.5<span class="keyword">\linewidth</span>]&#123;fig2.png&#125;</span><br><span class="line">  <span class="keyword">\caption</span>&#123;图片的解释&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>其中，width&#x3D;0.5[linewidth](<a href="https://so.csdn.net/so/search?q=linewidth&spm=1001.2101.3001.7020">https://so.csdn.net/so/search?q=linewidth&amp;spm=1001.2101.3001.7020</a>) 表明将插入的图像等比例缩小至0.6倍。经验证，调整比例后图像成功地缩小了。</p><p>这样可以适应模板 自动地调整大小 不用手动去调整长宽 非常好用</p><h1 id="4-空格"><a href="#4-空格" class="headerlink" title="4.空格"></a>4.空格</h1><p>quad空格a \quad b一个m的宽度<br>大空格a\ b1&#x2F;3m宽度<br>中等空格a;b2&#x2F;7m宽度<br>小空格a,b1&#x2F;6m宽度<br>没有空格ab<br>紧贴a!b缩进1&#x2F;6m宽度</p><h1 id="5-latex的粗体"><a href="#5-latex的粗体" class="headerlink" title="5.latex的粗体"></a>5.latex的粗体</h1><p>latTx的粗体一般用以下命令：</p><p>\textbf{}：文本环境加粗。在数学环境使用的话，会使斜体效果消失。并且无法输出加粗的希腊字母。 </p><p>\mathbf{}：会变为粗体，但同样会导致数学字母斜体形式的丢失。 \boldmath{}：数学环境里可以加粗且不会使斜体消失。需要添加amsmath宏包。 \boldsymbol{}：可以对希腊字母加粗。需要添加amsmath宏包。 在数学环境中，比较推荐的方式是添加宏包\usepackage{bm}, 使用\bm{}命令加粗。</p><p>但是在xelatex或Luatex引擎的unicode-math环境中中，\bm{}会报错。此时，可以使用以下命令：</p><p>\symbfit{}：加粗，且有斜体效果 \symbf{}：加粗，没有斜体效果 \mathbfcal{}：加粗的\mathcal字体</p><p>[<a href="https://blog.csdn.net/xovee/article/details/106325136">翻译] [Overleaf] LaTeX 中的粗体、斜体、下划线_latex 斜体-CSDN博客</a></p><h1 id="6-图片与引用"><a href="#6-图片与引用" class="headerlink" title="6.图片与引用"></a>6.图片与引用</h1><p>示例：</p><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure*&#125;</span><br><span class="line"><span class="keyword">\centering</span></span><br><span class="line"><span class="keyword">\includegraphics</span>[scale=0.45]&#123;double<span class="built_in">_</span>single.eps&#125; <span class="comment">%scale=缩小比例，或者用width=2in</span></span><br><span class="line"><span class="keyword">\caption</span>&#123;Search&#125;     <span class="keyword">\label</span>&#123;fig:ss&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure*&#125;</span><br></pre></td></tr></table></figure><p>引用注意</p><p>\label{} 必须写在 \caption{} 的后面。</p><p>\ref{}:引用</p><p>\ref{fig:ss}, 即\ref{}, {}内为标签名称,我这里的标签名称是：fig:ss</p><h1 id="7-宽度问题"><a href="#7-宽度问题" class="headerlink" title="7.宽度问题"></a>7.宽度问题</h1><p>\hsize: 是 Latex中定义的长度，是一种叫做水平盒子的长度，它的主要作用是告诉TeX系统什么时候换行。所以大部分时候和\textwidth是一致的，但是在分栏状况下，\hsize只是栏的宽度；</p><p>\textwidth: 是 Latex中定义的长度，等效于\hsize，并且是固定不变的，可以理解为一行文字的宽度。</p><p>\pagewidth: 包含了页边的宽度，比\textwidth要大</p><p>\linewidth: 这指得是目前环境的宽度，是依赖于上下文的一个宽度值，例如新建了一个box，在这个box中，</p><p>\linewidth是box中文字的宽度。再例如minipage环境中，\linewidth就和这个minipage的大小有关.</p><p>\columnwidth: 如果文章分栏的话，这个宽度就是每一栏的宽度。</p>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> overleaf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ lamda表达式</title>
      <link href="/2023/12/16/Programmer/c-c++/lambda/"/>
      <url>/2023/12/16/Programmer/c-c++/lambda/</url>
      
        <content type="html"><![CDATA[<p>创建一个匿名函数并执行。Objective-C采用的是上尖号^，而C++ 11采用的是配对的方括号[]。实例如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    []&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Hello,Worldn&quot;</span>; </span><br><span class="line">    &#125;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们也可以方便的将这个创建的匿名函数赋值出来调用：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [](<span class="type">int</span> i) &#123; <span class="comment">// (int i) 是指传入改匿名函数的参数</span></span><br><span class="line">        cout &lt;&lt; i;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">func</span>(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="捕获选项"><a href="#捕获选项" class="headerlink" title="捕获选项"></a>捕获选项</h1><ul><li>[] Capture nothing (or, a scorched earth strategy?)</li><li>[&amp;] Capture any referenced variable by reference</li><li>[&#x3D;] Capture any referenced variable by making a copy</li><li>[&#x3D;, &amp;foo] Capture any referenced variable by making a copy, but capture variable foo by reference</li><li>[bar] Capture bar by making a copy; don’t copy anything else</li><li>[this] Capture the this pointer of the enclosing class</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-[]不捕获任何变量</span><br><span class="line"></span><br><span class="line">-[&amp;]通过引用捕获任何引用变量</span><br><span class="line"></span><br><span class="line">-[=]通过复制捕获任何引用变量</span><br><span class="line"></span><br><span class="line">-[=，&amp;foo]通过复制捕获任何引用的变量，但通过引用捕获变量foo</span><br><span class="line"></span><br><span class="line">-[bar]通过复制来捕获bar；不要复制其他任何东西</span><br><span class="line"></span><br><span class="line">-[this]捕获封闭类的this指针</span><br></pre></td></tr></table></figure><h1 id="不捕获任何变量"><a href="#不捕获任何变量" class="headerlink" title="[] 不捕获任何变量"></a>[] 不捕获任何变量</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [] &#123; cout &lt;&lt; i; &#125;;</span><br><span class="line">    <span class="built_in">func</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>vs 报错<br>error C3493: 无法隐式捕获“i”，因为尚未指定默认捕获模式<br>error C2064: 项不会计算为接受 0 个参数的函数</p><p>g++ 报错：<br>error: ‘i’ is not captured</p><p>要直接沿用外部的变量需要在 [] 中指名捕获。</p><h1 id="拷贝捕获"><a href="#拷贝捕获" class="headerlink" title="[&#x3D;] 拷贝捕获"></a>[&#x3D;] 拷贝捕获</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> func = [=]&#123;  <span class="comment">// [=] 表明将外部的所有变量拷贝一份到该函数内部</span></span><br><span class="line">        cout &lt;&lt; i;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">func</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br>1024</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [=]&#123;</span><br><span class="line">        <span class="comment">// fun1 内存在 i</span></span><br><span class="line">        cout &lt;&lt; i; <span class="comment">// 1024</span></span><br><span class="line">        <span class="keyword">auto</span> fun2 = []&#123; <span class="comment">// 未指名捕获， i 不存在</span></span><br><span class="line">            cout &lt;&lt; i;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="built_in">fun2</span>();</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="引用捕获"><a href="#引用捕获" class="headerlink" title="[&amp;] 引用捕获"></a>[&amp;] 引用捕获</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>;</span><br><span class="line">    cout &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [&amp;]&#123;</span><br><span class="line">        cout &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果:<br>0x28ff0c<br>0x28ff0c</p><h1 id="拷贝与引用混合"><a href="#拷贝与引用混合" class="headerlink" title="[&#x3D;, &amp;] 拷贝与引用混合"></a>[&#x3D;, &amp;] 拷贝与引用混合</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [=, &amp;i]&#123; <span class="comment">// 默认拷贝外部所有变量，但引用变量 i</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果<br>outside i:0x28ff0c<br>outside j:0x28ff08<br>inside i:0x28ff0c<br>inside j:0x28ff04</p><h1 id="bar-指定引用或拷贝"><a href="#bar-指定引用或拷贝" class="headerlink" title="[bar] 指定引用或拷贝"></a>[bar] 指定引用或拷贝</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [i]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; j &lt;&lt; endl; // j 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br>outside i value:1024 addr:0x28ff08<br>inside i value:1024 addr:0x28ff04</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [&amp;i]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i value:&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; addr:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; j &lt;&lt; endl; // j 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br>outside i value:1024 addr:0x28ff08<br>inside i value:1024 addr:0x28ff08</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = <span class="number">1024</span>, j = <span class="number">2048</span>, k;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;outside j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">auto</span> fun1 = [i, &amp;j]&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  i:&quot;</span> &lt;&lt; &amp;i &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;inside  j:&quot;</span> &lt;&lt; &amp;j &lt;&lt; endl;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; k; // k 未捕获</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果：<br>outside i:0x28ff0c<br>outside j:0x28ff08<br>inside i:0x28ff00<br>inside j:0x28ff08</p><h1 id="this-捕获-this-指针"><a href="#this-捕获-this-指针" class="headerlink" title="[this] 捕获 this 指针"></a>[this] 捕获 this 指针</h1><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#include &lt;iostream&gt;</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="title">std</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title">test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">hello</span>()</span> &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;test hello!n&quot;</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">lambda</span>()</span> &#123;</span><br><span class="line">        auto fun = [<span class="keyword">this</span>]&#123; <span class="comment">// 捕获了 this 指针</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;hello(); <span class="comment">// 这里 this 调用的就是 class test 的对象了</span></span><br><span class="line">        &#125;;</span><br><span class="line">        fun();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">int</span> <span class="title">main</span>()</span></span><br><span class="line">&#123;</span><br><span class="line">    test t;</span><br><span class="line">    t.lambda();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C/C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C/C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overleaf 问题</title>
      <link href="/2023/12/16/Manuscripts/overleaf/overleaf%E9%97%AE%E9%A2%98%20/"/>
      <url>/2023/12/16/Manuscripts/overleaf/overleaf%E9%97%AE%E9%A2%98%20/</url>
      
        <content type="html"><![CDATA[<h1 id="algorithm最后出现-0"><a href="#algorithm最后出现-0" class="headerlink" title="algorithm最后出现&#x3D;0"></a>algorithm最后出现&#x3D;0</h1><p><strong>解决方法：</strong></p><p>注释掉”\usepackage{algpseudocode}”</p><p>因为<code>\usepackage&#123;algpseudocode&#125; %This introduces extra zero at the end of algorithm</code></p><h1 id="table位置问题"><a href="#table位置问题" class="headerlink" title="table位置问题"></a>table位置问题</h1><p>如果table默认置顶，在.sty文件中定义了table环境，那么可尝试将</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[h] </span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;table&#125;[pos=h] </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Manuscripts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> overleaf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Siamese Network</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="低秩分解的几何解释"><a href="#低秩分解的几何解释" class="headerlink" title="低秩分解的几何解释"></a>低秩分解的几何解释</h1><p>低秩分解（Low-rank factorization）也可以通过几何的方式来解释，帮助我们理解其含义和应用。</p><p>假设我们有一个m×n的矩阵A，我们希望对其进行低秩分解，即将其分解为两个低秩矩阵的乘积：A ≈ UV^T。其中，U是一个m×k的矩阵，V是一个n×k的矩阵，k远远小于m和n。</p><p>几何上，可以将矩阵A视为描述一个向量空间中的点集。每一列可以看作是一个向量，而这些向量组成了一个n维的向量空间。低秩分解可以理解为通过两个低维的向量空间的点集的线性组合来近似表示原始向量空间中的点集。</p><p>具体地说，U矩阵的列向量可以看作是原始向量空间的基向量，它们将原始向量空间中的点集映射到一个低维的子空间。V矩阵的列向量则表示这个低维子空间中的基向量。通过对这两个子空间的基向量的线性组合，我们可以近似表示原始向量空间中的点集。</p><p>这个分解可以理解为以下几个几何步骤：</p><ol><li>U矩阵的列向量将原始向量空间中的点集映射到一个低维的子空间。这个子空间具有较低的维度k。</li><li>V矩阵的列向量表示这个低维子空间中的基向量，用于描述子空间中的点集。</li><li>通过对U和V的线性组合，将低维子空间中的点集映射回原始向量空间，近似重构出原始的点集。</li></ol><p>通过低秩分解，我们可以利用较低维度的向量空间来近似表示原始向量空间中的点集。这种近似表示可以在降低存储和计算成本的同时，尽可能地保留原始数据的主要结构和特征。</p><p>综上所述，几何视角可以帮助我们将低秩分解理解为通过两个低维子空间的基向量的线性组合来近似表示原始向量空间中的点集，从而实现对原始数据的降维和近似表示。这种几何解释有助于我们理解低秩分解的概念和原理。</p><h1 id="奇异值分解的几何理解"><a href="#奇异值分解的几何理解" class="headerlink" title="奇异值分解的几何理解"></a><a href="https://www.cnblogs.com/lukairui/p/17475145.html">奇异值分解的几何理解</a></h1><p>奇异值分解（SVD）可以通过几何的方式来解释，从而帮助我们理解其含义和应用。</p><p>首先，我们可以将一个矩阵视为对向量空间的一种变换。假设有一个m×n的矩阵A，其中每一列可以看作是一个向量，而这些向量组成了一个n维的向量空间。奇异值分解可以将这个向量空间的变换分解为三个基本的几何操作：旋转、缩放和再次旋转。</p><p>具体地说，奇异值分解将矩阵A分解为三个矩阵的乘积：A &#x3D; UΣVT。其中，U是一个正交矩阵，表示一个旋转操作；Σ是一个对角矩阵，对角线上的元素是奇异值，表示一个缩放操作；VT是另一个正交矩阵，表示另一个旋转操作。</p><p>这个分解可以理解为以下几个几何步骤：</p><ol><li>U对应的旋转矩阵将原始向量空间进行旋转操作，使其与新的基向量相对应。</li><li>Σ对应的对角矩阵进行缩放操作，将每个基向量的长度进行缩放，即改变了向量空间的比例关系。</li><li>V^T对应的旋转矩阵将缩放后的向量空间进行进一步旋转操作，以使其与原始向量空间对齐。</li></ol><p>通过奇异值分解，我们可以将原始矩阵A分解为这三个操作的组合，从而更好地理解和描述原始矩阵A的结构和特征。</p><p>此外，奇异值分解还提供了一种基于奇异值的重要性排序。奇异值的大小表示了每个基向量在变换中的重要性。较大的奇异值对应的基向量在变换中具有更大的影响力，而较小的奇异值对应的基向量在变换中贡献较小。</p><p>综上所述，几何视角可以帮助我们将奇异值分解理解为对向量空间的旋转、缩放和再次旋转等几何操作的组合，从而更好地理解和应用奇异值分解的概念和原理。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Siamese Network</title>
      <link href="/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/siamese/"/>
      <url>/2023/12/16/AILearning/DL/%E7%9F%A5%E8%AF%86%E7%82%B9/siamese/</url>
      
        <content type="html"><![CDATA[<h2 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h2><h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>分类问题：</p><ul><li><p>分类数量较少，每一类的数据量较多，比如ImageNet、VOC等。这种分类问题可以使用神经网络或者SVM解决，只要事先知道了所有的类。</p></li><li><p>分类数量较多（或者说无法确认具体数量），每一类的数据量较少，比如人脸识别、人脸验证任务。</p></li></ul><p>解决方法：</p><ul><li>提出一种思路：将输入映射为一个特征向量，使用两个向量之间的距离来表示输入之间的差异，如图像语义上的差异。</li><li>Siamese网络，每次需要输入两个样本作为一个sample对计算损失函数。</li><li>提出Contrastive Loss用于训练。</li></ul><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>孪生神经网络用于处理两个输入”比较类似”的情况。伪孪生神经网络适用于处理两个输入”有一定差别”的情况。比如，我们要计算两个句子或者词汇的语义相似度，使用siamese network比较适合；如果验证标题与正文的描述是否一致（标题和正文长度差别很大），或者文字是否描述了一幅图片（一个是图片，一个是文字），就应该使用pseudo-siamese network。也就是说，要根据具体的应用，判断应该使用哪一种结构，哪一种Loss。</p><h4 id="Siamese创新点"><a href="#Siamese创新点" class="headerlink" title="Siamese创新点"></a>Siamese创新点</h4><p>网络的创新点是淡化了标签，是的网络具有很好的扩展性，可以对那些没有训练过的类别进行分类，这一点优于很多算法。</p><p>该算法对一些小数据量的数据集也适用，变相地增加了整个数据集的大小，使得数据量相对较小的数据集也能用深度神经网络训练出不错的效果。</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528397.png" alt="image-20220104195309340"></p><p>不同输入$X_1$, $X_2$通过统一$G_W$得到两个向量$G_W(X_1), G_W(X_2)$，计算两个向量之间的$L1$距离获得$E_W$。<br>其中，两个network是两个共享权值的网络，实际上就是两个完全相同的网络。孪生神经网络有两个输入$X1$ and $X2$,将两个输入feed进入两个神经网络（Network1 and Network2），这两个神经网络分别将输入映射到新的空间，形成输入在新的空间中的表示。通过$Loss$的计算，评价两个输入的相似度。</p><p>$如果左右两边不共享权值$，而是两个不同的神经网络，叫做$pseudo-siamese network$，伪孪生神经网络。对于$pseudo-siamese network$，两边可以是不同的神经网络（如一个是lstm，一个是cnn），也可以是相同类型的神经网络。</p><h2 id="Siamese的损失函数"><a href="#Siamese的损失函数" class="headerlink" title="Siamese的损失函数"></a>Siamese的损失函数</h2><blockquote><p>Contrastive Loss</p></blockquote><h4 id="损失函数的选择"><a href="#损失函数的选择" class="headerlink" title="损失函数的选择"></a>损失函数的选择</h4><p>Softmax当然是一种好的选择，但不一定是最优选择，即使是在分类问题中。传统的siamese network使用Contrastive Loss。损失函数还有更多的选择，siamese network的初衷是计算两个输入的相似度,。左右两个神经网络分别将输入转换成一个”向量”，在新的空间中，通过判断cosine距离就能得到相似度了。$Cosine$是一个选择，<code>exp function</code>也是一种选择，欧式距离什么的都可以，<strong>训练的目标是让两个相似的输入距离尽可能的小，两个不同类别的输入距离尽可能的大。</strong></p><h4 id="论文中Contrastive-Loss"><a href="#论文中Contrastive-Loss" class="headerlink" title="论文中Contrastive Loss"></a>论文中Contrastive Loss</h4><p>论文中的损失函数定义如下：<br>Y代表$X_1$, $X_2$是否属于同一类别。输入同一类别为0，不属于同一类别为1。<br>P代表输入数据数量。<br>i表示当前输入数据下标。<br>$L_G$代表两个输入数据属于同一类别时的损失函数（G，genuine）。<br>$L_I$代表两个输入数据不属于同一类别的损失函数（I，imposter）。<br>$$<br>\mathcal{L}(W)&#x3D;\sum_{i&#x3D;1}^{P}L(W,(Y,X_{1},X_{2})^{i})<br>$$</p><p>$$<br>\begin{array} {c}{ {L(W,(X_{1},X_{1},X_{2})^{i})&#x3D;(1-Y)L_{G}\left(E_{W}(X_{1},X_{2})^{i}\right)} }\ { { } } { {+ {Y}L_{I}\left(E_{W}(X_{1},{ { {X} } }_{2})^{i}\right)} }\end{array}<br>$$</p><p>根据我们对两个向量间举例的定义，可以得到以下条件：<br>即不同类别向量间的距离比相同类别向量间距离大。<br>两个向量之间距离越小，属于同一类别的可能性就越大。</p><h4 id="目前的Contrastive-Loss"><a href="#目前的Contrastive-Loss" class="headerlink" title="目前的Contrastive Loss"></a>目前的Contrastive Loss</h4><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528403.png" alt="img"></p><p>其中：</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528406.png" alt="img"></p><p>代表两个样本特征X1和X2 的欧氏距离（二范数）P 表示样本的特征维数，Y 为两个样本是否匹配的标签，Y&#x3D;1 代表两个样本相似或者匹配，Y&#x3D;0 则代表不匹配，m 为设定的阈值，N 为样本个数。</p><p>观察上述的contrastive loss的表达式可以发现，这种损失函数可以很好</p><p>的表达成对样本的匹配程度，也能够很好用于训练提取特征的模型。</p><p>当 Y&#x3D;1（即样本相似时），损失函数只剩下</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528402.png" alt="在这里插入图片描述"></p><p>当 Y&#x3D;0 (即样本不相似时），损失函数为</p><p><img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528405.png" alt="在这里插入图片描述"></p><p>即当样本不相似时，其特征空间的欧式距离反而小的话，损失值会变大，这也正好符号我们的要求。<br><strong>注意：</strong><br>这里设置了一个阈值<strong>ｍargin</strong>，表示我们只考虑不相似特征欧式距离在<strong>０～ｍargin</strong>之间的，当距离超过ｍargin的，则把其loss看做为０**(即不相似的特征离的很远，其loss应该是很低的；而对于相似的特征反而离的很远，我们就需要增加其loss，从而不断更新成对样本的匹配程度)**</p><h2 id="Siamese的思想总结"><a href="#Siamese的思想总结" class="headerlink" title="Siamese的思想总结"></a>Siamese的思想总结</h2><p>其实讲了这么多，主要思想就是三点：</p><ul><li>输入不再是单个样本，而是一对样本，不再给单个的样本确切的标签，而且给定一对样本是否来自同一个类的标签，是就是0，不是就是1</li><li>设计了两个一模一样的网络，网络共享权值W，对输出进行了距离度量，可以说l1、l2等。</li><li>针对输入的样本对是否来自同一个类别设计了损失函数，损失函数形式有点类似交叉熵损失：<img src="https://foresta.oss-cn-beijing.aliyuncs.com/images/202312191528415.png" alt="在这里插入图片描述"></li></ul><p>最后使用获得的损失函数，使用反向传播梯度下降去更新两个网络共享的权值W。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL知识点 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
